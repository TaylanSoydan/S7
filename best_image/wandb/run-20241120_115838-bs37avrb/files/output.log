[*] Running training...
Using embed encoder
/apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/anaconda3-2023.03-1-emayrkyj4zgh57gt37ztn55cwzrrhstk/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/jaxvenv/lib/python3.10/site-packages/jax/_src/lax/lax.py:2785: ComplexWarning: Casting complex values to real discards the imaginary part
  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)
| epoch 1 | 1000/2250 batches | ms/batch 67.34 | Performance/Training accuracy:  0.10 | Performance/Training loss: 426.43
| epoch 1 | 2000/2250 batches | ms/batch 51.07 | Performance/Training accuracy:  0.10 | Performance/Training loss: 321.26
/apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/anaconda3-2023.03-1-emayrkyj4zgh57gt37ztn55cwzrrhstk/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
-----------------------------------------------------------------------------------------
| end of epoch   1 | time per epoch: 131.34s |
| Train Metrics | accuracy:  0.10 | loss: 363.23
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
/apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/anaconda3-2023.03-1-emayrkyj4zgh57gt37ztn55cwzrrhstk/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
| Eval  Metrics | accuracy:  0.08 | loss: 8509458481152.00
-----------------------------------------------------------------------------------------
[2024-11-20 12:01:02,294][absl][INFO] - Saving checkpoint at step: 2250
[2024-11-20 12:01:02,302][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:01:02,305][absl][WARNING] - SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.
[2024-11-20 12:01:02,308][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250.
[2024-11-20 12:01:02,310][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_2250
[2024-11-20 12:01:02,310][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:01:02,312][absl][INFO] - Wrote CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1732100462311625992, commit_timestamp_nsecs=None), json={"init_timestamp_nsecs": 1732100462311625992, "commit_timestamp_nsecs": null} to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250.orbax-checkpoint-tmp-0
[2024-11-20 12:01:02,313][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_2250
[2024-11-20 12:01:02,458][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_2250
[2024-11-20 12:01:02,461][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250.orbax-checkpoint-tmp-0 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250
[2024-11-20 12:01:02,462][absl][INFO] - Read CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1732100462311625992, commit_timestamp_nsecs=None) from /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250
[2024-11-20 12:01:02,472][absl][INFO] - Updated CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1732100462311625992, commit_timestamp_nsecs=1732100462462384399) to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250
[2024-11-20 12:01:02,472][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250`.
[2024-11-20 12:01:02,472][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_2250
| epoch 2 | 1000/2250 batches | ms/batch 53.66 | Performance/Training accuracy:  0.10 | Performance/Training loss: 243.76
| epoch 2 | 2000/2250 batches | ms/batch 51.05 | Performance/Training accuracy:  0.10 | Performance/Training loss: 122.13
-----------------------------------------------------------------------------------------
| end of epoch   2 | time per epoch: 117.60s |
| Train Metrics | accuracy:  0.10 | loss: 172.96
| Eval  Metrics | accuracy:  0.09 | loss: 76260253696.00
-----------------------------------------------------------------------------------------
[2024-11-20 12:03:07,154][absl][INFO] - Saving checkpoint at step: 4500
[2024-11-20 12:03:07,159][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:03:07,164][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_4500.
[2024-11-20 12:03:07,165][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_4500
[2024-11-20 12:03:07,166][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:03:07,167][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_4500
[2024-11-20 12:03:07,307][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_4500
[2024-11-20 12:03:07,309][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_4500.orbax-checkpoint-tmp-1 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_4500
[2024-11-20 12:03:07,313][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_4500`.
[2024-11-20 12:03:07,314][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_4500
[2024-11-20 12:03:07,314][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_2250
| epoch 3 | 1000/2250 batches | ms/batch 53.40 | Performance/Training accuracy:  0.10 | Performance/Training loss: 43.17
| epoch 3 | 2000/2250 batches | ms/batch 51.20 | Performance/Training accuracy:  0.10 | Performance/Training loss: 16.90
-----------------------------------------------------------------------------------------
| end of epoch   3 | time per epoch: 117.60s |
| Train Metrics | accuracy:  0.10 | loss: 27.78
| Eval  Metrics | accuracy:  0.10 | loss: 5004106.50
-----------------------------------------------------------------------------------------
[2024-11-20 12:05:12,250][absl][INFO] - Saving checkpoint at step: 6750
[2024-11-20 12:05:12,252][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:05:12,257][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_6750.
[2024-11-20 12:05:12,257][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_6750
[2024-11-20 12:05:12,257][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:05:12,259][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_6750
[2024-11-20 12:05:12,437][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_6750
[2024-11-20 12:05:12,440][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_6750.orbax-checkpoint-tmp-2 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_6750
[2024-11-20 12:05:12,449][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_6750`.
[2024-11-20 12:05:12,450][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_6750
[2024-11-20 12:05:12,450][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_4500
| epoch 4 | 1000/2250 batches | ms/batch 54.23 | Performance/Training accuracy:  0.11 | Performance/Training loss:  7.33
| epoch 4 | 2000/2250 batches | ms/batch 51.30 | Performance/Training accuracy:  0.10 | Performance/Training loss:  3.47
-----------------------------------------------------------------------------------------
| end of epoch   4 | time per epoch: 118.52s |
| Train Metrics | accuracy:  0.10 | loss:  5.11
| Eval  Metrics | accuracy:  0.10 | loss: 16231732.00
-----------------------------------------------------------------------------------------
[2024-11-20 12:07:18,225][absl][INFO] - Saving checkpoint at step: 9000
[2024-11-20 12:07:18,227][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:07:18,233][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_9000.
[2024-11-20 12:07:18,234][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_9000
[2024-11-20 12:07:18,234][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:07:18,240][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_9000
[2024-11-20 12:07:18,395][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_9000
[2024-11-20 12:07:18,399][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_9000.orbax-checkpoint-tmp-3 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_9000
[2024-11-20 12:07:18,408][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_9000`.
[2024-11-20 12:07:18,408][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_9000
[2024-11-20 12:07:18,409][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_6750
| epoch 5 | 1000/2250 batches | ms/batch 54.13 | Performance/Training accuracy:  0.11 | Performance/Training loss:  2.53
| epoch 5 | 2000/2250 batches | ms/batch 51.21 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.35
-----------------------------------------------------------------------------------------
| end of epoch   5 | time per epoch: 118.21s |
| Train Metrics | accuracy:  0.11 | loss:  2.42
| Eval  Metrics | accuracy:  0.10 | loss: 311856384.00
-----------------------------------------------------------------------------------------
[2024-11-20 12:09:23,828][absl][INFO] - Saving checkpoint at step: 11250
[2024-11-20 12:09:23,829][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:09:23,836][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_11250.
[2024-11-20 12:09:23,837][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_11250
[2024-11-20 12:09:23,837][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:09:23,839][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_11250
[2024-11-20 12:09:23,976][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_11250
[2024-11-20 12:09:23,978][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_11250.orbax-checkpoint-tmp-4 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_11250
[2024-11-20 12:09:23,992][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_11250`.
[2024-11-20 12:09:23,993][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_11250
[2024-11-20 12:09:23,994][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_9000
| epoch 6 | 1000/2250 batches | ms/batch 54.13 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
| epoch 6 | 2000/2250 batches | ms/batch 51.44 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
-----------------------------------------------------------------------------------------
| end of epoch   6 | time per epoch: 118.54s |
| Train Metrics | accuracy:  0.10 | loss:  2.30
| Eval  Metrics | accuracy:  0.10 | loss: 16027.78
-----------------------------------------------------------------------------------------
| epoch 7 | 1000/2250 batches | ms/batch 53.51 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
| epoch 7 | 2000/2250 batches | ms/batch 51.48 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
-----------------------------------------------------------------------------------------
| end of epoch   7 | time per epoch: 118.14s |
| Train Metrics | accuracy:  0.10 | loss:  2.30
| Eval  Metrics | accuracy:  0.10 | loss:  2.30
-----------------------------------------------------------------------------------------
| epoch 8 | 1000/2250 batches | ms/batch 54.11 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
| epoch 8 | 2000/2250 batches | ms/batch 51.83 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
-----------------------------------------------------------------------------------------
| end of epoch   8 | time per epoch: 119.12s |
| Train Metrics | accuracy:  0.10 | loss:  2.30
| Eval  Metrics | accuracy:  0.10 | loss:  2.30
-----------------------------------------------------------------------------------------
| epoch 9 | 1000/2250 batches | ms/batch 54.05 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
| epoch 9 | 2000/2250 batches | ms/batch 51.57 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
-----------------------------------------------------------------------------------------
| end of epoch   9 | time per epoch: 118.82s |
| Train Metrics | accuracy:  0.10 | loss:  2.30
| Eval  Metrics | accuracy:  0.11 | loss:  2.30
-----------------------------------------------------------------------------------------
[2024-11-20 12:17:47,823][absl][INFO] - Saving checkpoint at step: 20250
[2024-11-20 12:17:47,825][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:17:47,830][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_20250.
[2024-11-20 12:17:47,831][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_20250
[2024-11-20 12:17:47,831][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:17:47,832][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_20250
[2024-11-20 12:17:47,992][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_20250
[2024-11-20 12:17:47,995][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_20250.orbax-checkpoint-tmp-5 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_20250
[2024-11-20 12:17:48,004][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_20250`.
[2024-11-20 12:17:48,005][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_20250
[2024-11-20 12:17:48,006][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_11250
| epoch 10 | 1000/2250 batches | ms/batch 53.90 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
| epoch 10 | 2000/2250 batches | ms/batch 51.51 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
-----------------------------------------------------------------------------------------
| end of epoch  10 | time per epoch: 118.48s |
| Train Metrics | accuracy:  0.10 | loss:  2.30
| Eval  Metrics | accuracy:  0.09 | loss:  2.30
-----------------------------------------------------------------------------------------
| epoch 11 | 1000/2250 batches | ms/batch 53.70 | Performance/Training accuracy:  0.10 | Performance/Training loss:  2.30
| epoch 11 | 2000/2250 batches | ms/batch 50.70 | Performance/Training accuracy:  0.13 | Performance/Training loss:  2.27
-----------------------------------------------------------------------------------------
| end of epoch  11 | time per epoch: 117.23s |
| Train Metrics | accuracy:  0.13 | loss:  2.28
| Eval  Metrics | accuracy:  0.20 | loss:  2.15
-----------------------------------------------------------------------------------------
[2024-11-20 12:21:58,511][absl][INFO] - Saving checkpoint at step: 24750
[2024-11-20 12:21:58,513][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:21:58,518][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_24750.
[2024-11-20 12:21:58,519][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_24750
[2024-11-20 12:21:58,519][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:21:58,520][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_24750
[2024-11-20 12:21:58,675][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_24750
[2024-11-20 12:21:58,679][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_24750.orbax-checkpoint-tmp-6 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_24750
[2024-11-20 12:21:58,691][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_24750`.
[2024-11-20 12:21:58,692][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_24750
[2024-11-20 12:21:58,693][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_20250
| epoch 12 | 1000/2250 batches | ms/batch 55.03 | Performance/Training accuracy:  0.23 | Performance/Training loss:  2.10
| epoch 12 | 2000/2250 batches | ms/batch 51.11 | Performance/Training accuracy:  0.27 | Performance/Training loss:  1.98
-----------------------------------------------------------------------------------------
| end of epoch  12 | time per epoch: 119.22s |
| Train Metrics | accuracy:  0.25 | loss:  2.03
| Eval  Metrics | accuracy:  0.30 | loss:  1.88
-----------------------------------------------------------------------------------------
[2024-11-20 12:24:05,068][absl][INFO] - Saving checkpoint at step: 27000
[2024-11-20 12:24:05,076][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:24:05,081][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_27000.
[2024-11-20 12:24:05,082][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_27000
[2024-11-20 12:24:05,082][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:24:05,084][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_27000
[2024-11-20 12:24:05,239][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_27000
[2024-11-20 12:24:05,241][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_27000.orbax-checkpoint-tmp-7 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_27000
[2024-11-20 12:24:05,251][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_27000`.
[2024-11-20 12:24:05,251][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_27000
[2024-11-20 12:24:05,252][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_24750
| epoch 13 | 1000/2250 batches | ms/batch 53.71 | Performance/Training accuracy:  0.31 | Performance/Training loss:  1.89
| epoch 13 | 2000/2250 batches | ms/batch 50.95 | Performance/Training accuracy:  0.32 | Performance/Training loss:  1.83
-----------------------------------------------------------------------------------------
| end of epoch  13 | time per epoch: 117.48s |
| Train Metrics | accuracy:  0.32 | loss:  1.85
| Eval  Metrics | accuracy:  0.38 | loss:  1.72
-----------------------------------------------------------------------------------------
[2024-11-20 12:26:09,860][absl][INFO] - Saving checkpoint at step: 29250
[2024-11-20 12:26:09,862][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:26:09,867][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_29250.
[2024-11-20 12:26:09,867][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_29250
[2024-11-20 12:26:09,868][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:26:09,870][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_29250
[2024-11-20 12:26:10,020][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_29250
[2024-11-20 12:26:10,022][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_29250.orbax-checkpoint-tmp-8 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_29250
[2024-11-20 12:26:10,031][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_29250`.
[2024-11-20 12:26:10,031][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_29250
[2024-11-20 12:26:10,032][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_27000
| epoch 14 | 1000/2250 batches | ms/batch 54.19 | Performance/Training accuracy:  0.36 | Performance/Training loss:  1.73
| epoch 14 | 2000/2250 batches | ms/batch 50.77 | Performance/Training accuracy:  0.39 | Performance/Training loss:  1.67
-----------------------------------------------------------------------------------------
| end of epoch  14 | time per epoch: 117.92s |
| Train Metrics | accuracy:  0.38 | loss:  1.70
| Eval  Metrics | accuracy:  0.41 | loss:  1.65
-----------------------------------------------------------------------------------------
[2024-11-20 12:28:15,258][absl][INFO] - Saving checkpoint at step: 31500
[2024-11-20 12:28:15,266][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:28:15,269][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_31500.
[2024-11-20 12:28:15,270][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_31500
[2024-11-20 12:28:15,270][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:28:15,271][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_31500
[2024-11-20 12:28:15,426][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_31500
[2024-11-20 12:28:15,428][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_31500.orbax-checkpoint-tmp-9 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_31500
[2024-11-20 12:28:15,439][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_31500`.
[2024-11-20 12:28:15,440][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_31500
[2024-11-20 12:28:15,440][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_29250
| epoch 15 | 1000/2250 batches | ms/batch 54.12 | Performance/Training accuracy:  0.42 | Performance/Training loss:  1.61
| epoch 15 | 2000/2250 batches | ms/batch 50.94 | Performance/Training accuracy:  0.44 | Performance/Training loss:  1.55
-----------------------------------------------------------------------------------------
| end of epoch  15 | time per epoch: 117.99s |
| Train Metrics | accuracy:  0.43 | loss:  1.57
| Eval  Metrics | accuracy:  0.46 | loss:  1.50
-----------------------------------------------------------------------------------------
[2024-11-20 12:30:21,091][absl][INFO] - Saving checkpoint at step: 33750
[2024-11-20 12:30:21,103][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:30:21,110][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_33750.
[2024-11-20 12:30:21,111][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_33750
[2024-11-20 12:30:21,111][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:30:21,113][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_33750
[2024-11-20 12:30:21,253][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_33750
[2024-11-20 12:30:21,257][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_33750.orbax-checkpoint-tmp-10 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_33750
[2024-11-20 12:30:21,265][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_33750`.
[2024-11-20 12:30:21,265][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_33750
[2024-11-20 12:30:21,266][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_31500
| epoch 16 | 1000/2250 batches | ms/batch 53.91 | Performance/Training accuracy:  0.46 | Performance/Training loss:  1.49
| epoch 16 | 2000/2250 batches | ms/batch 51.55 | Performance/Training accuracy:  0.48 | Performance/Training loss:  1.46
-----------------------------------------------------------------------------------------
| end of epoch  16 | time per epoch: 118.46s |
| Train Metrics | accuracy:  0.47 | loss:  1.47
| Eval  Metrics | accuracy:  0.50 | loss:  1.38
-----------------------------------------------------------------------------------------
[2024-11-20 12:32:27,278][absl][INFO] - Saving checkpoint at step: 36000
[2024-11-20 12:32:27,279][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:32:27,285][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_36000.
[2024-11-20 12:32:27,286][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_36000
[2024-11-20 12:32:27,287][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:32:27,289][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_36000
[2024-11-20 12:32:27,428][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_36000
[2024-11-20 12:32:27,431][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_36000.orbax-checkpoint-tmp-11 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_36000
[2024-11-20 12:32:27,440][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_36000`.
[2024-11-20 12:32:27,440][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_36000
[2024-11-20 12:32:27,442][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_33750
| epoch 17 | 1000/2250 batches | ms/batch 54.37 | Performance/Training accuracy:  0.49 | Performance/Training loss:  1.42
| epoch 17 | 2000/2250 batches | ms/batch 51.58 | Performance/Training accuracy:  0.51 | Performance/Training loss:  1.38
-----------------------------------------------------------------------------------------
| end of epoch  17 | time per epoch: 118.99s |
| Train Metrics | accuracy:  0.50 | loss:  1.40
| Eval  Metrics | accuracy:  0.50 | loss:  1.41
-----------------------------------------------------------------------------------------
| epoch 18 | 1000/2250 batches | ms/batch 54.03 | Performance/Training accuracy:  0.52 | Performance/Training loss:  1.35
| epoch 18 | 2000/2250 batches | ms/batch 51.13 | Performance/Training accuracy:  0.52 | Performance/Training loss:  1.34
-----------------------------------------------------------------------------------------
| end of epoch  18 | time per epoch: 118.19s |
| Train Metrics | accuracy:  0.52 | loss:  1.34
| Eval  Metrics | accuracy:  0.55 | loss:  1.29
-----------------------------------------------------------------------------------------
[2024-11-20 12:36:39,827][absl][INFO] - Saving checkpoint at step: 40500
[2024-11-20 12:36:39,829][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:36:39,834][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_40500.
[2024-11-20 12:36:39,834][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_40500
[2024-11-20 12:36:39,835][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:36:39,836][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_40500
[2024-11-20 12:36:39,981][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_40500
[2024-11-20 12:36:39,985][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_40500.orbax-checkpoint-tmp-12 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_40500
[2024-11-20 12:36:39,993][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_40500`.
[2024-11-20 12:36:39,993][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_40500
[2024-11-20 12:36:39,995][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_36000
| epoch 19 | 1000/2250 batches | ms/batch 54.33 | Performance/Training accuracy:  0.54 | Performance/Training loss:  1.29
| epoch 19 | 2000/2250 batches | ms/batch 51.31 | Performance/Training accuracy:  0.54 | Performance/Training loss:  1.28
-----------------------------------------------------------------------------------------
| end of epoch  19 | time per epoch: 118.56s |
| Train Metrics | accuracy:  0.54 | loss:  1.29
| Eval  Metrics | accuracy:  0.57 | loss:  1.24
-----------------------------------------------------------------------------------------
[2024-11-20 12:38:46,341][absl][INFO] - Saving checkpoint at step: 42750
[2024-11-20 12:38:46,343][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:38:46,348][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_42750.
[2024-11-20 12:38:46,348][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_42750
[2024-11-20 12:38:46,349][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:38:46,350][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_42750
[2024-11-20 12:38:46,517][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_42750
[2024-11-20 12:38:46,520][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_42750.orbax-checkpoint-tmp-13 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_42750
[2024-11-20 12:38:46,528][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_42750`.
[2024-11-20 12:38:46,528][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_42750
[2024-11-20 12:38:46,529][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_40500
| epoch 20 | 1000/2250 batches | ms/batch 54.10 | Performance/Training accuracy:  0.56 | Performance/Training loss:  1.24
| epoch 20 | 2000/2250 batches | ms/batch 51.08 | Performance/Training accuracy:  0.56 | Performance/Training loss:  1.25
-----------------------------------------------------------------------------------------
| end of epoch  20 | time per epoch: 118.29s |
| Train Metrics | accuracy:  0.56 | loss:  1.24
| Eval  Metrics | accuracy:  0.57 | loss:  1.23
-----------------------------------------------------------------------------------------
[2024-11-20 12:40:52,257][absl][INFO] - Saving checkpoint at step: 45000
[2024-11-20 12:40:52,259][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:40:52,263][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_45000.
[2024-11-20 12:40:52,264][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_45000
[2024-11-20 12:40:52,264][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:40:52,265][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_45000
[2024-11-20 12:40:52,435][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_45000
[2024-11-20 12:40:52,438][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_45000.orbax-checkpoint-tmp-14 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_45000
[2024-11-20 12:40:52,447][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_45000`.
[2024-11-20 12:40:52,448][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_45000
[2024-11-20 12:40:52,448][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_42750
| epoch 21 | 1000/2250 batches | ms/batch 53.71 | Performance/Training accuracy:  0.58 | Performance/Training loss:  1.20
| epoch 21 | 2000/2250 batches | ms/batch 51.61 | Performance/Training accuracy:  0.57 | Performance/Training loss:  1.21
-----------------------------------------------------------------------------------------
| end of epoch  21 | time per epoch: 118.36s |
| Train Metrics | accuracy:  0.58 | loss:  1.20
| Eval  Metrics | accuracy:  0.55 | loss:  1.28
-----------------------------------------------------------------------------------------
| epoch 22 | 1000/2250 batches | ms/batch 54.06 | Performance/Training accuracy:  0.58 | Performance/Training loss:  1.17
| epoch 22 | 2000/2250 batches | ms/batch 51.59 | Performance/Training accuracy:  0.59 | Performance/Training loss:  1.17
-----------------------------------------------------------------------------------------
| end of epoch  22 | time per epoch: 118.68s |
| Train Metrics | accuracy:  0.59 | loss:  1.17
| Eval  Metrics | accuracy:  0.58 | loss:  1.23
-----------------------------------------------------------------------------------------
[2024-11-20 12:45:04,692][absl][INFO] - Saving checkpoint at step: 49500
[2024-11-20 12:45:04,694][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:45:05,081][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_49500.
[2024-11-20 12:45:05,082][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_49500
[2024-11-20 12:45:05,082][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:45:05,084][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_49500
[2024-11-20 12:45:05,231][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_49500
[2024-11-20 12:45:05,233][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_49500.orbax-checkpoint-tmp-15 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_49500
[2024-11-20 12:45:05,242][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_49500`.
[2024-11-20 12:45:05,242][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_49500
[2024-11-20 12:45:05,243][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_45000
| epoch 23 | 1000/2250 batches | ms/batch 54.39 | Performance/Training accuracy:  0.60 | Performance/Training loss:  1.12

| epoch 23 | 2000/2250 batches | ms/batch 51.71 | Performance/Training accuracy:  0.59 | Performance/Training loss:  1.16
-----------------------------------------------------------------------------------------
| end of epoch  23 | time per epoch: 118.94s |
| Train Metrics | accuracy:  0.60 | loss:  1.14
| Eval  Metrics | accuracy:  0.59 | loss:  1.17
-----------------------------------------------------------------------------------------
[2024-11-20 12:47:11,626][absl][INFO] - Saving checkpoint at step: 51750
[2024-11-20 12:47:11,628][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:47:11,634][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_51750.
[2024-11-20 12:47:11,634][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_51750
[2024-11-20 12:47:11,635][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:47:11,636][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_51750
[2024-11-20 12:47:11,778][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_51750
[2024-11-20 12:47:11,782][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_51750.orbax-checkpoint-tmp-16 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_51750
[2024-11-20 12:47:11,787][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_51750`.
[2024-11-20 12:47:11,788][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_51750
[2024-11-20 12:47:11,789][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_49500
| epoch 24 | 1000/2250 batches | ms/batch 54.32 | Performance/Training accuracy:  0.61 | Performance/Training loss:  1.11
| epoch 24 | 2000/2250 batches | ms/batch 50.91 | Performance/Training accuracy:  0.61 | Performance/Training loss:  1.11
-----------------------------------------------------------------------------------------
| end of epoch  24 | time per epoch: 118.37s |
| Train Metrics | accuracy:  0.61 | loss:  1.11
| Eval  Metrics | accuracy:  0.59 | loss:  1.16
-----------------------------------------------------------------------------------------
[2024-11-20 12:49:18,003][absl][INFO] - Saving checkpoint at step: 54000
[2024-11-20 12:49:18,005][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:49:18,010][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_54000.
[2024-11-20 12:49:18,011][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_54000
[2024-11-20 12:49:18,011][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:49:18,012][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_54000
[2024-11-20 12:49:18,158][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_54000
[2024-11-20 12:49:18,161][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_54000.orbax-checkpoint-tmp-17 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_54000
[2024-11-20 12:49:18,168][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_54000`.
[2024-11-20 12:49:18,168][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_54000
[2024-11-20 12:49:18,170][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_51750
| epoch 25 | 1000/2250 batches | ms/batch 53.82 | Performance/Training accuracy:  0.62 | Performance/Training loss:  1.10
| epoch 25 | 2000/2250 batches | ms/batch 51.60 | Performance/Training accuracy:  0.61 | Performance/Training loss:  1.11
-----------------------------------------------------------------------------------------
| end of epoch  25 | time per epoch: 118.41s |
| Train Metrics | accuracy:  0.61 | loss:  1.10
| Eval  Metrics | accuracy:  0.61 | loss:  1.13
-----------------------------------------------------------------------------------------
[2024-11-20 12:51:24,169][absl][INFO] - Saving checkpoint at step: 56250
[2024-11-20 12:51:24,170][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:51:24,176][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_56250.
[2024-11-20 12:51:24,177][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_56250
[2024-11-20 12:51:24,177][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:51:24,179][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_56250
[2024-11-20 12:51:24,314][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_56250
[2024-11-20 12:51:24,316][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_56250.orbax-checkpoint-tmp-18 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_56250
[2024-11-20 12:51:24,330][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_56250`.
[2024-11-20 12:51:24,330][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_56250
[2024-11-20 12:51:24,331][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_54000
| epoch 26 | 1000/2250 batches | ms/batch 54.72 | Performance/Training accuracy:  0.63 | Performance/Training loss:  1.07
| epoch 26 | 2000/2250 batches | ms/batch 51.41 | Performance/Training accuracy:  0.62 | Performance/Training loss:  1.09
-----------------------------------------------------------------------------------------
| end of epoch  26 | time per epoch: 119.13s |
| Train Metrics | accuracy:  0.62 | loss:  1.08
| Eval  Metrics | accuracy:  0.62 | loss:  1.09
-----------------------------------------------------------------------------------------
[2024-11-20 12:53:31,517][absl][INFO] - Saving checkpoint at step: 58500
[2024-11-20 12:53:31,519][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:53:31,524][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_58500.
[2024-11-20 12:53:31,524][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_58500
[2024-11-20 12:53:31,525][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:53:31,526][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_58500
[2024-11-20 12:53:31,680][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_58500
[2024-11-20 12:53:31,684][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_58500.orbax-checkpoint-tmp-19 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_58500
[2024-11-20 12:53:31,696][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_58500`.
[2024-11-20 12:53:31,697][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_58500
[2024-11-20 12:53:31,698][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_56250
| epoch 27 | 1000/2250 batches | ms/batch 54.12 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.04
| epoch 27 | 2000/2250 batches | ms/batch 51.53 | Performance/Training accuracy:  0.63 | Performance/Training loss:  1.08
-----------------------------------------------------------------------------------------
| end of epoch  27 | time per epoch: 118.73s |
| Train Metrics | accuracy:  0.63 | loss:  1.07
| Eval  Metrics | accuracy:  0.63 | loss:  1.04
-----------------------------------------------------------------------------------------
[2024-11-20 12:55:38,231][absl][INFO] - Saving checkpoint at step: 60750
[2024-11-20 12:55:38,233][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:55:38,239][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_60750.
[2024-11-20 12:55:38,240][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_60750
[2024-11-20 12:55:38,240][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:55:38,242][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_60750
[2024-11-20 12:55:38,384][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_60750
[2024-11-20 12:55:38,386][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_60750.orbax-checkpoint-tmp-20 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_60750
[2024-11-20 12:55:38,397][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_60750`.
[2024-11-20 12:55:38,397][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_60750
[2024-11-20 12:55:38,398][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_58500
| epoch 28 | 1000/2250 batches | ms/batch 54.55 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.04
| epoch 28 | 2000/2250 batches | ms/batch 51.18 | Performance/Training accuracy:  0.63 | Performance/Training loss:  1.06
-----------------------------------------------------------------------------------------
| end of epoch  28 | time per epoch: 118.69s |
| Train Metrics | accuracy:  0.63 | loss:  1.05
| Eval  Metrics | accuracy:  0.63 | loss:  1.06
-----------------------------------------------------------------------------------------
| epoch 29 | 1000/2250 batches | ms/batch 53.83 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.03
| epoch 29 | 2000/2250 batches | ms/batch 50.57 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.05
-----------------------------------------------------------------------------------------
| end of epoch  29 | time per epoch: 117.20s |
| Train Metrics | accuracy:  0.64 | loss:  1.04
| Eval  Metrics | accuracy:  0.65 | loss:  1.00
-----------------------------------------------------------------------------------------
[2024-11-20 12:59:49,961][absl][INFO] - Saving checkpoint at step: 65250
[2024-11-20 12:59:49,964][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 12:59:49,973][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_65250.
[2024-11-20 12:59:49,974][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_65250
[2024-11-20 12:59:49,974][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 12:59:49,976][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_65250
[2024-11-20 12:59:50,107][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_65250
[2024-11-20 12:59:50,112][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_65250.orbax-checkpoint-tmp-21 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_65250
[2024-11-20 12:59:50,124][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_65250`.
[2024-11-20 12:59:50,125][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_65250
[2024-11-20 12:59:50,126][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_60750
| epoch 30 | 1000/2250 batches | ms/batch 54.22 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.02
| epoch 30 | 2000/2250 batches | ms/batch 51.34 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.04
-----------------------------------------------------------------------------------------
| end of epoch  30 | time per epoch: 118.61s |
| Train Metrics | accuracy:  0.64 | loss:  1.03
| Eval  Metrics | accuracy:  0.64 | loss:  1.04
-----------------------------------------------------------------------------------------
| epoch 31 | 1000/2250 batches | ms/batch 54.38 | Performance/Training accuracy:  0.65 | Performance/Training loss:  1.00
| epoch 31 | 2000/2250 batches | ms/batch 51.56 | Performance/Training accuracy:  0.64 | Performance/Training loss:  1.03
-----------------------------------------------------------------------------------------
| end of epoch  31 | time per epoch: 118.86s |
| Train Metrics | accuracy:  0.65 | loss:  1.02
| Eval  Metrics | accuracy:  0.64 | loss:  1.04
-----------------------------------------------------------------------------------------
| epoch 32 | 1000/2250 batches | ms/batch 54.44 | Performance/Training accuracy:  0.65 | Performance/Training loss:  1.00
| epoch 32 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.65 | Performance/Training loss:  1.01
-----------------------------------------------------------------------------------------
| end of epoch  32 | time per epoch: 118.76s |
| Train Metrics | accuracy:  0.65 | loss:  1.01
| Eval  Metrics | accuracy:  0.65 | loss:  1.01
-----------------------------------------------------------------------------------------
| epoch 33 | 1000/2250 batches | ms/batch 54.24 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.98
| epoch 33 | 2000/2250 batches | ms/batch 51.37 | Performance/Training accuracy:  0.65 | Performance/Training loss:  1.01
-----------------------------------------------------------------------------------------
| end of epoch  33 | time per epoch: 118.72s |
| Train Metrics | accuracy:  0.65 | loss:  1.00
| Eval  Metrics | accuracy:  0.65 | loss:  1.01
-----------------------------------------------------------------------------------------
| epoch 34 | 1000/2250 batches | ms/batch 54.79 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.98
| epoch 34 | 2000/2250 batches | ms/batch 51.74 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.99
-----------------------------------------------------------------------------------------
| end of epoch  34 | time per epoch: 119.69s |
| Train Metrics | accuracy:  0.66 | loss:  0.99
| Eval  Metrics | accuracy:  0.65 | loss:  0.99
-----------------------------------------------------------------------------------------
[2024-11-20 13:10:23,610][absl][INFO] - Saving checkpoint at step: 76500
[2024-11-20 13:10:23,619][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:10:23,624][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_76500.
[2024-11-20 13:10:23,625][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_76500
[2024-11-20 13:10:23,625][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:10:23,627][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_76500
[2024-11-20 13:10:24,303][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_76500
[2024-11-20 13:10:24,306][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_76500.orbax-checkpoint-tmp-22 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_76500
[2024-11-20 13:10:24,318][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_76500`.
[2024-11-20 13:10:24,319][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_76500
[2024-11-20 13:10:24,320][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_65250
| epoch 35 | 1000/2250 batches | ms/batch 54.70 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.97
| epoch 35 | 2000/2250 batches | ms/batch 51.53 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.99
-----------------------------------------------------------------------------------------
| end of epoch  35 | time per epoch: 119.30s |
| Train Metrics | accuracy:  0.66 | loss:  0.98
| Eval  Metrics | accuracy:  0.66 | loss:  1.01
-----------------------------------------------------------------------------------------
[2024-11-20 13:12:31,518][absl][INFO] - Saving checkpoint at step: 78750
[2024-11-20 13:12:31,525][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:12:31,530][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_78750.
[2024-11-20 13:12:31,530][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_78750
[2024-11-20 13:12:31,531][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:12:31,532][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_78750
[2024-11-20 13:12:31,666][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_78750
[2024-11-20 13:12:31,669][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_78750.orbax-checkpoint-tmp-23 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_78750
[2024-11-20 13:12:31,682][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_78750`.
[2024-11-20 13:12:31,683][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_78750
[2024-11-20 13:12:31,686][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_76500
| epoch 36 | 1000/2250 batches | ms/batch 54.32 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.96
| epoch 36 | 2000/2250 batches | ms/batch 51.27 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.98
-----------------------------------------------------------------------------------------
| end of epoch  36 | time per epoch: 118.55s |
| Train Metrics | accuracy:  0.66 | loss:  0.97
| Eval  Metrics | accuracy:  0.65 | loss:  1.01
-----------------------------------------------------------------------------------------
| epoch 37 | 1000/2250 batches | ms/batch 53.51 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.93
| epoch 37 | 2000/2250 batches | ms/batch 51.15 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.99
-----------------------------------------------------------------------------------------
| end of epoch  37 | time per epoch: 117.60s |
| Train Metrics | accuracy:  0.66 | loss:  0.97
| Eval  Metrics | accuracy:  0.65 | loss:  1.03
-----------------------------------------------------------------------------------------
| epoch 38 | 1000/2250 batches | ms/batch 54.55 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.95
| epoch 38 | 2000/2250 batches | ms/batch 51.17 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.97
-----------------------------------------------------------------------------------------
| end of epoch  38 | time per epoch: 118.76s |
| Train Metrics | accuracy:  0.67 | loss:  0.96
| Eval  Metrics | accuracy:  0.63 | loss:  1.05
-----------------------------------------------------------------------------------------
| epoch 39 | 1000/2250 batches | ms/batch 54.20 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.93
| epoch 39 | 2000/2250 batches | ms/batch 51.19 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.98
-----------------------------------------------------------------------------------------
| end of epoch  39 | time per epoch: 118.36s |
| Train Metrics | accuracy:  0.67 | loss:  0.96
| Eval  Metrics | accuracy:  0.64 | loss:  1.04
-----------------------------------------------------------------------------------------
| epoch 40 | 1000/2250 batches | ms/batch 54.30 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.92
| epoch 40 | 2000/2250 batches | ms/batch 50.92 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.97
-----------------------------------------------------------------------------------------
| end of epoch  40 | time per epoch: 118.07s |
| Train Metrics | accuracy:  0.67 | loss:  0.95
| Eval  Metrics | accuracy:  0.67 | loss:  0.94
-----------------------------------------------------------------------------------------
[2024-11-20 13:23:01,699][absl][INFO] - Saving checkpoint at step: 90000
[2024-11-20 13:23:01,704][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:23:01,710][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_90000.
[2024-11-20 13:23:01,710][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_90000
[2024-11-20 13:23:01,711][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:23:01,713][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_90000
[2024-11-20 13:23:01,863][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_90000
[2024-11-20 13:23:01,866][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_90000.orbax-checkpoint-tmp-24 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_90000
[2024-11-20 13:23:01,873][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_90000`.
[2024-11-20 13:23:01,873][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_90000
[2024-11-20 13:23:01,874][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_78750
| epoch 41 | 1000/2250 batches | ms/batch 54.18 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.92
| epoch 41 | 2000/2250 batches | ms/batch 50.84 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.95
-----------------------------------------------------------------------------------------
| end of epoch  41 | time per epoch: 117.90s |
| Train Metrics | accuracy:  0.67 | loss:  0.94
| Eval  Metrics | accuracy:  0.67 | loss:  0.94
-----------------------------------------------------------------------------------------
[2024-11-20 13:25:07,698][absl][INFO] - Saving checkpoint at step: 92250
[2024-11-20 13:25:07,700][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:25:07,705][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_92250.
[2024-11-20 13:25:07,706][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_92250
[2024-11-20 13:25:07,707][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:25:07,708][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_92250
[2024-11-20 13:25:07,841][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_92250
[2024-11-20 13:25:07,844][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_92250.orbax-checkpoint-tmp-25 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_92250
[2024-11-20 13:25:07,850][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_92250`.
[2024-11-20 13:25:07,851][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_92250
[2024-11-20 13:25:07,852][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_90000
| epoch 42 | 1000/2250 batches | ms/batch 54.76 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.92
| epoch 42 | 2000/2250 batches | ms/batch 51.10 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.94
-----------------------------------------------------------------------------------------
| end of epoch  42 | time per epoch: 118.80s |
| Train Metrics | accuracy:  0.67 | loss:  0.93
| Eval  Metrics | accuracy:  0.67 | loss:  0.93
-----------------------------------------------------------------------------------------
[2024-11-20 13:27:14,766][absl][INFO] - Saving checkpoint at step: 94500
[2024-11-20 13:27:14,768][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:27:14,774][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_94500.
[2024-11-20 13:27:14,775][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_94500
[2024-11-20 13:27:14,775][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:27:14,777][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_94500
[2024-11-20 13:27:14,925][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_94500
[2024-11-20 13:27:14,929][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_94500.orbax-checkpoint-tmp-26 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_94500
[2024-11-20 13:27:14,939][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_94500`.
[2024-11-20 13:27:14,940][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_94500
[2024-11-20 13:27:14,941][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_92250
| epoch 43 | 1000/2250 batches | ms/batch 53.51 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.91
| epoch 43 | 2000/2250 batches | ms/batch 50.84 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.95
-----------------------------------------------------------------------------------------
| end of epoch  43 | time per epoch: 117.28s |
| Train Metrics | accuracy:  0.68 | loss:  0.93
| Eval  Metrics | accuracy:  0.68 | loss:  0.92
-----------------------------------------------------------------------------------------
[2024-11-20 13:29:20,098][absl][INFO] - Saving checkpoint at step: 96750
[2024-11-20 13:29:20,104][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:29:20,109][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_96750.
[2024-11-20 13:29:20,110][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_96750
[2024-11-20 13:29:20,110][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:29:20,112][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_96750
[2024-11-20 13:29:20,256][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_96750
[2024-11-20 13:29:20,259][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_96750.orbax-checkpoint-tmp-27 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_96750
[2024-11-20 13:29:20,271][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_96750`.
[2024-11-20 13:29:20,271][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_96750
[2024-11-20 13:29:20,272][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_94500

| epoch 44 | 1000/2250 batches | ms/batch 54.45 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.91
| epoch 44 | 2000/2250 batches | ms/batch 51.05 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.94
-----------------------------------------------------------------------------------------
| end of epoch  44 | time per epoch: 118.44s |
| Train Metrics | accuracy:  0.68 | loss:  0.93
| Eval  Metrics | accuracy:  0.67 | loss:  0.93
-----------------------------------------------------------------------------------------
| epoch 45 | 1000/2250 batches | ms/batch 54.30 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.91
| epoch 45 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.92
-----------------------------------------------------------------------------------------
| end of epoch  45 | time per epoch: 118.74s |
| Train Metrics | accuracy:  0.68 | loss:  0.92
| Eval  Metrics | accuracy:  0.69 | loss:  0.91
-----------------------------------------------------------------------------------------
[2024-11-20 13:33:33,108][absl][INFO] - Saving checkpoint at step: 101250
[2024-11-20 13:33:33,114][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:33:33,119][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_101250.
[2024-11-20 13:33:33,120][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_101250
[2024-11-20 13:33:33,120][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:33:33,122][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_101250
[2024-11-20 13:33:33,269][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_101250
[2024-11-20 13:33:33,272][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_101250.orbax-checkpoint-tmp-28 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_101250
[2024-11-20 13:33:33,284][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_101250`.
[2024-11-20 13:33:33,285][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_101250
[2024-11-20 13:33:33,286][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_96750
| epoch 46 | 1000/2250 batches | ms/batch 54.91 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.90
| epoch 46 | 2000/2250 batches | ms/batch 51.49 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.94
-----------------------------------------------------------------------------------------
| end of epoch  46 | time per epoch: 119.61s |
| Train Metrics | accuracy:  0.68 | loss:  0.92
| Eval  Metrics | accuracy:  0.68 | loss:  0.92
-----------------------------------------------------------------------------------------
| epoch 47 | 1000/2250 batches | ms/batch 54.42 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.90
| epoch 47 | 2000/2250 batches | ms/batch 51.36 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.92
-----------------------------------------------------------------------------------------
| end of epoch  47 | time per epoch: 118.84s |
| Train Metrics | accuracy:  0.69 | loss:  0.91
| Eval  Metrics | accuracy:  0.67 | loss:  0.98
-----------------------------------------------------------------------------------------
| epoch 48 | 1000/2250 batches | ms/batch 54.34 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.89
| epoch 48 | 2000/2250 batches | ms/batch 50.96 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.91
-----------------------------------------------------------------------------------------
| end of epoch  48 | time per epoch: 118.25s |
| Train Metrics | accuracy:  0.69 | loss:  0.91
| Eval  Metrics | accuracy:  0.68 | loss:  0.92
-----------------------------------------------------------------------------------------
| epoch 49 | 1000/2250 batches | ms/batch 53.81 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.87
| epoch 49 | 2000/2250 batches | ms/batch 51.00 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.92
-----------------------------------------------------------------------------------------
| end of epoch  49 | time per epoch: 117.71s |
| Train Metrics | accuracy:  0.69 | loss:  0.90
| Eval  Metrics | accuracy:  0.68 | loss:  0.93
-----------------------------------------------------------------------------------------
| epoch 50 | 1000/2250 batches | ms/batch 54.52 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.88
| epoch 50 | 2000/2250 batches | ms/batch 50.94 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.90
-----------------------------------------------------------------------------------------
| end of epoch  50 | time per epoch: 118.42s |
| Train Metrics | accuracy:  0.69 | loss:  0.90
| Eval  Metrics | accuracy:  0.69 | loss:  0.91
-----------------------------------------------------------------------------------------
[2024-11-20 13:44:05,530][absl][INFO] - Saving checkpoint at step: 112500
[2024-11-20 13:44:05,535][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:44:05,541][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_112500.
[2024-11-20 13:44:05,541][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_112500
[2024-11-20 13:44:05,542][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:44:05,543][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_112500
[2024-11-20 13:44:06,174][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_112500
[2024-11-20 13:44:06,176][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_112500.orbax-checkpoint-tmp-29 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_112500
[2024-11-20 13:44:06,185][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_112500`.
[2024-11-20 13:44:06,186][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_112500
[2024-11-20 13:44:06,186][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_101250
| epoch 51 | 1000/2250 batches | ms/batch 54.08 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.88
| epoch 51 | 2000/2250 batches | ms/batch 51.37 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.91
-----------------------------------------------------------------------------------------
| end of epoch  51 | time per epoch: 118.49s |
| Train Metrics | accuracy:  0.69 | loss:  0.90
| Eval  Metrics | accuracy:  0.67 | loss:  0.95
-----------------------------------------------------------------------------------------
| epoch 52 | 1000/2250 batches | ms/batch 54.20 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.88
| epoch 52 | 2000/2250 batches | ms/batch 50.99 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.90
-----------------------------------------------------------------------------------------
| end of epoch  52 | time per epoch: 118.10s |
| Train Metrics | accuracy:  0.69 | loss:  0.89
| Eval  Metrics | accuracy:  0.70 | loss:  0.86
-----------------------------------------------------------------------------------------
[2024-11-20 13:48:18,536][absl][INFO] - Saving checkpoint at step: 117000
[2024-11-20 13:48:18,543][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:48:18,548][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_117000.
[2024-11-20 13:48:18,549][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_117000
[2024-11-20 13:48:18,549][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:48:18,551][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_117000
[2024-11-20 13:48:18,688][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_117000
[2024-11-20 13:48:18,692][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_117000.orbax-checkpoint-tmp-30 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_117000
[2024-11-20 13:48:18,702][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_117000`.
[2024-11-20 13:48:18,702][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_117000
[2024-11-20 13:48:18,703][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_112500
| epoch 53 | 1000/2250 batches | ms/batch 54.70 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.86
| epoch 53 | 2000/2250 batches | ms/batch 50.94 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.90
-----------------------------------------------------------------------------------------
| end of epoch  53 | time per epoch: 118.59s |
| Train Metrics | accuracy:  0.70 | loss:  0.88
| Eval  Metrics | accuracy:  0.68 | loss:  0.92
-----------------------------------------------------------------------------------------
| epoch 54 | 1000/2250 batches | ms/batch 54.60 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.87
| epoch 54 | 2000/2250 batches | ms/batch 51.15 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.88
-----------------------------------------------------------------------------------------
| end of epoch  54 | time per epoch: 118.71s |
| Train Metrics | accuracy:  0.69 | loss:  0.88
| Eval  Metrics | accuracy:  0.70 | loss:  0.87
-----------------------------------------------------------------------------------------
[2024-11-20 13:52:31,868][absl][INFO] - Saving checkpoint at step: 121500
[2024-11-20 13:52:31,870][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 13:52:31,874][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_121500.
[2024-11-20 13:52:31,875][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_121500
[2024-11-20 13:52:31,875][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 13:52:31,877][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_121500
[2024-11-20 13:52:32,025][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_121500
[2024-11-20 13:52:32,029][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_121500.orbax-checkpoint-tmp-31 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_121500
[2024-11-20 13:52:32,040][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_121500`.
[2024-11-20 13:52:32,040][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_121500
[2024-11-20 13:52:32,041][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_117000

| epoch 55 | 1000/2250 batches | ms/batch 54.46 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.85
| epoch 55 | 2000/2250 batches | ms/batch 51.42 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.89
-----------------------------------------------------------------------------------------
| end of epoch  55 | time per epoch: 118.86s |
| Train Metrics | accuracy:  0.70 | loss:  0.88
| Eval  Metrics | accuracy:  0.69 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 56 | 1000/2250 batches | ms/batch 54.65 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.85
| epoch 56 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.88
-----------------------------------------------------------------------------------------
| end of epoch  56 | time per epoch: 119.11s |
| Train Metrics | accuracy:  0.70 | loss:  0.87
| Eval  Metrics | accuracy:  0.69 | loss:  0.90
-----------------------------------------------------------------------------------------
| epoch 57 | 1000/2250 batches | ms/batch 54.04 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
| epoch 57 | 2000/2250 batches | ms/batch 51.26 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.89
-----------------------------------------------------------------------------------------
| end of epoch  57 | time per epoch: 118.27s |
| Train Metrics | accuracy:  0.70 | loss:  0.86
| Eval  Metrics | accuracy:  0.69 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 58 | 1000/2250 batches | ms/batch 54.49 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
| epoch 58 | 2000/2250 batches | ms/batch 51.13 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.87
-----------------------------------------------------------------------------------------
| end of epoch  58 | time per epoch: 118.72s |
| Train Metrics | accuracy:  0.70 | loss:  0.86
| Eval  Metrics | accuracy:  0.70 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 59 | 1000/2250 batches | ms/batch 54.63 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.83
| epoch 59 | 2000/2250 batches | ms/batch 51.51 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.88
-----------------------------------------------------------------------------------------
| end of epoch  59 | time per epoch: 119.13s |
| Train Metrics | accuracy:  0.70 | loss:  0.85
| Eval  Metrics | accuracy:  0.69 | loss:  0.90
-----------------------------------------------------------------------------------------
| epoch 60 | 1000/2250 batches | ms/batch 54.66 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.83
| epoch 60 | 2000/2250 batches | ms/batch 51.61 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.86
-----------------------------------------------------------------------------------------
| end of epoch  60 | time per epoch: 119.52s |
| Train Metrics | accuracy:  0.71 | loss:  0.85
| Eval  Metrics | accuracy:  0.68 | loss:  0.93
-----------------------------------------------------------------------------------------
| epoch 61 | 1000/2250 batches | ms/batch 54.28 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.83
| epoch 61 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.86
-----------------------------------------------------------------------------------------
| end of epoch  61 | time per epoch: 118.60s |
| Train Metrics | accuracy:  0.71 | loss:  0.85
| Eval  Metrics | accuracy:  0.69 | loss:  0.94
-----------------------------------------------------------------------------------------
| epoch 62 | 1000/2250 batches | ms/batch 54.72 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.81
| epoch 62 | 2000/2250 batches | ms/batch 51.57 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.86
-----------------------------------------------------------------------------------------
| end of epoch  62 | time per epoch: 119.37s |
| Train Metrics | accuracy:  0.71 | loss:  0.84
| Eval  Metrics | accuracy:  0.70 | loss:  0.86
-----------------------------------------------------------------------------------------
[2024-11-20 14:09:27,368][absl][INFO] - Saving checkpoint at step: 139500
[2024-11-20 14:09:27,383][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 14:09:27,390][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_139500.
[2024-11-20 14:09:27,390][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_139500
[2024-11-20 14:09:27,391][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 14:09:27,392][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_139500
[2024-11-20 14:09:27,532][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_139500
[2024-11-20 14:09:27,534][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_139500.orbax-checkpoint-tmp-32 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_139500
[2024-11-20 14:09:27,546][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_139500`.
[2024-11-20 14:09:27,547][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_139500
[2024-11-20 14:09:27,548][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_121500
| epoch 63 | 1000/2250 batches | ms/batch 54.39 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.82
| epoch 63 | 2000/2250 batches | ms/batch 50.82 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.83
-----------------------------------------------------------------------------------------
| end of epoch  63 | time per epoch: 118.22s |
| Train Metrics | accuracy:  0.71 | loss:  0.83
| Eval  Metrics | accuracy:  0.67 | loss:  0.96
-----------------------------------------------------------------------------------------
| epoch 64 | 1000/2250 batches | ms/batch 54.66 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.81
| epoch 64 | 2000/2250 batches | ms/batch 50.86 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
-----------------------------------------------------------------------------------------
| end of epoch  64 | time per epoch: 118.40s |
| Train Metrics | accuracy:  0.71 | loss:  0.82
| Eval  Metrics | accuracy:  0.69 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 65 | 1000/2250 batches | ms/batch 54.56 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.81
| epoch 65 | 2000/2250 batches | ms/batch 51.22 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
-----------------------------------------------------------------------------------------
| end of epoch  65 | time per epoch: 118.72s |
| Train Metrics | accuracy:  0.71 | loss:  0.83
| Eval  Metrics | accuracy:  0.71 | loss:  0.83
-----------------------------------------------------------------------------------------
[2024-11-20 14:15:46,914][absl][INFO] - Saving checkpoint at step: 146250
[2024-11-20 14:15:46,920][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 14:15:46,926][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_146250.
[2024-11-20 14:15:46,927][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_146250
[2024-11-20 14:15:46,927][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 14:15:46,929][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_146250
[2024-11-20 14:15:47,066][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_146250
[2024-11-20 14:15:47,068][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_146250.orbax-checkpoint-tmp-33 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_146250
[2024-11-20 14:15:47,077][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_146250`.
[2024-11-20 14:15:47,077][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_146250
[2024-11-20 14:15:47,078][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_139500
| epoch 66 | 1000/2250 batches | ms/batch 54.58 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.80
| epoch 66 | 2000/2250 batches | ms/batch 51.14 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
-----------------------------------------------------------------------------------------
| end of epoch  66 | time per epoch: 118.74s |
| Train Metrics | accuracy:  0.72 | loss:  0.82
| Eval  Metrics | accuracy:  0.68 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 67 | 1000/2250 batches | ms/batch 54.20 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.78
| epoch 67 | 2000/2250 batches | ms/batch 51.22 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
-----------------------------------------------------------------------------------------
| end of epoch  67 | time per epoch: 118.35s |
| Train Metrics | accuracy:  0.72 | loss:  0.81
| Eval  Metrics | accuracy:  0.71 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 68 | 1000/2250 batches | ms/batch 54.47 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.78
| epoch 68 | 2000/2250 batches | ms/batch 51.18 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.84
-----------------------------------------------------------------------------------------
| end of epoch  68 | time per epoch: 118.73s |
| Train Metrics | accuracy:  0.72 | loss:  0.81
| Eval  Metrics | accuracy:  0.70 | loss:  0.86
-----------------------------------------------------------------------------------------
| epoch 69 | 1000/2250 batches | ms/batch 54.31 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.78
| epoch 69 | 2000/2250 batches | ms/batch 51.34 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.81
-----------------------------------------------------------------------------------------
| end of epoch  69 | time per epoch: 118.81s |
| Train Metrics | accuracy:  0.72 | loss:  0.80
| Eval  Metrics | accuracy:  0.69 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 70 | 1000/2250 batches | ms/batch 54.06 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.78
| epoch 70 | 2000/2250 batches | ms/batch 50.67 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.81
-----------------------------------------------------------------------------------------
| end of epoch  70 | time per epoch: 117.71s |
| Train Metrics | accuracy:  0.72 | loss:  0.80
| Eval  Metrics | accuracy:  0.71 | loss:  0.84
-----------------------------------------------------------------------------------------
| epoch 71 | 1000/2250 batches | ms/batch 54.48 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.78
| epoch 71 | 2000/2250 batches | ms/batch 51.17 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.80
-----------------------------------------------------------------------------------------
| end of epoch  71 | time per epoch: 118.59s |
| Train Metrics | accuracy:  0.73 | loss:  0.79
| Eval  Metrics | accuracy:  0.69 | loss:  0.90
-----------------------------------------------------------------------------------------
| epoch 72 | 1000/2250 batches | ms/batch 54.64 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.76
| epoch 72 | 2000/2250 batches | ms/batch 51.61 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.80
-----------------------------------------------------------------------------------------
| end of epoch  72 | time per epoch: 119.28s |
| Train Metrics | accuracy:  0.73 | loss:  0.78
| Eval  Metrics | accuracy:  0.70 | loss:  0.86
-----------------------------------------------------------------------------------------
| epoch 73 | 1000/2250 batches | ms/batch 54.45 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.75
| epoch 73 | 2000/2250 batches | ms/batch 51.40 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.81
-----------------------------------------------------------------------------------------
| end of epoch  73 | time per epoch: 118.81s |
| Train Metrics | accuracy:  0.73 | loss:  0.78
| Eval  Metrics | accuracy:  0.71 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 74 | 1000/2250 batches | ms/batch 54.34 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.75
| epoch 74 | 2000/2250 batches | ms/batch 50.74 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.78
-----------------------------------------------------------------------------------------
| end of epoch  74 | time per epoch: 118.15s |
| Train Metrics | accuracy:  0.73 | loss:  0.77
| Eval  Metrics | accuracy:  0.71 | loss:  0.83
-----------------------------------------------------------------------------------------
| epoch 75 | 1000/2250 batches | ms/batch 54.35 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.76
| epoch 75 | 2000/2250 batches | ms/batch 51.10 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.77
-----------------------------------------------------------------------------------------
| end of epoch  75 | time per epoch: 118.48s |
| Train Metrics | accuracy:  0.73 | loss:  0.77
| Eval  Metrics | accuracy:  0.69 | loss:  0.90
-----------------------------------------------------------------------------------------
| epoch 76 | 1000/2250 batches | ms/batch 53.67 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.75
| epoch 76 | 2000/2250 batches | ms/batch 50.89 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.79
-----------------------------------------------------------------------------------------
| end of epoch  76 | time per epoch: 117.48s |
| Train Metrics | accuracy:  0.73 | loss:  0.77
| Eval  Metrics | accuracy:  0.70 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 77 | 1000/2250 batches | ms/batch 54.26 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.74
| epoch 77 | 2000/2250 batches | ms/batch 51.09 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.76
-----------------------------------------------------------------------------------------
| end of epoch  77 | time per epoch: 118.26s |
| Train Metrics | accuracy:  0.74 | loss:  0.76
| Eval  Metrics | accuracy:  0.71 | loss:  0.84
-----------------------------------------------------------------------------------------
| epoch 78 | 1000/2250 batches | ms/batch 54.47 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.73
| epoch 78 | 2000/2250 batches | ms/batch 51.12 | Performance/Training accuracy:  0.73 | Performance/Training loss:  0.77
-----------------------------------------------------------------------------------------
| end of epoch  78 | time per epoch: 118.63s |
| Train Metrics | accuracy:  0.74 | loss:  0.75
| Eval  Metrics | accuracy:  0.71 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 79 | 1000/2250 batches | ms/batch 54.57 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.73
| epoch 79 | 2000/2250 batches | ms/batch 50.88 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.76
-----------------------------------------------------------------------------------------
| end of epoch  79 | time per epoch: 118.47s |
| Train Metrics | accuracy:  0.74 | loss:  0.75
| Eval  Metrics | accuracy:  0.71 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 80 | 1000/2250 batches | ms/batch 54.24 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.72
| epoch 80 | 2000/2250 batches | ms/batch 51.24 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.76
-----------------------------------------------------------------------------------------
| end of epoch  80 | time per epoch: 118.42s |
| Train Metrics | accuracy:  0.74 | loss:  0.74
| Eval  Metrics | accuracy:  0.70 | loss:  0.88
-----------------------------------------------------------------------------------------
| epoch 81 | 1000/2250 batches | ms/batch 54.52 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.72
| epoch 81 | 2000/2250 batches | ms/batch 51.26 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.75
-----------------------------------------------------------------------------------------
| end of epoch  81 | time per epoch: 118.81s |
| Train Metrics | accuracy:  0.74 | loss:  0.74
| Eval  Metrics | accuracy:  0.72 | loss:  0.82
-----------------------------------------------------------------------------------------
[2024-11-20 14:49:29,868][absl][INFO] - Saving checkpoint at step: 182250
[2024-11-20 14:49:29,872][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 14:49:29,877][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_182250.
[2024-11-20 14:49:29,877][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_182250
[2024-11-20 14:49:29,878][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 14:49:29,879][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_182250
[2024-11-20 14:49:30,019][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_182250
[2024-11-20 14:49:30,022][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_182250.orbax-checkpoint-tmp-34 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_182250
[2024-11-20 14:49:30,031][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_182250`.
[2024-11-20 14:49:30,032][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_182250
[2024-11-20 14:49:30,033][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_146250
| epoch 82 | 1000/2250 batches | ms/batch 54.58 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.71
| epoch 82 | 2000/2250 batches | ms/batch 51.10 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.75
-----------------------------------------------------------------------------------------
| end of epoch  82 | time per epoch: 118.73s |
| Train Metrics | accuracy:  0.75 | loss:  0.73
| Eval  Metrics | accuracy:  0.71 | loss:  0.84
-----------------------------------------------------------------------------------------
| epoch 83 | 1000/2250 batches | ms/batch 54.70 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.70
| epoch 83 | 2000/2250 batches | ms/batch 51.49 | Performance/Training accuracy:  0.74 | Performance/Training loss:  0.74
-----------------------------------------------------------------------------------------
| end of epoch  83 | time per epoch: 119.16s |
| Train Metrics | accuracy:  0.75 | loss:  0.73
| Eval  Metrics | accuracy:  0.71 | loss:  0.82
-----------------------------------------------------------------------------------------
| epoch 84 | 1000/2250 batches | ms/batch 53.98 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.70
| epoch 84 | 2000/2250 batches | ms/batch 51.13 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.73
-----------------------------------------------------------------------------------------
| end of epoch  84 | time per epoch: 117.95s |
| Train Metrics | accuracy:  0.75 | loss:  0.72
| Eval  Metrics | accuracy:  0.71 | loss:  0.83
-----------------------------------------------------------------------------------------
| epoch 85 | 1000/2250 batches | ms/batch 54.50 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.70
| epoch 85 | 2000/2250 batches | ms/batch 51.18 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.72
-----------------------------------------------------------------------------------------
| end of epoch  85 | time per epoch: 118.75s |
| Train Metrics | accuracy:  0.75 | loss:  0.71
| Eval  Metrics | accuracy:  0.72 | loss:  0.80
-----------------------------------------------------------------------------------------
[2024-11-20 14:57:56,989][absl][INFO] - Saving checkpoint at step: 191250
[2024-11-20 14:57:56,995][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 14:57:57,002][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_191250.
[2024-11-20 14:57:57,003][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_191250
[2024-11-20 14:57:57,003][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 14:57:57,004][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_191250
[2024-11-20 14:57:57,142][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_191250
[2024-11-20 14:57:57,145][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_191250.orbax-checkpoint-tmp-35 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_191250
[2024-11-20 14:57:57,171][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_191250`.
[2024-11-20 14:57:57,172][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_191250
[2024-11-20 14:57:57,173][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_182250
| epoch 86 | 1000/2250 batches | ms/batch 53.56 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.69
| epoch 86 | 2000/2250 batches | ms/batch 51.11 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.72
-----------------------------------------------------------------------------------------
| end of epoch  86 | time per epoch: 117.55s |
| Train Metrics | accuracy:  0.76 | loss:  0.71
| Eval  Metrics | accuracy:  0.73 | loss:  0.77
-----------------------------------------------------------------------------------------
[2024-11-20 15:00:02,700][absl][INFO] - Saving checkpoint at step: 193500
[2024-11-20 15:00:02,706][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 15:00:02,710][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_193500.
[2024-11-20 15:00:02,711][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_193500
[2024-11-20 15:00:02,711][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 15:00:02,713][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_193500
[2024-11-20 15:00:02,847][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_193500
[2024-11-20 15:00:02,851][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_193500.orbax-checkpoint-tmp-36 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_193500
[2024-11-20 15:00:02,859][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_193500`.
[2024-11-20 15:00:02,860][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_193500
[2024-11-20 15:00:02,860][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_191250
| epoch 87 | 1000/2250 batches | ms/batch 54.35 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.67
| epoch 87 | 2000/2250 batches | ms/batch 51.10 | Performance/Training accuracy:  0.75 | Performance/Training loss:  0.72
-----------------------------------------------------------------------------------------
| end of epoch  87 | time per epoch: 118.56s |
| Train Metrics | accuracy:  0.75 | loss:  0.70
| Eval  Metrics | accuracy:  0.70 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 88 | 1000/2250 batches | ms/batch 54.53 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.68
| epoch 88 | 2000/2250 batches | ms/batch 51.31 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.71
-----------------------------------------------------------------------------------------
| end of epoch  88 | time per epoch: 118.82s |
| Train Metrics | accuracy:  0.76 | loss:  0.70
| Eval  Metrics | accuracy:  0.73 | loss:  0.79
-----------------------------------------------------------------------------------------
[2024-11-20 15:04:16,310][absl][INFO] - Saving checkpoint at step: 198000
[2024-11-20 15:04:16,317][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 15:04:16,323][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_198000.
[2024-11-20 15:04:16,324][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_198000
[2024-11-20 15:04:16,325][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 15:04:16,326][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_198000
[2024-11-20 15:04:16,504][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_198000
[2024-11-20 15:04:16,506][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_198000.orbax-checkpoint-tmp-37 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_198000
[2024-11-20 15:04:16,517][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_198000`.
[2024-11-20 15:04:16,517][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_198000
[2024-11-20 15:04:16,518][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_193500
| epoch 89 | 1000/2250 batches | ms/batch 54.59 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.67
| epoch 89 | 2000/2250 batches | ms/batch 50.63 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  89 | time per epoch: 118.08s |
| Train Metrics | accuracy:  0.76 | loss:  0.69
| Eval  Metrics | accuracy:  0.73 | loss:  0.76
-----------------------------------------------------------------------------------------
[2024-11-20 15:06:22,820][absl][INFO] - Saving checkpoint at step: 200250
[2024-11-20 15:06:22,822][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 15:06:22,827][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_200250.
[2024-11-20 15:06:22,827][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_200250
[2024-11-20 15:06:22,827][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 15:06:22,829][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_200250
[2024-11-20 15:06:22,971][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_200250
[2024-11-20 15:06:22,974][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_200250.orbax-checkpoint-tmp-38 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_200250
[2024-11-20 15:06:22,986][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_200250`.
[2024-11-20 15:06:22,986][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_200250
[2024-11-20 15:06:22,987][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_198000
| epoch 90 | 1000/2250 batches | ms/batch 54.69 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.67
| epoch 90 | 2000/2250 batches | ms/batch 51.49 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  90 | time per epoch: 119.16s |
| Train Metrics | accuracy:  0.76 | loss:  0.69
| Eval  Metrics | accuracy:  0.72 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 91 | 1000/2250 batches | ms/batch 54.07 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.66
| epoch 91 | 2000/2250 batches | ms/batch 50.88 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  91 | time per epoch: 117.99s |
| Train Metrics | accuracy:  0.76 | loss:  0.68
| Eval  Metrics | accuracy:  0.73 | loss:  0.80
-----------------------------------------------------------------------------------------
| epoch 92 | 1000/2250 batches | ms/batch 54.54 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.65
| epoch 92 | 2000/2250 batches | ms/batch 51.47 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  92 | time per epoch: 119.07s |
| Train Metrics | accuracy:  0.77 | loss:  0.67
| Eval  Metrics | accuracy:  0.72 | loss:  0.81
-----------------------------------------------------------------------------------------
| epoch 93 | 1000/2250 batches | ms/batch 54.47 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.64
| epoch 93 | 2000/2250 batches | ms/batch 51.17 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.68
-----------------------------------------------------------------------------------------
| end of epoch  93 | time per epoch: 118.45s |
| Train Metrics | accuracy:  0.77 | loss:  0.66
| Eval  Metrics | accuracy:  0.72 | loss:  0.83
-----------------------------------------------------------------------------------------
| epoch 94 | 1000/2250 batches | ms/batch 54.13 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.63
| epoch 94 | 2000/2250 batches | ms/batch 50.72 | Performance/Training accuracy:  0.76 | Performance/Training loss:  0.68
-----------------------------------------------------------------------------------------
| end of epoch  94 | time per epoch: 117.67s |
| Train Metrics | accuracy:  0.77 | loss:  0.66
| Eval  Metrics | accuracy:  0.73 | loss:  0.79
-----------------------------------------------------------------------------------------
| epoch 95 | 1000/2250 batches | ms/batch 54.40 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.63
| epoch 95 | 2000/2250 batches | ms/batch 51.03 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.67
-----------------------------------------------------------------------------------------
| end of epoch  95 | time per epoch: 118.46s |
| Train Metrics | accuracy:  0.77 | loss:  0.65
| Eval  Metrics | accuracy:  0.72 | loss:  0.79
-----------------------------------------------------------------------------------------
| epoch 96 | 1000/2250 batches | ms/batch 54.41 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.63
| epoch 96 | 2000/2250 batches | ms/batch 50.18 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.67
-----------------------------------------------------------------------------------------
| end of epoch  96 | time per epoch: 117.42s |
| Train Metrics | accuracy:  0.77 | loss:  0.65
| Eval  Metrics | accuracy:  0.73 | loss:  0.79
-----------------------------------------------------------------------------------------
| epoch 97 | 1000/2250 batches | ms/batch 54.54 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.61
| epoch 97 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.67
-----------------------------------------------------------------------------------------
| end of epoch  97 | time per epoch: 118.95s |
| Train Metrics | accuracy:  0.77 | loss:  0.64
| Eval  Metrics | accuracy:  0.73 | loss:  0.78
-----------------------------------------------------------------------------------------
| epoch 98 | 1000/2250 batches | ms/batch 54.46 | Performance/Training accuracy:  0.79 | Performance/Training loss:  0.61
| epoch 98 | 2000/2250 batches | ms/batch 51.15 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.65
-----------------------------------------------------------------------------------------
| end of epoch  98 | time per epoch: 118.75s |
| Train Metrics | accuracy:  0.78 | loss:  0.63
| Eval  Metrics | accuracy:  0.73 | loss:  0.81
-----------------------------------------------------------------------------------------
| epoch 99 | 1000/2250 batches | ms/batch 53.72 | Performance/Training accuracy:  0.79 | Performance/Training loss:  0.60
| epoch 99 | 2000/2250 batches | ms/batch 50.50 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.64
-----------------------------------------------------------------------------------------
| end of epoch  99 | time per epoch: 117.08s |
| Train Metrics | accuracy:  0.78 | loss:  0.63
| Eval  Metrics | accuracy:  0.73 | loss:  0.78
-----------------------------------------------------------------------------------------
| epoch 100 | 1000/2250 batches | ms/batch 54.21 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.59
| epoch 100 | 2000/2250 batches | ms/batch 50.93 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.64
-----------------------------------------------------------------------------------------
| end of epoch 100 | time per epoch: 118.04s |
| Train Metrics | accuracy:  0.78 | loss:  0.62
| Eval  Metrics | accuracy:  0.71 | loss:  0.81
-----------------------------------------------------------------------------------------
| epoch 101 | 1000/2250 batches | ms/batch 54.55 | Performance/Training accuracy:  0.79 | Performance/Training loss:  0.59
| epoch 101 | 2000/2250 batches | ms/batch 51.13 | Performance/Training accuracy:  0.77 | Performance/Training loss:  0.64
-----------------------------------------------------------------------------------------
| end of epoch 101 | time per epoch: 118.63s |
| Train Metrics | accuracy:  0.78 | loss:  0.62
| Eval  Metrics | accuracy:  0.74 | loss:  0.76
-----------------------------------------------------------------------------------------
[2024-11-20 15:31:38,576][absl][INFO] - Saving checkpoint at step: 227250
[2024-11-20 15:31:38,583][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 15:31:38,587][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_227250.
[2024-11-20 15:31:38,588][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_227250
[2024-11-20 15:31:38,588][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 15:31:38,590][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_227250
[2024-11-20 15:31:38,728][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_227250
[2024-11-20 15:31:38,732][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_227250.orbax-checkpoint-tmp-39 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_227250
[2024-11-20 15:31:38,741][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_227250`.
[2024-11-20 15:31:38,741][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_227250
[2024-11-20 15:31:38,742][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_200250
| epoch 102 | 1000/2250 batches | ms/batch 54.41 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.57
| epoch 102 | 2000/2250 batches | ms/batch 51.08 | Performance/Training accuracy:  0.78 | Performance/Training loss:  0.63
-----------------------------------------------------------------------------------------
| end of epoch 102 | time per epoch: 118.43s |
| Train Metrics | accuracy:  0.79 | loss:  0.60
| Eval  Metrics | accuracy:  0.74 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 103 | 1000/2250 batches | ms/batch 54.66 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.57
| epoch 103 | 2000/2250 batches | ms/batch 51.03 | Performance/Training accuracy:  0.79 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 103 | time per epoch: 118.64s |
| Train Metrics | accuracy:  0.79 | loss:  0.60
| Eval  Metrics | accuracy:  0.75 | loss:  0.76
-----------------------------------------------------------------------------------------
[2024-11-20 15:35:51,878][absl][INFO] - Saving checkpoint at step: 231750
[2024-11-20 15:35:51,884][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 15:35:51,889][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_231750.
[2024-11-20 15:35:51,890][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_231750
[2024-11-20 15:35:51,890][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 15:35:51,892][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_231750
[2024-11-20 15:35:52,467][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_231750
[2024-11-20 15:35:52,469][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_231750.orbax-checkpoint-tmp-40 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_231750
[2024-11-20 15:35:52,478][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_231750`.
[2024-11-20 15:35:52,479][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_231750
[2024-11-20 15:35:52,480][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_227250
| epoch 104 | 1000/2250 batches | ms/batch 54.71 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.56
| epoch 104 | 2000/2250 batches | ms/batch 50.90 | Performance/Training accuracy:  0.79 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 104 | time per epoch: 118.68s |
| Train Metrics | accuracy:  0.80 | loss:  0.59
| Eval  Metrics | accuracy:  0.75 | loss:  0.76
-----------------------------------------------------------------------------------------
| epoch 105 | 1000/2250 batches | ms/batch 54.96 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.55
| epoch 105 | 2000/2250 batches | ms/batch 51.57 | Performance/Training accuracy:  0.79 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 105 | time per epoch: 119.70s |
| Train Metrics | accuracy:  0.80 | loss:  0.58
| Eval  Metrics | accuracy:  0.74 | loss:  0.76
-----------------------------------------------------------------------------------------
| epoch 106 | 1000/2250 batches | ms/batch 54.78 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.55
| epoch 106 | 2000/2250 batches | ms/batch 51.38 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.59
-----------------------------------------------------------------------------------------
| end of epoch 106 | time per epoch: 119.28s |
| Train Metrics | accuracy:  0.80 | loss:  0.57
| Eval  Metrics | accuracy:  0.73 | loss:  0.79
-----------------------------------------------------------------------------------------
| epoch 107 | 1000/2250 batches | ms/batch 54.94 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.54
| epoch 107 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.58
-----------------------------------------------------------------------------------------
| end of epoch 107 | time per epoch: 119.30s |
| Train Metrics | accuracy:  0.80 | loss:  0.57
| Eval  Metrics | accuracy:  0.74 | loss:  0.75
-----------------------------------------------------------------------------------------
| epoch 108 | 1000/2250 batches | ms/batch 55.01 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.54
| epoch 108 | 2000/2250 batches | ms/batch 51.26 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.57
-----------------------------------------------------------------------------------------
| end of epoch 108 | time per epoch: 119.30s |
| Train Metrics | accuracy:  0.81 | loss:  0.56
| Eval  Metrics | accuracy:  0.75 | loss:  0.75
-----------------------------------------------------------------------------------------
| epoch 109 | 1000/2250 batches | ms/batch 55.16 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.52
| epoch 109 | 2000/2250 batches | ms/batch 51.23 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.57
-----------------------------------------------------------------------------------------
| end of epoch 109 | time per epoch: 119.40s |
| Train Metrics | accuracy:  0.81 | loss:  0.55
| Eval  Metrics | accuracy:  0.75 | loss:  0.74
-----------------------------------------------------------------------------------------
| epoch 110 | 1000/2250 batches | ms/batch 54.76 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.52
| epoch 110 | 2000/2250 batches | ms/batch 51.34 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 110 | time per epoch: 119.13s |
| Train Metrics | accuracy:  0.81 | loss:  0.55
| Eval  Metrics | accuracy:  0.73 | loss:  0.79
-----------------------------------------------------------------------------------------
| epoch 111 | 1000/2250 batches | ms/batch 54.54 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.51
| epoch 111 | 2000/2250 batches | ms/batch 51.25 | Performance/Training accuracy:  0.80 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 111 | time per epoch: 118.68s |
| Train Metrics | accuracy:  0.81 | loss:  0.54
| Eval  Metrics | accuracy:  0.73 | loss:  0.81
-----------------------------------------------------------------------------------------

| epoch 112 | 1000/2250 batches | ms/batch 54.08 | Performance/Training accuracy:  0.83 | Performance/Training loss:  0.50
| epoch 112 | 2000/2250 batches | ms/batch 51.40 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 112 | time per epoch: 118.50s |
| Train Metrics | accuracy:  0.82 | loss:  0.53
| Eval  Metrics | accuracy:  0.74 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 113 | 1000/2250 batches | ms/batch 54.61 | Performance/Training accuracy:  0.83 | Performance/Training loss:  0.49
| epoch 113 | 2000/2250 batches | ms/batch 51.49 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 113 | time per epoch: 119.16s |
| Train Metrics | accuracy:  0.82 | loss:  0.52
| Eval  Metrics | accuracy:  0.74 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 114 | 1000/2250 batches | ms/batch 54.05 | Performance/Training accuracy:  0.83 | Performance/Training loss:  0.49
| epoch 114 | 2000/2250 batches | ms/batch 51.07 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.53
-----------------------------------------------------------------------------------------
| end of epoch 114 | time per epoch: 118.06s |
| Train Metrics | accuracy:  0.82 | loss:  0.52
| Eval  Metrics | accuracy:  0.74 | loss:  0.80
-----------------------------------------------------------------------------------------
| epoch 115 | 1000/2250 batches | ms/batch 54.34 | Performance/Training accuracy:  0.83 | Performance/Training loss:  0.48
| epoch 115 | 2000/2250 batches | ms/batch 50.96 | Performance/Training accuracy:  0.81 | Performance/Training loss:  0.53
-----------------------------------------------------------------------------------------
| end of epoch 115 | time per epoch: 118.35s |
| Train Metrics | accuracy:  0.82 | loss:  0.51
| Eval  Metrics | accuracy:  0.75 | loss:  0.73
-----------------------------------------------------------------------------------------
[2024-11-20 16:01:16,010][absl][INFO] - Saving checkpoint at step: 258750
[2024-11-20 16:01:16,016][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 16:01:16,021][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_258750.
[2024-11-20 16:01:16,022][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_258750
[2024-11-20 16:01:16,022][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 16:01:16,024][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_258750
[2024-11-20 16:01:16,171][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_258750
[2024-11-20 16:01:16,173][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_258750.orbax-checkpoint-tmp-41 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_258750
[2024-11-20 16:01:16,181][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_258750`.
[2024-11-20 16:01:16,181][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_258750
[2024-11-20 16:01:16,182][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_231750
| epoch 116 | 1000/2250 batches | ms/batch 54.44 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.47
| epoch 116 | 2000/2250 batches | ms/batch 51.05 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.52
-----------------------------------------------------------------------------------------
| end of epoch 116 | time per epoch: 118.48s |
| Train Metrics | accuracy:  0.83 | loss:  0.50
| Eval  Metrics | accuracy:  0.74 | loss:  0.80
-----------------------------------------------------------------------------------------
| epoch 117 | 1000/2250 batches | ms/batch 54.89 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.46
| epoch 117 | 2000/2250 batches | ms/batch 51.12 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.51
-----------------------------------------------------------------------------------------
| end of epoch 117 | time per epoch: 118.92s |
| Train Metrics | accuracy:  0.83 | loss:  0.49
| Eval  Metrics | accuracy:  0.75 | loss:  0.76
-----------------------------------------------------------------------------------------
| epoch 118 | 1000/2250 batches | ms/batch 54.85 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.44
| epoch 118 | 2000/2250 batches | ms/batch 51.37 | Performance/Training accuracy:  0.82 | Performance/Training loss:  0.51
-----------------------------------------------------------------------------------------
| end of epoch 118 | time per epoch: 119.18s |
| Train Metrics | accuracy:  0.83 | loss:  0.48
| Eval  Metrics | accuracy:  0.74 | loss:  0.83
-----------------------------------------------------------------------------------------
| epoch 119 | 1000/2250 batches | ms/batch 54.70 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.44
| epoch 119 | 2000/2250 batches | ms/batch 50.95 | Performance/Training accuracy:  0.83 | Performance/Training loss:  0.50
-----------------------------------------------------------------------------------------
| end of epoch 119 | time per epoch: 118.59s |
| Train Metrics | accuracy:  0.84 | loss:  0.47
| Eval  Metrics | accuracy:  0.75 | loss:  0.78
-----------------------------------------------------------------------------------------
| epoch 120 | 1000/2250 batches | ms/batch 54.58 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.44
| epoch 120 | 2000/2250 batches | ms/batch 51.10 | Performance/Training accuracy:  0.83 | Performance/Training loss:  0.49
-----------------------------------------------------------------------------------------
| end of epoch 120 | time per epoch: 118.74s |
| Train Metrics | accuracy:  0.84 | loss:  0.46
| Eval  Metrics | accuracy:  0.75 | loss:  0.75
-----------------------------------------------------------------------------------------
[2024-11-20 16:11:50,547][absl][INFO] - Saving checkpoint at step: 270000
[2024-11-20 16:11:50,556][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 16:11:50,563][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_270000.
[2024-11-20 16:11:50,564][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_270000
[2024-11-20 16:11:50,564][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 16:11:50,566][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_270000
[2024-11-20 16:11:50,714][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_270000
[2024-11-20 16:11:50,717][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_270000.orbax-checkpoint-tmp-42 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_270000
[2024-11-20 16:11:50,726][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_270000`.
[2024-11-20 16:11:50,727][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_270000
[2024-11-20 16:11:50,728][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_258750
| epoch 121 | 1000/2250 batches | ms/batch 54.58 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.42
| epoch 121 | 2000/2250 batches | ms/batch 51.06 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.47
-----------------------------------------------------------------------------------------
| end of epoch 121 | time per epoch: 118.66s |
| Train Metrics | accuracy:  0.84 | loss:  0.45
| Eval  Metrics | accuracy:  0.74 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 122 | 1000/2250 batches | ms/batch 54.62 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.41
| epoch 122 | 2000/2250 batches | ms/batch 51.32 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.46
-----------------------------------------------------------------------------------------
| end of epoch 122 | time per epoch: 119.01s |
| Train Metrics | accuracy:  0.85 | loss:  0.44
| Eval  Metrics | accuracy:  0.75 | loss:  0.75
-----------------------------------------------------------------------------------------
| epoch 123 | 1000/2250 batches | ms/batch 54.92 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.41
| epoch 123 | 2000/2250 batches | ms/batch 50.64 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.45
-----------------------------------------------------------------------------------------
| end of epoch 123 | time per epoch: 118.52s |
| Train Metrics | accuracy:  0.85 | loss:  0.43
| Eval  Metrics | accuracy:  0.75 | loss:  0.79
-----------------------------------------------------------------------------------------
| epoch 124 | 1000/2250 batches | ms/batch 54.92 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.39
| epoch 124 | 2000/2250 batches | ms/batch 50.85 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.46
-----------------------------------------------------------------------------------------
| end of epoch 124 | time per epoch: 118.54s |
| Train Metrics | accuracy:  0.85 | loss:  0.43
| Eval  Metrics | accuracy:  0.75 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 125 | 1000/2250 batches | ms/batch 53.75 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.38
| epoch 125 | 2000/2250 batches | ms/batch 50.10 | Performance/Training accuracy:  0.84 | Performance/Training loss:  0.44
-----------------------------------------------------------------------------------------
| end of epoch 125 | time per epoch: 116.62s |
| Train Metrics | accuracy:  0.85 | loss:  0.42
| Eval  Metrics | accuracy:  0.76 | loss:  0.76
-----------------------------------------------------------------------------------------
[2024-11-20 16:22:21,971][absl][INFO] - Saving checkpoint at step: 281250
[2024-11-20 16:22:21,978][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 16:22:21,983][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_281250.
[2024-11-20 16:22:21,984][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_281250
[2024-11-20 16:22:21,984][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 16:22:21,986][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_281250
[2024-11-20 16:22:22,115][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_281250
[2024-11-20 16:22:22,118][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_281250.orbax-checkpoint-tmp-43 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_281250
[2024-11-20 16:22:22,129][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_281250`.
[2024-11-20 16:22:22,130][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_281250
[2024-11-20 16:22:22,131][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_270000
| epoch 126 | 1000/2250 batches | ms/batch 53.63 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.39
| epoch 126 | 2000/2250 batches | ms/batch 50.19 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.42
-----------------------------------------------------------------------------------------
| end of epoch 126 | time per epoch: 116.61s |
| Train Metrics | accuracy:  0.86 | loss:  0.41
| Eval  Metrics | accuracy:  0.76 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 127 | 1000/2250 batches | ms/batch 53.89 | Performance/Training accuracy:  0.87 | Performance/Training loss:  0.38
| epoch 127 | 2000/2250 batches | ms/batch 50.22 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.42
-----------------------------------------------------------------------------------------
| end of epoch 127 | time per epoch: 116.84s |
| Train Metrics | accuracy:  0.86 | loss:  0.40
| Eval  Metrics | accuracy:  0.76 | loss:  0.79
-----------------------------------------------------------------------------------------
[2024-11-20 16:26:31,430][absl][INFO] - Saving checkpoint at step: 285750
[2024-11-20 16:26:31,436][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 16:26:31,440][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_285750.
[2024-11-20 16:26:31,441][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_285750
[2024-11-20 16:26:31,441][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 16:26:31,443][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_285750
[2024-11-20 16:26:31,620][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_285750
[2024-11-20 16:26:31,623][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_285750.orbax-checkpoint-tmp-44 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_285750
[2024-11-20 16:26:31,636][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_285750`.
[2024-11-20 16:26:31,637][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_285750
[2024-11-20 16:26:31,638][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_281250
| epoch 128 | 1000/2250 batches | ms/batch 53.85 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.35
| epoch 128 | 2000/2250 batches | ms/batch 50.44 | Performance/Training accuracy:  0.85 | Performance/Training loss:  0.42
-----------------------------------------------------------------------------------------
| end of epoch 128 | time per epoch: 117.04s |
| Train Metrics | accuracy:  0.86 | loss:  0.39
| Eval  Metrics | accuracy:  0.75 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 129 | 1000/2250 batches | ms/batch 53.65 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.35
| epoch 129 | 2000/2250 batches | ms/batch 49.97 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.40
-----------------------------------------------------------------------------------------
| end of epoch 129 | time per epoch: 116.33s |
| Train Metrics | accuracy:  0.87 | loss:  0.38
| Eval  Metrics | accuracy:  0.76 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 130 | 1000/2250 batches | ms/batch 53.20 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.35
| epoch 130 | 2000/2250 batches | ms/batch 50.25 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.39
-----------------------------------------------------------------------------------------
| end of epoch 130 | time per epoch: 116.24s |
| Train Metrics | accuracy:  0.87 | loss:  0.37
| Eval  Metrics | accuracy:  0.76 | loss:  0.76
-----------------------------------------------------------------------------------------
| epoch 131 | 1000/2250 batches | ms/batch 53.46 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.33
| epoch 131 | 2000/2250 batches | ms/batch 50.08 | Performance/Training accuracy:  0.87 | Performance/Training loss:  0.38
-----------------------------------------------------------------------------------------
| end of epoch 131 | time per epoch: 116.36s |
| Train Metrics | accuracy:  0.87 | loss:  0.36
| Eval  Metrics | accuracy:  0.75 | loss:  0.80
-----------------------------------------------------------------------------------------
| epoch 132 | 1000/2250 batches | ms/batch 53.56 | Performance/Training accuracy:  0.89 | Performance/Training loss:  0.32
| epoch 132 | 2000/2250 batches | ms/batch 50.34 | Performance/Training accuracy:  0.86 | Performance/Training loss:  0.38
-----------------------------------------------------------------------------------------
| end of epoch 132 | time per epoch: 116.68s |
| Train Metrics | accuracy:  0.87 | loss:  0.35
| Eval  Metrics | accuracy:  0.75 | loss:  0.80
-----------------------------------------------------------------------------------------
| epoch 133 | 1000/2250 batches | ms/batch 53.61 | Performance/Training accuracy:  0.89 | Performance/Training loss:  0.31
| epoch 133 | 2000/2250 batches | ms/batch 50.22 | Performance/Training accuracy:  0.87 | Performance/Training loss:  0.37
-----------------------------------------------------------------------------------------
| end of epoch 133 | time per epoch: 116.56s |
| Train Metrics | accuracy:  0.88 | loss:  0.35
| Eval  Metrics | accuracy:  0.76 | loss:  0.79
-----------------------------------------------------------------------------------------
[2024-11-20 16:38:58,695][absl][INFO] - Saving checkpoint at step: 299250
[2024-11-20 16:38:58,701][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 16:38:58,706][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_299250.
[2024-11-20 16:38:58,706][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_299250
[2024-11-20 16:38:58,707][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 16:38:58,708][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_299250
[2024-11-20 16:38:58,863][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_299250
[2024-11-20 16:38:58,866][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_299250.orbax-checkpoint-tmp-45 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_299250
[2024-11-20 16:38:58,877][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_299250`.
[2024-11-20 16:38:58,877][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_299250
[2024-11-20 16:38:58,878][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_285750
| epoch 134 | 1000/2250 batches | ms/batch 53.42 | Performance/Training accuracy:  0.89 | Performance/Training loss:  0.31
| epoch 134 | 2000/2250 batches | ms/batch 50.35 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.34
-----------------------------------------------------------------------------------------
| end of epoch 134 | time per epoch: 116.52s |
| Train Metrics | accuracy:  0.89 | loss:  0.33
| Eval  Metrics | accuracy:  0.76 | loss:  0.82
-----------------------------------------------------------------------------------------
| epoch 135 | 1000/2250 batches | ms/batch 53.83 | Performance/Training accuracy:  0.90 | Performance/Training loss:  0.30
| epoch 135 | 2000/2250 batches | ms/batch 50.12 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.35
-----------------------------------------------------------------------------------------
| end of epoch 135 | time per epoch: 116.66s |
| Train Metrics | accuracy:  0.88 | loss:  0.33
| Eval  Metrics | accuracy:  0.75 | loss:  0.86
-----------------------------------------------------------------------------------------
| epoch 136 | 1000/2250 batches | ms/batch 54.02 | Performance/Training accuracy:  0.90 | Performance/Training loss:  0.29
| epoch 136 | 2000/2250 batches | ms/batch 50.43 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.34
-----------------------------------------------------------------------------------------
| end of epoch 136 | time per epoch: 117.25s |
| Train Metrics | accuracy:  0.89 | loss:  0.32
| Eval  Metrics | accuracy:  0.75 | loss:  0.85
-----------------------------------------------------------------------------------------
| epoch 137 | 1000/2250 batches | ms/batch 53.09 | Performance/Training accuracy:  0.90 | Performance/Training loss:  0.28
| epoch 137 | 2000/2250 batches | ms/batch 50.07 | Performance/Training accuracy:  0.88 | Performance/Training loss:  0.33
-----------------------------------------------------------------------------------------
| end of epoch 137 | time per epoch: 116.11s |
| Train Metrics | accuracy:  0.89 | loss:  0.30
| Eval  Metrics | accuracy:  0.76 | loss:  0.83
-----------------------------------------------------------------------------------------
| epoch 138 | 1000/2250 batches | ms/batch 53.91 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.27
| epoch 138 | 2000/2250 batches | ms/batch 50.48 | Performance/Training accuracy:  0.89 | Performance/Training loss:  0.32
-----------------------------------------------------------------------------------------
| end of epoch 138 | time per epoch: 117.27s |
| Train Metrics | accuracy:  0.90 | loss:  0.30
| Eval  Metrics | accuracy:  0.75 | loss:  0.88
-----------------------------------------------------------------------------------------
| epoch 139 | 1000/2250 batches | ms/batch 54.02 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.27
| epoch 139 | 2000/2250 batches | ms/batch 50.70 | Performance/Training accuracy:  0.89 | Performance/Training loss:  0.31
-----------------------------------------------------------------------------------------
| end of epoch 139 | time per epoch: 117.57s |
| Train Metrics | accuracy:  0.90 | loss:  0.29
| Eval  Metrics | accuracy:  0.75 | loss:  0.87
-----------------------------------------------------------------------------------------
| epoch 140 | 1000/2250 batches | ms/batch 53.42 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.26
| epoch 140 | 2000/2250 batches | ms/batch 50.25 | Performance/Training accuracy:  0.90 | Performance/Training loss:  0.30
-----------------------------------------------------------------------------------------
| end of epoch 140 | time per epoch: 116.52s |
| Train Metrics | accuracy:  0.90 | loss:  0.28
| Eval  Metrics | accuracy:  0.76 | loss:  0.84
-----------------------------------------------------------------------------------------
| epoch 141 | 1000/2250 batches | ms/batch 54.05 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.24
| epoch 141 | 2000/2250 batches | ms/batch 50.39 | Performance/Training accuracy:  0.90 | Performance/Training loss:  0.29
-----------------------------------------------------------------------------------------
| end of epoch 141 | time per epoch: 117.37s |
| Train Metrics | accuracy:  0.91 | loss:  0.27
| Eval  Metrics | accuracy:  0.75 | loss:  0.87
-----------------------------------------------------------------------------------------
| epoch 142 | 1000/2250 batches | ms/batch 53.69 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.23
| epoch 142 | 2000/2250 batches | ms/batch 51.01 | Performance/Training accuracy:  0.90 | Performance/Training loss:  0.27
-----------------------------------------------------------------------------------------
| end of epoch 142 | time per epoch: 117.59s |
| Train Metrics | accuracy:  0.91 | loss:  0.26
| Eval  Metrics | accuracy:  0.76 | loss:  0.90
-----------------------------------------------------------------------------------------
| epoch 143 | 1000/2250 batches | ms/batch 53.33 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.23
| epoch 143 | 2000/2250 batches | ms/batch 50.21 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.26
-----------------------------------------------------------------------------------------
| end of epoch 143 | time per epoch: 116.36s |
| Train Metrics | accuracy:  0.91 | loss:  0.25
| Eval  Metrics | accuracy:  0.75 | loss:  0.90
-----------------------------------------------------------------------------------------
| epoch 144 | 1000/2250 batches | ms/batch 53.67 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.23
| epoch 144 | 2000/2250 batches | ms/batch 50.55 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.26
-----------------------------------------------------------------------------------------
| end of epoch 144 | time per epoch: 117.08s |
| Train Metrics | accuracy:  0.91 | loss:  0.25
| Eval  Metrics | accuracy:  0.76 | loss:  0.87
-----------------------------------------------------------------------------------------
| epoch 145 | 1000/2250 batches | ms/batch 53.45 | Performance/Training accuracy:  0.93 | Performance/Training loss:  0.20
| epoch 145 | 2000/2250 batches | ms/batch 50.28 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.26
-----------------------------------------------------------------------------------------
| end of epoch 145 | time per epoch: 116.62s |
| Train Metrics | accuracy:  0.92 | loss:  0.23
| Eval  Metrics | accuracy:  0.76 | loss:  0.92
-----------------------------------------------------------------------------------------
| epoch 146 | 1000/2250 batches | ms/batch 53.65 | Performance/Training accuracy:  0.93 | Performance/Training loss:  0.20
| epoch 146 | 2000/2250 batches | ms/batch 50.51 | Performance/Training accuracy:  0.91 | Performance/Training loss:  0.25
-----------------------------------------------------------------------------------------
| end of epoch 146 | time per epoch: 116.92s |
| Train Metrics | accuracy:  0.92 | loss:  0.23
| Eval  Metrics | accuracy:  0.76 | loss:  0.87
-----------------------------------------------------------------------------------------
| epoch 147 | 1000/2250 batches | ms/batch 53.50 | Performance/Training accuracy:  0.93 | Performance/Training loss:  0.19
| epoch 147 | 2000/2250 batches | ms/batch 50.32 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.24
-----------------------------------------------------------------------------------------
| end of epoch 147 | time per epoch: 116.63s |
| Train Metrics | accuracy:  0.92 | loss:  0.22
| Eval  Metrics | accuracy:  0.76 | loss:  0.89
-----------------------------------------------------------------------------------------
| epoch 148 | 1000/2250 batches | ms/batch 53.22 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.18
| epoch 148 | 2000/2250 batches | ms/batch 50.10 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.22
-----------------------------------------------------------------------------------------
| end of epoch 148 | time per epoch: 116.16s |
| Train Metrics | accuracy:  0.93 | loss:  0.20
| Eval  Metrics | accuracy:  0.76 | loss:  0.91
-----------------------------------------------------------------------------------------
| epoch 149 | 1000/2250 batches | ms/batch 53.99 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.18
| epoch 149 | 2000/2250 batches | ms/batch 50.45 | Performance/Training accuracy:  0.92 | Performance/Training loss:  0.22
-----------------------------------------------------------------------------------------
| end of epoch 149 | time per epoch: 117.25s |
| Train Metrics | accuracy:  0.93 | loss:  0.20
| Eval  Metrics | accuracy:  0.76 | loss:  0.91
-----------------------------------------------------------------------------------------
| epoch 150 | 1000/2250 batches | ms/batch 53.69 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.17
| epoch 150 | 2000/2250 batches | ms/batch 50.32 | Performance/Training accuracy:  0.93 | Performance/Training loss:  0.21
-----------------------------------------------------------------------------------------
| end of epoch 150 | time per epoch: 116.90s |
| Train Metrics | accuracy:  0.93 | loss:  0.19
| Eval  Metrics | accuracy:  0.76 | loss:  0.95
-----------------------------------------------------------------------------------------
[2024-11-20 17:14:19,942][absl][INFO] - Saving checkpoint at step: 337500
[2024-11-20 17:14:19,951][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 17:14:19,956][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_337500.
[2024-11-20 17:14:19,957][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_337500
[2024-11-20 17:14:19,957][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 17:14:19,959][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_337500
[2024-11-20 17:14:20,099][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_337500
[2024-11-20 17:14:20,102][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_337500.orbax-checkpoint-tmp-46 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_337500
[2024-11-20 17:14:20,113][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_337500`.
[2024-11-20 17:14:20,114][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_337500
[2024-11-20 17:14:20,115][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_299250
| epoch 151 | 1000/2250 batches | ms/batch 53.51 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.17
| epoch 151 | 2000/2250 batches | ms/batch 50.51 | Performance/Training accuracy:  0.93 | Performance/Training loss:  0.20
-----------------------------------------------------------------------------------------
| end of epoch 151 | time per epoch: 116.90s |
| Train Metrics | accuracy:  0.94 | loss:  0.19
| Eval  Metrics | accuracy:  0.76 | loss:  0.93
-----------------------------------------------------------------------------------------
| epoch 152 | 1000/2250 batches | ms/batch 53.97 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.16
| epoch 152 | 2000/2250 batches | ms/batch 50.67 | Performance/Training accuracy:  0.93 | Performance/Training loss:  0.19
-----------------------------------------------------------------------------------------
| end of epoch 152 | time per epoch: 117.54s |
| Train Metrics | accuracy:  0.94 | loss:  0.18
| Eval  Metrics | accuracy:  0.76 | loss:  0.93
-----------------------------------------------------------------------------------------
[2024-11-20 17:18:30,022][absl][INFO] - Saving checkpoint at step: 342000
[2024-11-20 17:18:30,028][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 17:18:30,033][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_342000.
[2024-11-20 17:18:30,034][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_342000
[2024-11-20 17:18:30,034][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 17:18:30,036][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_342000
[2024-11-20 17:18:30,190][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_342000
[2024-11-20 17:18:30,192][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_342000.orbax-checkpoint-tmp-47 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_342000
[2024-11-20 17:18:30,200][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_342000`.
[2024-11-20 17:18:30,200][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_342000
[2024-11-20 17:18:30,201][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_337500
| epoch 153 | 1000/2250 batches | ms/batch 53.52 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.16
| epoch 153 | 2000/2250 batches | ms/batch 50.43 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.18
-----------------------------------------------------------------------------------------
| end of epoch 153 | time per epoch: 116.81s |
| Train Metrics | accuracy:  0.94 | loss:  0.17
| Eval  Metrics | accuracy:  0.76 | loss:  0.95
-----------------------------------------------------------------------------------------
| epoch 154 | 1000/2250 batches | ms/batch 53.58 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.15
| epoch 154 | 2000/2250 batches | ms/batch 50.30 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.17
-----------------------------------------------------------------------------------------
| end of epoch 154 | time per epoch: 116.65s |
| Train Metrics | accuracy:  0.94 | loss:  0.16
| Eval  Metrics | accuracy:  0.76 | loss:  0.93
-----------------------------------------------------------------------------------------
[2024-11-20 17:22:39,376][absl][INFO] - Saving checkpoint at step: 346500
[2024-11-20 17:22:39,419][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 17:22:39,425][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_346500.
[2024-11-20 17:22:39,426][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_346500
[2024-11-20 17:22:39,427][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 17:22:39,429][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_346500
[2024-11-20 17:22:39,597][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_346500
[2024-11-20 17:22:39,603][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_346500.orbax-checkpoint-tmp-48 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_346500
[2024-11-20 17:22:39,623][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_346500`.
[2024-11-20 17:22:39,624][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_346500
[2024-11-20 17:22:39,626][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_342000
| epoch 155 | 1000/2250 batches | ms/batch 53.33 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.14
| epoch 155 | 2000/2250 batches | ms/batch 50.16 | Performance/Training accuracy:  0.94 | Performance/Training loss:  0.17
-----------------------------------------------------------------------------------------
| end of epoch 155 | time per epoch: 116.24s |
| Train Metrics | accuracy:  0.95 | loss:  0.15
| Eval  Metrics | accuracy:  0.76 | loss:  0.96
-----------------------------------------------------------------------------------------
| epoch 156 | 1000/2250 batches | ms/batch 53.67 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.12
| epoch 156 | 2000/2250 batches | ms/batch 50.60 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.16
-----------------------------------------------------------------------------------------
| end of epoch 156 | time per epoch: 117.13s |
| Train Metrics | accuracy:  0.95 | loss:  0.14
| Eval  Metrics | accuracy:  0.76 | loss:  0.97
-----------------------------------------------------------------------------------------
| epoch 157 | 1000/2250 batches | ms/batch 53.61 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.13
| epoch 157 | 2000/2250 batches | ms/batch 50.56 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.15
-----------------------------------------------------------------------------------------
| end of epoch 157 | time per epoch: 116.90s |
| Train Metrics | accuracy:  0.95 | loss:  0.14
| Eval  Metrics | accuracy:  0.78 | loss:  0.96
-----------------------------------------------------------------------------------------
[2024-11-20 17:28:53,629][absl][INFO] - Saving checkpoint at step: 353250
[2024-11-20 17:28:53,636][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 17:28:53,641][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_353250.
[2024-11-20 17:28:53,642][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_353250
[2024-11-20 17:28:53,642][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 17:28:53,644][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_353250
[2024-11-20 17:28:53,810][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_353250
[2024-11-20 17:28:53,812][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_353250.orbax-checkpoint-tmp-49 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_353250
[2024-11-20 17:28:53,823][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_353250`.
[2024-11-20 17:28:53,823][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_353250
[2024-11-20 17:28:53,824][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_346500
| epoch 158 | 1000/2250 batches | ms/batch 53.72 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.12
| epoch 158 | 2000/2250 batches | ms/batch 50.43 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.14
-----------------------------------------------------------------------------------------
| end of epoch 158 | time per epoch: 116.95s |
| Train Metrics | accuracy:  0.96 | loss:  0.13
| Eval  Metrics | accuracy:  0.76 | loss:  0.99
-----------------------------------------------------------------------------------------
| epoch 159 | 1000/2250 batches | ms/batch 53.82 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.12
| epoch 159 | 2000/2250 batches | ms/batch 50.56 | Performance/Training accuracy:  0.95 | Performance/Training loss:  0.14
-----------------------------------------------------------------------------------------
| end of epoch 159 | time per epoch: 117.11s |
| Train Metrics | accuracy:  0.96 | loss:  0.13
| Eval  Metrics | accuracy:  0.77 | loss:  0.97
-----------------------------------------------------------------------------------------
| epoch 160 | 1000/2250 batches | ms/batch 53.92 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.11

| epoch 160 | 2000/2250 batches | ms/batch 50.14 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.13
-----------------------------------------------------------------------------------------
| end of epoch 160 | time per epoch: 116.81s |
| Train Metrics | accuracy:  0.96 | loss:  0.12
| Eval  Metrics | accuracy:  0.76 | loss:  1.06
-----------------------------------------------------------------------------------------
| epoch 161 | 1000/2250 batches | ms/batch 53.68 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.10
| epoch 161 | 2000/2250 batches | ms/batch 50.73 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.12
-----------------------------------------------------------------------------------------
| end of epoch 161 | time per epoch: 117.28s |
| Train Metrics | accuracy:  0.96 | loss:  0.11
| Eval  Metrics | accuracy:  0.77 | loss:  1.03
-----------------------------------------------------------------------------------------
| epoch 162 | 1000/2250 batches | ms/batch 54.04 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.10
| epoch 162 | 2000/2250 batches | ms/batch 50.57 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.11
-----------------------------------------------------------------------------------------
| end of epoch 162 | time per epoch: 117.46s |
| Train Metrics | accuracy:  0.96 | loss:  0.11
| Eval  Metrics | accuracy:  0.77 | loss:  1.04
-----------------------------------------------------------------------------------------
| epoch 163 | 1000/2250 batches | ms/batch 53.92 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.10
| epoch 163 | 2000/2250 batches | ms/batch 50.60 | Performance/Training accuracy:  0.96 | Performance/Training loss:  0.11
-----------------------------------------------------------------------------------------
| end of epoch 163 | time per epoch: 117.35s |
| Train Metrics | accuracy:  0.96 | loss:  0.10
| Eval  Metrics | accuracy:  0.77 | loss:  1.02
-----------------------------------------------------------------------------------------
| epoch 164 | 1000/2250 batches | ms/batch 53.83 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.10
| epoch 164 | 2000/2250 batches | ms/batch 50.67 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.10
-----------------------------------------------------------------------------------------
| end of epoch 164 | time per epoch: 117.32s |
| Train Metrics | accuracy:  0.97 | loss:  0.10
| Eval  Metrics | accuracy:  0.76 | loss:  1.08
-----------------------------------------------------------------------------------------
| epoch 165 | 1000/2250 batches | ms/batch 53.75 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.08
| epoch 165 | 2000/2250 batches | ms/batch 50.54 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.09
-----------------------------------------------------------------------------------------
| end of epoch 165 | time per epoch: 117.12s |
| Train Metrics | accuracy:  0.97 | loss:  0.09
| Eval  Metrics | accuracy:  0.76 | loss:  1.07
-----------------------------------------------------------------------------------------
| epoch 166 | 1000/2250 batches | ms/batch 53.91 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.08
| epoch 166 | 2000/2250 batches | ms/batch 50.41 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.09
-----------------------------------------------------------------------------------------
| end of epoch 166 | time per epoch: 117.08s |
| Train Metrics | accuracy:  0.97 | loss:  0.08
| Eval  Metrics | accuracy:  0.76 | loss:  1.09
-----------------------------------------------------------------------------------------
| epoch 167 | 1000/2250 batches | ms/batch 53.81 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.07
| epoch 167 | 2000/2250 batches | ms/batch 50.36 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.08
-----------------------------------------------------------------------------------------
| end of epoch 167 | time per epoch: 116.91s |
| Train Metrics | accuracy:  0.97 | loss:  0.08
| Eval  Metrics | accuracy:  0.76 | loss:  1.10
-----------------------------------------------------------------------------------------
| epoch 168 | 1000/2250 batches | ms/batch 53.61 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.07
| epoch 168 | 2000/2250 batches | ms/batch 50.29 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.08
-----------------------------------------------------------------------------------------
| end of epoch 168 | time per epoch: 116.73s |
| Train Metrics | accuracy:  0.98 | loss:  0.08
| Eval  Metrics | accuracy:  0.77 | loss:  1.11
-----------------------------------------------------------------------------------------
| epoch 169 | 1000/2250 batches | ms/batch 53.80 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.07
| epoch 169 | 2000/2250 batches | ms/batch 50.16 | Performance/Training accuracy:  0.97 | Performance/Training loss:  0.08
-----------------------------------------------------------------------------------------
| end of epoch 169 | time per epoch: 116.75s |
| Train Metrics | accuracy:  0.98 | loss:  0.07
| Eval  Metrics | accuracy:  0.77 | loss:  1.12
-----------------------------------------------------------------------------------------
| epoch 170 | 1000/2250 batches | ms/batch 53.89 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.06
| epoch 170 | 2000/2250 batches | ms/batch 50.55 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.07
-----------------------------------------------------------------------------------------
| end of epoch 170 | time per epoch: 117.31s |
| Train Metrics | accuracy:  0.98 | loss:  0.06
| Eval  Metrics | accuracy:  0.77 | loss:  1.11
-----------------------------------------------------------------------------------------
| epoch 171 | 1000/2250 batches | ms/batch 54.10 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.06
| epoch 171 | 2000/2250 batches | ms/batch 50.47 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.06
-----------------------------------------------------------------------------------------
| end of epoch 171 | time per epoch: 117.37s |
| Train Metrics | accuracy:  0.98 | loss:  0.06
| Eval  Metrics | accuracy:  0.77 | loss:  1.13
-----------------------------------------------------------------------------------------
| epoch 172 | 1000/2250 batches | ms/batch 54.00 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.05
| epoch 172 | 2000/2250 batches | ms/batch 50.40 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.06
-----------------------------------------------------------------------------------------
| end of epoch 172 | time per epoch: 117.26s |
| Train Metrics | accuracy:  0.98 | loss:  0.06
| Eval  Metrics | accuracy:  0.77 | loss:  1.15
-----------------------------------------------------------------------------------------
| epoch 173 | 1000/2250 batches | ms/batch 53.63 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.05
| epoch 173 | 2000/2250 batches | ms/batch 50.33 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.05
-----------------------------------------------------------------------------------------
| end of epoch 173 | time per epoch: 116.76s |
| Train Metrics | accuracy:  0.98 | loss:  0.05
| Eval  Metrics | accuracy:  0.77 | loss:  1.16
-----------------------------------------------------------------------------------------
| epoch 174 | 1000/2250 batches | ms/batch 53.84 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.05
| epoch 174 | 2000/2250 batches | ms/batch 50.49 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.05
-----------------------------------------------------------------------------------------
| end of epoch 174 | time per epoch: 117.19s |
| Train Metrics | accuracy:  0.98 | loss:  0.05
| Eval  Metrics | accuracy:  0.77 | loss:  1.16
-----------------------------------------------------------------------------------------
| epoch 175 | 1000/2250 batches | ms/batch 53.44 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.04
| epoch 175 | 2000/2250 batches | ms/batch 50.16 | Performance/Training accuracy:  0.98 | Performance/Training loss:  0.05
-----------------------------------------------------------------------------------------
| end of epoch 175 | time per epoch: 116.33s |
| Train Metrics | accuracy:  0.99 | loss:  0.05
| Eval  Metrics | accuracy:  0.77 | loss:  1.18
-----------------------------------------------------------------------------------------
| epoch 176 | 1000/2250 batches | ms/batch 53.58 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.04
| epoch 176 | 2000/2250 batches | ms/batch 50.51 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.04
-----------------------------------------------------------------------------------------
| end of epoch 176 | time per epoch: 116.87s |
| Train Metrics | accuracy:  0.99 | loss:  0.04
| Eval  Metrics | accuracy:  0.77 | loss:  1.18
-----------------------------------------------------------------------------------------
| epoch 177 | 1000/2250 batches | ms/batch 53.98 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.04
| epoch 177 | 2000/2250 batches | ms/batch 50.10 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.04
-----------------------------------------------------------------------------------------
| end of epoch 177 | time per epoch: 116.80s |
| Train Metrics | accuracy:  0.99 | loss:  0.04
| Eval  Metrics | accuracy:  0.77 | loss:  1.19
-----------------------------------------------------------------------------------------
| epoch 178 | 1000/2250 batches | ms/batch 54.02 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
| epoch 178 | 2000/2250 batches | ms/batch 50.56 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.04
-----------------------------------------------------------------------------------------
| end of epoch 178 | time per epoch: 117.34s |
| Train Metrics | accuracy:  0.99 | loss:  0.04
| Eval  Metrics | accuracy:  0.77 | loss:  1.21
-----------------------------------------------------------------------------------------
| epoch 179 | 1000/2250 batches | ms/batch 53.34 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
| epoch 179 | 2000/2250 batches | ms/batch 50.38 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
-----------------------------------------------------------------------------------------
| end of epoch 179 | time per epoch: 116.51s |
| Train Metrics | accuracy:  0.99 | loss:  0.03
| Eval  Metrics | accuracy:  0.77 | loss:  1.19
-----------------------------------------------------------------------------------------
| epoch 180 | 1000/2250 batches | ms/batch 54.36 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
| epoch 180 | 2000/2250 batches | ms/batch 50.70 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
-----------------------------------------------------------------------------------------
| end of epoch 180 | time per epoch: 117.86s |
| Train Metrics | accuracy:  0.99 | loss:  0.03
| Eval  Metrics | accuracy:  0.77 | loss:  1.21
-----------------------------------------------------------------------------------------
| epoch 181 | 1000/2250 batches | ms/batch 53.57 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
| epoch 181 | 2000/2250 batches | ms/batch 49.85 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
-----------------------------------------------------------------------------------------
| end of epoch 181 | time per epoch: 116.07s |
| Train Metrics | accuracy:  0.99 | loss:  0.03
| Eval  Metrics | accuracy:  0.78 | loss:  1.18
-----------------------------------------------------------------------------------------
[2024-11-20 18:18:53,020][absl][INFO] - Saving checkpoint at step: 407250
[2024-11-20 18:18:53,028][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 18:18:53,034][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_407250.
[2024-11-20 18:18:53,035][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_407250
[2024-11-20 18:18:53,036][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 18:18:53,038][absl][INFO] - Wrote CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1732123133037124118, commit_timestamp_nsecs=None), json={"init_timestamp_nsecs": 1732123133037124118, "commit_timestamp_nsecs": null} to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_407250.orbax-checkpoint-tmp-50
[2024-11-20 18:18:53,038][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_407250
[2024-11-20 18:18:53,203][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_407250
[2024-11-20 18:18:53,205][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_407250.orbax-checkpoint-tmp-50 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_407250
[2024-11-20 18:18:53,215][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_407250`.
[2024-11-20 18:18:53,215][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_407250
[2024-11-20 18:18:53,216][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_353250
| epoch 182 | 1000/2250 batches | ms/batch 54.09 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
| epoch 182 | 2000/2250 batches | ms/batch 50.16 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
-----------------------------------------------------------------------------------------
| end of epoch 182 | time per epoch: 117.01s |
| Train Metrics | accuracy:  0.99 | loss:  0.03
| Eval  Metrics | accuracy:  0.78 | loss:  1.21
-----------------------------------------------------------------------------------------
[2024-11-20 18:20:58,242][absl][INFO] - Saving checkpoint at step: 409500
[2024-11-20 18:20:58,249][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 18:20:58,254][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_409500.
[2024-11-20 18:20:58,255][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_409500
[2024-11-20 18:20:58,255][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 18:20:58,257][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_409500
[2024-11-20 18:20:58,391][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_409500
[2024-11-20 18:20:58,393][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_409500.orbax-checkpoint-tmp-51 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_409500
[2024-11-20 18:20:58,407][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_409500`.
[2024-11-20 18:20:58,407][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_409500
[2024-11-20 18:20:58,408][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_407250
| epoch 183 | 1000/2250 batches | ms/batch 53.60 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 183 | 2000/2250 batches | ms/batch 49.90 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.03
-----------------------------------------------------------------------------------------
| end of epoch 183 | time per epoch: 116.25s |
| Train Metrics | accuracy:  0.99 | loss:  0.03
| Eval  Metrics | accuracy:  0.78 | loss:  1.20
-----------------------------------------------------------------------------------------
| epoch 184 | 1000/2250 batches | ms/batch 53.94 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 184 | 2000/2250 batches | ms/batch 50.30 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 184 | time per epoch: 116.99s |
| Train Metrics | accuracy:  0.99 | loss:  0.02
| Eval  Metrics | accuracy:  0.78 | loss:  1.22
-----------------------------------------------------------------------------------------
[2024-11-20 18:25:07,680][absl][INFO] - Saving checkpoint at step: 414000
[2024-11-20 18:25:07,686][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 18:25:07,691][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_414000.
[2024-11-20 18:25:07,692][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_414000
[2024-11-20 18:25:07,692][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 18:25:07,694][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_414000
[2024-11-20 18:25:07,847][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_414000
[2024-11-20 18:25:07,850][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_414000.orbax-checkpoint-tmp-52 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_414000
[2024-11-20 18:25:07,861][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_414000`.
[2024-11-20 18:25:07,861][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_414000
[2024-11-20 18:25:07,863][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_409500
| epoch 185 | 1000/2250 batches | ms/batch 53.29 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 185 | 2000/2250 batches | ms/batch 49.99 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 185 | time per epoch: 115.94s |
| Train Metrics | accuracy:  0.99 | loss:  0.02
| Eval  Metrics | accuracy:  0.77 | loss:  1.25
-----------------------------------------------------------------------------------------
| epoch 186 | 1000/2250 batches | ms/batch 53.89 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 186 | 2000/2250 batches | ms/batch 50.17 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 186 | time per epoch: 116.83s |
| Train Metrics | accuracy:  0.99 | loss:  0.02
| Eval  Metrics | accuracy:  0.78 | loss:  1.24
-----------------------------------------------------------------------------------------
[2024-11-20 18:29:16,363][absl][INFO] - Saving checkpoint at step: 418500
[2024-11-20 18:29:16,374][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-11-20 18:29:16,378][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500.
[2024-11-20 18:29:16,379][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_418500
[2024-11-20 18:29:16,379][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-11-20 18:29:16,381][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_418500
[2024-11-20 18:29:16,513][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_418500
[2024-11-20 18:29:16,515][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500.orbax-checkpoint-tmp-53 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500
[2024-11-20 18:29:16,526][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500`.
[2024-11-20 18:29:16,526][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_418500
[2024-11-20 18:29:16,527][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-11-20-11-58-09/checkpoints/checkpoint_414000
| epoch 187 | 1000/2250 batches | ms/batch 53.35 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 187 | 2000/2250 batches | ms/batch 49.62 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 187 | time per epoch: 115.62s |
| Train Metrics | accuracy:  0.99 | loss:  0.02
| Eval  Metrics | accuracy:  0.78 | loss:  1.27
-----------------------------------------------------------------------------------------
| epoch 188 | 1000/2250 batches | ms/batch 53.22 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 188 | 2000/2250 batches | ms/batch 49.93 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 188 | time per epoch: 115.88s |
| Train Metrics | accuracy:  0.99 | loss:  0.02
| Eval  Metrics | accuracy:  0.78 | loss:  1.24
-----------------------------------------------------------------------------------------
| epoch 189 | 1000/2250 batches | ms/batch 53.51 | Performance/Training accuracy:  0.99 | Performance/Training loss:  0.02
| epoch 189 | 2000/2250 batches | ms/batch 50.05 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 189 | time per epoch: 116.29s |
| Train Metrics | accuracy:  1.00 | loss:  0.02
| Eval  Metrics | accuracy:  0.78 | loss:  1.23
-----------------------------------------------------------------------------------------
| epoch 190 | 1000/2250 batches | ms/batch 53.52 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.02
| epoch 190 | 2000/2250 batches | ms/batch 50.12 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.02
-----------------------------------------------------------------------------------------
| end of epoch 190 | time per epoch: 116.40s |
| Train Metrics | accuracy:  1.00 | loss:  0.02
| Eval  Metrics | accuracy:  0.78 | loss:  1.24
-----------------------------------------------------------------------------------------
| epoch 191 | 1000/2250 batches | ms/batch 53.83 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.02
| epoch 191 | 2000/2250 batches | ms/batch 50.33 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 191 | time per epoch: 116.88s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.26
-----------------------------------------------------------------------------------------
| epoch 192 | 1000/2250 batches | ms/batch 53.38 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 192 | 2000/2250 batches | ms/batch 50.28 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 192 | time per epoch: 116.45s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.26
-----------------------------------------------------------------------------------------
| epoch 193 | 1000/2250 batches | ms/batch 53.63 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.02
| epoch 193 | 2000/2250 batches | ms/batch 50.09 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 193 | time per epoch: 116.47s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.26
-----------------------------------------------------------------------------------------
| epoch 194 | 1000/2250 batches | ms/batch 53.40 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 194 | 2000/2250 batches | ms/batch 50.10 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 194 | time per epoch: 116.27s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.26
-----------------------------------------------------------------------------------------
| epoch 195 | 1000/2250 batches | ms/batch 53.85 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 195 | 2000/2250 batches | ms/batch 49.88 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 195 | time per epoch: 116.43s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.26
-----------------------------------------------------------------------------------------
| epoch 196 | 1000/2250 batches | ms/batch 53.39 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 196 | 2000/2250 batches | ms/batch 50.10 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 196 | time per epoch: 116.21s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.27
-----------------------------------------------------------------------------------------
| epoch 197 | 1000/2250 batches | ms/batch 53.69 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01

| epoch 197 | 2000/2250 batches | ms/batch 49.90 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 197 | time per epoch: 116.31s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.27
-----------------------------------------------------------------------------------------
| epoch 198 | 1000/2250 batches | ms/batch 53.37 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 198 | 2000/2250 batches | ms/batch 50.05 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 198 | time per epoch: 116.05s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.26
-----------------------------------------------------------------------------------------
| epoch 199 | 1000/2250 batches | ms/batch 52.38 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 199 | 2000/2250 batches | ms/batch 49.33 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 199 | time per epoch: 114.21s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.27
-----------------------------------------------------------------------------------------
| epoch 200 | 1000/2250 batches | ms/batch 52.47 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
| epoch 200 | 2000/2250 batches | ms/batch 49.26 | Performance/Training accuracy:  1.00 | Performance/Training loss:  0.01
-----------------------------------------------------------------------------------------
| end of epoch 200 | time per epoch: 114.24s |
| Train Metrics | accuracy:  1.00 | loss:  0.01
| Eval  Metrics | accuracy:  0.78 | loss:  1.27
-----------------------------------------------------------------------------------------
[2024-11-20 18:58:12,523][absl][INFO] - Restoring orbax checkpoint from outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500
[2024-11-20 18:58:12,540][absl][INFO] - Restoring item from outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500.
[2024-11-20 18:58:12,980][absl][WARNING] - The transformations API will eventually be replaced by an upgraded design. The current API will not be removed until this point, but it will no longer be actively worked on.
[2024-11-20 18:58:12,988][absl][INFO] - Finished restoring checkpoint from outputs/2024-11-20-11-58-09/checkpoints/checkpoint_418500.
[2024-11-20 18:58:12,988][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:restore.checkpoint_418500
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 30 -> 48 -> 30
-----------------------------------------------------------------------------------------
| End of Training |
| Test  Metrics |  accuracy:  0.78 |  loss:  1.24
-----------------------------------------------------------------------------------------