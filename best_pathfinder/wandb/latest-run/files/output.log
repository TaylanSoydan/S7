/apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/anaconda3-2023.03-1-emayrkyj4zgh57gt37ztn55cwzrrhstk/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
[*] Running training...
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/jaxvenv/lib/python3.10/site-packages/jax/_src/lax/lax.py:2785: ComplexWarning: Casting complex values to real discards the imaginary part
  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
| epoch 1 | 1000/10000 batches | ms/batch 30.04 | Performance/Training accuracy:  0.51 | Performance/Training loss: 210.07
| epoch 1 | 2000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss: 236.51
| epoch 1 | 3000/10000 batches | ms/batch  9.52 | Performance/Training accuracy:  0.51 | Performance/Training loss: 148.02

| epoch 1 | 4000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss: 97.04
| epoch 1 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss: 26.22
| epoch 1 | 6000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss: 12.95
| epoch 1 | 7000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss: 20.02
| epoch 1 | 8000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.51 | Performance/Training loss: 19.40
| epoch 1 | 9000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss: 42.28
/apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/anaconda3-2023.03-1-emayrkyj4zgh57gt37ztn55cwzrrhstk/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
| epoch 1 | 10000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss: 25.17
-----------------------------------------------------------------------------------------
| end of epoch   1 | time per epoch: 112.90s |
| Train Metrics | accuracy:  0.50 | loss: 83.77
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
/apps/opt/spack/linux-ubuntu20.04-x86_64/gcc-9.3.0/anaconda3-2023.03-1-emayrkyj4zgh57gt37ztn55cwzrrhstk/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.
  self.pid = os.fork()
| Eval  Metrics | accuracy:  0.50 | loss: 14437.84
-----------------------------------------------------------------------------------------
[2024-10-02 04:26:33,680][absl][INFO] - Saving checkpoint at step: 10000
[2024-10-02 04:26:33,684][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 04:26:33,686][absl][WARNING] - SaveArgs.aggregate is deprecated, please use custom TypeHandler (https://orbax.readthedocs.io/en/latest/custom_handlers.html#typehandler) or contact Orbax team to migrate before August 1st, 2024.
[2024-10-02 04:26:33,688][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000.
[2024-10-02 04:26:33,690][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_10000
[2024-10-02 04:26:33,690][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 04:26:33,692][absl][INFO] - Wrote CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1727835993691737946, commit_timestamp_nsecs=None), json={"init_timestamp_nsecs": 1727835993691737946, "commit_timestamp_nsecs": null} to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000.orbax-checkpoint-tmp-0
[2024-10-02 04:26:33,693][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_10000
[2024-10-02 04:26:33,816][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_10000
[2024-10-02 04:26:33,825][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000.orbax-checkpoint-tmp-0 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000
[2024-10-02 04:26:33,832][absl][INFO] - Read CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1727835993691737946, commit_timestamp_nsecs=None) from /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000
[2024-10-02 04:26:33,842][absl][INFO] - Updated CheckpointMetadata=CheckpointMetadata(init_timestamp_nsecs=1727835993691737946, commit_timestamp_nsecs=1727835993827252443) to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000
[2024-10-02 04:26:33,843][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000`.
[2024-10-02 04:26:33,843][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_10000
| epoch 2 | 1000/10000 batches | ms/batch 15.75 | Performance/Training accuracy:  0.50 | Performance/Training loss: 10.75
| epoch 2 | 2000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  5.29
| epoch 2 | 3000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.41
| epoch 2 | 4000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.49 | Performance/Training loss: 29.96
| epoch 2 | 5000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss: 34.60
| epoch 2 | 6000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  4.51
| epoch 2 | 7000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  2.06
| epoch 2 | 8000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.40
| epoch 2 | 9000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.50 | Performance/Training loss:  8.78
| epoch 2 | 10000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss: 33.62
-----------------------------------------------------------------------------------------
| end of epoch   2 | time per epoch: 98.46s |
| Train Metrics | accuracy:  0.50 | loss: 13.24
| Eval  Metrics | accuracy:  0.50 | loss: 28785424465920.00
-----------------------------------------------------------------------------------------
| epoch 3 | 1000/10000 batches | ms/batch 15.66 | Performance/Training accuracy:  0.50 | Performance/Training loss:  6.77
| epoch 3 | 2000/10000 batches | ms/batch  8.63 | Performance/Training accuracy:  0.50 | Performance/Training loss:  6.32
| epoch 3 | 3000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.50 | Performance/Training loss:  3.39
| epoch 3 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.82
| epoch 3 | 5000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.82

| epoch 3 | 6000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.50 | Performance/Training loss:  5.53
| epoch 3 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  2.19
| epoch 3 | 8000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.14
| epoch 3 | 9000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  4.59
| epoch 3 | 10000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  3.03
-----------------------------------------------------------------------------------------
| end of epoch   3 | time per epoch: 97.91s |
| Train Metrics | accuracy:  0.50 | loss:  3.46
| Eval  Metrics | accuracy:  0.50 | loss: 6856935211008.00
-----------------------------------------------------------------------------------------
| epoch 4 | 1000/10000 batches | ms/batch 15.67 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.81
| epoch 4 | 2000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.79
| epoch 4 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.97
| epoch 4 | 4000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  5.65
| epoch 4 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.30
| epoch 4 | 6000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.13
| epoch 4 | 7000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.14
| epoch 4 | 8000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.83
| epoch 4 | 9000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.95
| epoch 4 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.78
-----------------------------------------------------------------------------------------
| end of epoch   4 | time per epoch: 98.97s |
| Train Metrics | accuracy:  0.50 | loss:  1.43
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 5 | 1000/10000 batches | ms/batch 15.15 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.75
| epoch 5 | 2000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.73
| epoch 5 | 3000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.00
| epoch 5 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.77
| epoch 5 | 5000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.72
| epoch 5 | 6000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 5 | 7000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 5 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.77
| epoch 5 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 5 | 10000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  1.29
-----------------------------------------------------------------------------------------
| end of epoch   5 | time per epoch: 98.92s |
| Train Metrics | accuracy:  0.50 | loss:  0.82
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 6 | 1000/10000 batches | ms/batch 15.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.86
| epoch 6 | 2000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.71
| epoch 6 | 3000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.71
| epoch 6 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 6 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 6 | 6000/10000 batches | ms/batch  9.57 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.92
| epoch 6 | 7000/10000 batches | ms/batch  9.67 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.83
| epoch 6 | 8000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 6 | 9000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 6 | 10000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.72
-----------------------------------------------------------------------------------------
| end of epoch   6 | time per epoch: 99.76s |
| Train Metrics | accuracy:  0.50 | loss:  0.76
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 7 | 1000/10000 batches | ms/batch 15.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 7 | 2000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 7 | 3000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 7 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 7 | 5000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.76
| epoch 7 | 6000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 7 | 7000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 7 | 8000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.74
| epoch 7 | 9000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 7 | 10000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch   7 | time per epoch: 98.43s |
| Train Metrics | accuracy:  0.50 | loss:  0.71
| Eval  Metrics | accuracy:  0.50 | loss: 32.16
-----------------------------------------------------------------------------------------
| epoch 8 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 8 | 2000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 8 | 3000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 8 | 4000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 8 | 5000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 8 | 6000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.74
| epoch 8 | 7000/10000 batches | ms/batch  9.59 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.71
| epoch 8 | 8000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 8 | 9000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 8 | 10000/10000 batches | ms/batch  9.57 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch   8 | time per epoch: 99.99s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 4342460.50
-----------------------------------------------------------------------------------------
| epoch 9 | 1000/10000 batches | ms/batch 15.97 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 9 | 2000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 9 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 9 | 4000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 9 | 5000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 9 | 6000/10000 batches | ms/batch  9.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 9 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 9 | 8000/10000 batches | ms/batch  9.51 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 9 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 9 | 10000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch   9 | time per epoch: 99.94s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 10 | 1000/10000 batches | ms/batch 15.64 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 10 | 2000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 10 | 3000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 10 | 4000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 10 | 5000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 10 | 6000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 10 | 7000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 10 | 8000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 10 | 9000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 10 | 10000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  10 | time per epoch: 98.69s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.70
-----------------------------------------------------------------------------------------
| epoch 11 | 1000/10000 batches | ms/batch 15.65 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 11 | 2000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 11 | 3000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.71
| epoch 11 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 11 | 5000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 11 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 11 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 11 | 8000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 11 | 9000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 11 | 10000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  11 | time per epoch: 98.77s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  1.55
-----------------------------------------------------------------------------------------
| epoch 12 | 1000/10000 batches | ms/batch 15.63 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 12 | 2000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 12 | 3000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 12 | 4000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 12 | 5000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 12 | 6000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 12 | 7000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 12 | 8000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 12 | 9000/10000 batches | ms/batch  9.51 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 12 | 10000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  12 | time per epoch: 99.19s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 28.68
-----------------------------------------------------------------------------------------
| epoch 13 | 1000/10000 batches | ms/batch 15.65 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 13 | 2000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 13 | 3000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 13 | 4000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 13 | 5000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 13 | 6000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 13 | 7000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 13 | 8000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 13 | 9000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 13 | 10000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  13 | time per epoch: 99.12s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 14 | 1000/10000 batches | ms/batch 15.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 14 | 2000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 14 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 14 | 4000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 14 | 5000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 14 | 6000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 14 | 7000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 14 | 8000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 14 | 9000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 14 | 10000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  14 | time per epoch: 99.05s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 710989774848.00
-----------------------------------------------------------------------------------------
| epoch 15 | 1000/10000 batches | ms/batch 15.79 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.73
| epoch 15 | 2000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 15 | 3000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 15 | 4000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 15 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 15 | 6000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 15 | 7000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 15 | 8000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 15 | 9000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 15 | 10000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  15 | time per epoch: 99.50s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 16 | 1000/10000 batches | ms/batch 15.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 16 | 2000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 16 | 3000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 16 | 4000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 16 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 16 | 6000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 16 | 7000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 16 | 8000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 16 | 9000/10000 batches | ms/batch  9.50 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 16 | 10000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  16 | time per epoch: 99.48s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 2558.97
-----------------------------------------------------------------------------------------
| epoch 17 | 1000/10000 batches | ms/batch 15.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 17 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 17 | 3000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 17 | 4000/10000 batches | ms/batch  9.59 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.73
| epoch 17 | 5000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 17 | 6000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 17 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 17 | 8000/10000 batches | ms/batch  8.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 17 | 9000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.71
| epoch 17 | 10000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  17 | time per epoch: 98.96s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 4831499833296551936.00
-----------------------------------------------------------------------------------------
[2024-10-02 04:55:41,292][absl][INFO] - Saving checkpoint at step: 170000
[2024-10-02 04:55:41,299][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 04:55:41,307][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_170000.
[2024-10-02 04:55:41,308][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_170000
[2024-10-02 04:55:41,309][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 04:55:41,310][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_170000
[2024-10-02 04:55:41,817][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_170000
[2024-10-02 04:55:41,827][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_170000.orbax-checkpoint-tmp-1 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_170000
[2024-10-02 04:55:41,844][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_170000`.
[2024-10-02 04:55:41,844][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_170000
[2024-10-02 04:55:41,845][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_10000
| epoch 18 | 1000/10000 batches | ms/batch 15.37 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 18 | 2000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.71

| epoch 18 | 3000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 18 | 4000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 18 | 5000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 18 | 6000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 18 | 7000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 18 | 8000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 18 | 9000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 18 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  18 | time per epoch: 99.45s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 19 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 19 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 19 | 3000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 19 | 4000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 19 | 5000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 19 | 6000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 19 | 7000/10000 batches | ms/batch  9.58 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 19 | 8000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 19 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 19 | 10000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.73
-----------------------------------------------------------------------------------------
| end of epoch  19 | time per epoch: 99.28s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 135466.53
-----------------------------------------------------------------------------------------
| epoch 20 | 1000/10000 batches | ms/batch 15.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 20 | 2000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 20 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 20 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 20 | 5000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 20 | 6000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 20 | 7000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 20 | 8000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 20 | 9000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 20 | 10000/10000 batches | ms/batch  8.68 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  20 | time per epoch: 97.86s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 40.17
-----------------------------------------------------------------------------------------
[2024-10-02 05:01:08,494][absl][INFO] - Saving checkpoint at step: 200000
[2024-10-02 05:01:08,495][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 05:01:08,498][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_200000.
[2024-10-02 05:01:08,499][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_200000
[2024-10-02 05:01:08,499][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 05:01:08,501][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_200000
[2024-10-02 05:01:08,617][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_200000
[2024-10-02 05:01:08,621][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_200000.orbax-checkpoint-tmp-2 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_200000
[2024-10-02 05:01:08,629][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_200000`.
[2024-10-02 05:01:08,629][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_200000
[2024-10-02 05:01:08,630][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_170000

| epoch 21 | 1000/10000 batches | ms/batch 15.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 21 | 2000/10000 batches | ms/batch  9.58 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 21 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 21 | 4000/10000 batches | ms/batch  8.81 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 21 | 5000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 21 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 21 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 21 | 8000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 21 | 9000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 21 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  21 | time per epoch: 99.29s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 22 | 1000/10000 batches | ms/batch 15.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 22 | 2000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 22 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.73
| epoch 22 | 4000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 22 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 22 | 6000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 22 | 7000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 22 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 22 | 9000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 22 | 10000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  22 | time per epoch: 98.33s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  1.52
-----------------------------------------------------------------------------------------
| epoch 23 | 1000/10000 batches | ms/batch 15.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 23 | 2000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 23 | 3000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 23 | 4000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 23 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 23 | 6000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 23 | 7000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 23 | 8000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 23 | 9000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 23 | 10000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  23 | time per epoch: 99.41s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 24 | 1000/10000 batches | ms/batch 15.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 24 | 2000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 4000/10000 batches | ms/batch 10.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 5000/10000 batches | ms/batch 10.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 6000/10000 batches | ms/batch  9.50 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 7000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 24 | 8000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 9000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 24 | 10000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  24 | time per epoch: 101.57s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 934635760211640451072.00
-----------------------------------------------------------------------------------------
| epoch 25 | 1000/10000 batches | ms/batch 15.77 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 25 | 2000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 25 | 3000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 25 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 25 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 25 | 6000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 25 | 7000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 25 | 8000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 25 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 25 | 10000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  25 | time per epoch: 98.95s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 26 | 1000/10000 batches | ms/batch 15.47 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 26 | 2000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 26 | 3000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 26 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 26 | 5000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 26 | 6000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70

| epoch 26 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 26 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 26 | 9000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 26 | 10000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  26 | time per epoch: 98.09s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.70
-----------------------------------------------------------------------------------------
| epoch 27 | 1000/10000 batches | ms/batch 15.85 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 27 | 2000/10000 batches | ms/batch  8.81 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 27 | 3000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 27 | 4000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 27 | 5000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 27 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 27 | 7000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 27 | 8000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 27 | 9000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 27 | 10000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.71
-----------------------------------------------------------------------------------------
| end of epoch  27 | time per epoch: 98.60s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 1660320137347072.00
-----------------------------------------------------------------------------------------
| epoch 28 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 28 | 2000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 28 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 28 | 4000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 28 | 5000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 28 | 6000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 28 | 7000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 28 | 8000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 28 | 9000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 28 | 10000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  28 | time per epoch: 98.77s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 5340040750170112.00
-----------------------------------------------------------------------------------------
| epoch 29 | 1000/10000 batches | ms/batch 15.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 29 | 2000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 29 | 3000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 29 | 4000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 29 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 29 | 6000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 29 | 7000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 29 | 8000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 29 | 9000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 29 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  29 | time per epoch: 98.49s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  1.74
-----------------------------------------------------------------------------------------
| epoch 30 | 1000/10000 batches | ms/batch 15.66 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 30 | 2000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 30 | 3000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 30 | 4000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 30 | 5000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 30 | 6000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 30 | 7000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 30 | 8000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 30 | 9000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 30 | 10000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  30 | time per epoch: 99.06s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.71
-----------------------------------------------------------------------------------------
[2024-10-02 05:19:20,575][absl][INFO] - Saving checkpoint at step: 300000
[2024-10-02 05:19:20,581][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 05:19:20,589][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_300000.
[2024-10-02 05:19:20,590][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_300000
[2024-10-02 05:19:20,591][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 05:19:20,593][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_300000
[2024-10-02 05:19:20,703][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_300000
[2024-10-02 05:19:20,710][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_300000.orbax-checkpoint-tmp-3 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_300000
[2024-10-02 05:19:20,726][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_300000`.
[2024-10-02 05:19:20,726][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_300000
[2024-10-02 05:19:20,727][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_200000
| epoch 31 | 1000/10000 batches | ms/batch 15.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 31 | 2000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 31 | 3000/10000 batches | ms/batch  8.77 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 31 | 4000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 31 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 31 | 6000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 31 | 7000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 31 | 8000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 31 | 9000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 31 | 10000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  31 | time per epoch: 98.67s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.94
-----------------------------------------------------------------------------------------
| epoch 32 | 1000/10000 batches | ms/batch 15.65 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 32 | 2000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 32 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 32 | 4000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 32 | 5000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 32 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 32 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 32 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 32 | 9000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 32 | 10000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  32 | time per epoch: 99.00s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 7094996.50
-----------------------------------------------------------------------------------------
| epoch 33 | 1000/10000 batches | ms/batch 15.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 33 | 2000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 33 | 3000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 33 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 33 | 5000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 33 | 6000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 33 | 7000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 33 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 33 | 9000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 33 | 10000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  33 | time per epoch: 99.71s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 34 | 1000/10000 batches | ms/batch 15.86 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 34 | 2000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 34 | 3000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 34 | 4000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 34 | 5000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 34 | 6000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 34 | 7000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 34 | 8000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 34 | 9000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 34 | 10000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  34 | time per epoch: 99.44s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 35 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 35 | 2000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 35 | 3000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 35 | 4000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 35 | 5000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 35 | 6000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 35 | 7000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 35 | 8000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 35 | 9000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 35 | 10000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  35 | time per epoch: 98.69s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.86
-----------------------------------------------------------------------------------------
| epoch 36 | 1000/10000 batches | ms/batch 15.51 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 36 | 2000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 36 | 3000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 36 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 36 | 5000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 36 | 6000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 36 | 7000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 36 | 8000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 36 | 9000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 36 | 10000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  36 | time per epoch: 98.60s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 2781838592.00
-----------------------------------------------------------------------------------------
| epoch 37 | 1000/10000 batches | ms/batch 15.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 37 | 2000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 37 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 37 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 37 | 5000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 37 | 6000/10000 batches | ms/batch  9.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 37 | 7000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 37 | 8000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 37 | 9000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 37 | 10000/10000 batches | ms/batch  8.52 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  37 | time per epoch: 99.22s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 38 | 1000/10000 batches | ms/batch 15.62 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 2000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 3000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 4000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 38 | 5000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 38 | 7000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 8000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 9000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 38 | 10000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  38 | time per epoch: 98.79s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss: 10395882440622080000.00
-----------------------------------------------------------------------------------------
| epoch 39 | 1000/10000 batches | ms/batch 15.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 39 | 2000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 39 | 3000/10000 batches | ms/batch  8.75 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.72
| epoch 39 | 4000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 39 | 5000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 39 | 6000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 39 | 7000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 39 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 39 | 9000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 39 | 10000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  39 | time per epoch: 97.78s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 40 | 1000/10000 batches | ms/batch 15.78 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 40 | 2000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 40 | 3000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 40 | 4000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 40 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 40 | 6000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 40 | 7000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 40 | 8000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 40 | 9000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 40 | 10000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  40 | time per epoch: 99.12s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 41 | 1000/10000 batches | ms/batch 15.61 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 41 | 2000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 41 | 3000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 41 | 4000/10000 batches | ms/batch  8.66 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 41 | 5000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 41 | 6000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 41 | 7000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 41 | 8000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 41 | 9000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 41 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  41 | time per epoch: 97.96s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 42 | 1000/10000 batches | ms/batch 15.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 2000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 3000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 4000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 42 | 5000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 6000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 8000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 42 | 9000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 42 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  42 | time per epoch: 98.76s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 43 | 1000/10000 batches | ms/batch 15.66 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 43 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 43 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 4000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 6000/10000 batches | ms/batch  9.53 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 7000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 8000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 9000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 43 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  43 | time per epoch: 99.32s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 44 | 1000/10000 batches | ms/batch 15.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 2000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 3000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 4000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69

| epoch 44 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 6000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 8000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 9000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 44 | 10000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  44 | time per epoch: 98.78s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 45 | 1000/10000 batches | ms/batch 15.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 2000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 3000/10000 batches | ms/batch  8.67 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 4000/10000 batches | ms/batch  8.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 5000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 6000/10000 batches | ms/batch  8.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 7000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 45 | 8000/10000 batches | ms/batch  8.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 45 | 9000/10000 batches | ms/batch  8.72 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 45 | 10000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  45 | time per epoch: 94.06s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 46 | 1000/10000 batches | ms/batch 15.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 46 | 2000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 46 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 46 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 46 | 5000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 46 | 6000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 46 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 46 | 8000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 46 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 46 | 10000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  46 | time per epoch: 98.62s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
[2024-10-02 05:48:19,641][absl][INFO] - Saving checkpoint at step: 460000
[2024-10-02 05:48:19,645][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 05:48:19,651][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_460000.
[2024-10-02 05:48:19,652][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_460000
[2024-10-02 05:48:19,652][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 05:48:19,654][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_460000
[2024-10-02 05:48:19,778][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_460000
[2024-10-02 05:48:19,781][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_460000.orbax-checkpoint-tmp-4 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_460000
[2024-10-02 05:48:19,794][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_460000`.
[2024-10-02 05:48:19,794][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_460000
[2024-10-02 05:48:19,795][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_300000
| epoch 47 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 47 | 2000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 3000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 4000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 5000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 47 | 7000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 8000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 9000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 47 | 10000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  47 | time per epoch: 98.46s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 48 | 1000/10000 batches | ms/batch 15.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 2000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 3000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 48 | 5000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 6000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 7000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 9000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 48 | 10000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  48 | time per epoch: 99.12s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 49 | 1000/10000 batches | ms/batch 15.42 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 49 | 2000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 3000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 5000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 6000/10000 batches | ms/batch  9.58 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 7000/10000 batches | ms/batch  9.53 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 49 | 8000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 9000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 49 | 10000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  49 | time per epoch: 99.15s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 50 | 1000/10000 batches | ms/batch 15.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 2000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 50 | 3000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 4000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 5000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 6000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 7000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 8000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 50 | 9000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 50 | 10000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  50 | time per epoch: 98.01s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 51 | 1000/10000 batches | ms/batch 15.70 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 3000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 5000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 51 | 6000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 7000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 51 | 8000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 9000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 51 | 10000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  51 | time per epoch: 98.55s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 52 | 1000/10000 batches | ms/batch 15.52 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 2000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 3000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 4000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 5000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 6000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 52 | 8000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 9000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 52 | 10000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  52 | time per epoch: 99.10s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 53 | 1000/10000 batches | ms/batch 15.78 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 53 | 2000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 53 | 3000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 53 | 4000/10000 batches | ms/batch  9.49 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 53 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 53 | 6000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 53 | 7000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 53 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69

| epoch 53 | 9000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 53 | 10000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  53 | time per epoch: 99.59s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 54 | 1000/10000 batches | ms/batch 15.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 2000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 3000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 4000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 6000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 7000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 8000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 9000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 54 | 10000/10000 batches | ms/batch  9.47 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  54 | time per epoch: 99.06s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 55 | 1000/10000 batches | ms/batch 15.78 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 2000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 3000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 4000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 5000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 6000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 7000/10000 batches | ms/batch  8.78 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 8000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 9000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 55 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  55 | time per epoch: 98.52s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 56 | 1000/10000 batches | ms/batch 15.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 2000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 3000/10000 batches | ms/batch  8.66 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 4000/10000 batches | ms/batch  8.44 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 5000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 6000/10000 batches | ms/batch  8.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 7000/10000 batches | ms/batch  8.61 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 56 | 8000/10000 batches | ms/batch  8.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 9000/10000 batches | ms/batch  8.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 56 | 10000/10000 batches | ms/batch  8.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  56 | time per epoch: 92.93s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 57 | 1000/10000 batches | ms/batch 15.73 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 57 | 2000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 57 | 3000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 57 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 57 | 5000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 57 | 6000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 57 | 7000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 57 | 8000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 57 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 57 | 10000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  57 | time per epoch: 98.99s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 58 | 1000/10000 batches | ms/batch 15.50 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 2000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 3000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 58 | 4000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 5000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 6000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 8000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 9000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 58 | 10000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  58 | time per epoch: 98.64s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 59 | 1000/10000 batches | ms/batch 15.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 2000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 3000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 4000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 5000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 6000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 59 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 59 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 9000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 59 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  59 | time per epoch: 99.15s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 60 | 1000/10000 batches | ms/batch 15.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 60 | 2000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 60 | 3000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 60 | 4000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 60 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 60 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 60 | 7000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 60 | 8000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 60 | 9000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 60 | 10000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  60 | time per epoch: 99.31s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 61 | 1000/10000 batches | ms/batch 15.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 61 | 2000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 61 | 3000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 61 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 61 | 5000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 61 | 6000/10000 batches | ms/batch  9.47 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 61 | 7000/10000 batches | ms/batch  9.67 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 61 | 8000/10000 batches | ms/batch  9.59 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 61 | 9000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 61 | 10000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  61 | time per epoch: 99.44s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 62 | 1000/10000 batches | ms/batch 15.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 3000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 4000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 62 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 6000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 7000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 8000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 62 | 9000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 62 | 10000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  62 | time per epoch: 99.12s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 63 | 1000/10000 batches | ms/batch 15.55 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 2000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 63 | 3000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 5000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 6000/10000 batches | ms/batch  9.53 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 7000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 8000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 9000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 63 | 10000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  63 | time per epoch: 98.78s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 64 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 2000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 3000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 4000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 64 | 5000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 6000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 64 | 7000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 8000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 9000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 64 | 10000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  64 | time per epoch: 98.63s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 65 | 1000/10000 batches | ms/batch 15.72 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 65 | 2000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 65 | 3000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 65 | 4000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 65 | 5000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 65 | 6000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 65 | 7000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 65 | 8000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 65 | 9000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 65 | 10000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  65 | time per epoch: 99.74s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 66 | 1000/10000 batches | ms/batch 15.96 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 2000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 4000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 5000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 66 | 6000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 7000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 8000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 9000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 66 | 10000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  66 | time per epoch: 99.45s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.77
-----------------------------------------------------------------------------------------
| epoch 67 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 2000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 4000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 5000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 6000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 67 | 7000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 8000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 9000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 67 | 10000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  67 | time per epoch: 97.23s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 68 | 1000/10000 batches | ms/batch 15.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 68 | 2000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 68 | 3000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 68 | 4000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 68 | 5000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 68 | 6000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 68 | 7000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 68 | 8000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 68 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 68 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  68 | time per epoch: 98.58s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 69 | 1000/10000 batches | ms/batch 15.71 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 2000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 3000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 4000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 69 | 5000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 7000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 8000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 9000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 69 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  69 | time per epoch: 99.08s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------

| epoch 70 | 1000/10000 batches | ms/batch 14.79 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 2000/10000 batches | ms/batch  8.81 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 3000/10000 batches | ms/batch  8.69 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 70 | 4000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 5000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 6000/10000 batches | ms/batch  8.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 7000/10000 batches | ms/batch  8.70 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 8000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 9000/10000 batches | ms/batch  8.70 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 70 | 10000/10000 batches | ms/batch  8.44 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  70 | time per epoch: 93.12s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 71 | 1000/10000 batches | ms/batch 15.81 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 2000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 3000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 4000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 71 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 71 | 6000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 7000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 8000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 71 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  71 | time per epoch: 98.96s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 72 | 1000/10000 batches | ms/batch 14.84 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 72 | 2000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 72 | 3000/10000 batches | ms/batch  8.80 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 72 | 4000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 72 | 5000/10000 batches | ms/batch  8.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 72 | 6000/10000 batches | ms/batch  8.78 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 72 | 7000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 72 | 8000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 72 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 72 | 10000/10000 batches | ms/batch  8.47 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  72 | time per epoch: 93.90s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 73 | 1000/10000 batches | ms/batch 16.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 73 | 2000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 73 | 3000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 73 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 73 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 73 | 6000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 73 | 7000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 73 | 8000/10000 batches | ms/batch  8.77 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 73 | 9000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 73 | 10000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  73 | time per epoch: 99.26s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 74 | 1000/10000 batches | ms/batch 15.76 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 74 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 3000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 74 | 4000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 5000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 6000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 8000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 74 | 10000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  74 | time per epoch: 98.54s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 75 | 1000/10000 batches | ms/batch 15.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 2000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 75 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 5000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 75 | 6000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 7000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 9000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 75 | 10000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  75 | time per epoch: 98.73s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 76 | 1000/10000 batches | ms/batch 15.60 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 76 | 2000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 3000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 4000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 5000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 6000/10000 batches | ms/batch  9.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 7000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 8000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 76 | 9000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 76 | 10000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  76 | time per epoch: 98.62s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 77 | 1000/10000 batches | ms/batch 15.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 77 | 2000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 77 | 3000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 77 | 4000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 77 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 77 | 6000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 77 | 7000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.71
| epoch 77 | 8000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 77 | 9000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 77 | 10000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  77 | time per epoch: 98.96s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 78 | 1000/10000 batches | ms/batch 15.70 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 78 | 3000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 4000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 5000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 6000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 8000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 78 | 9000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 78 | 10000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  78 | time per epoch: 99.03s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 79 | 1000/10000 batches | ms/batch 15.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 79 | 2000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 3000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 4000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 5000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 6000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 7000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 8000/10000 batches | ms/batch  9.50 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 79 | 10000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  79 | time per epoch: 99.47s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 80 | 1000/10000 batches | ms/batch 15.80 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 80 | 2000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 80 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 80 | 4000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 80 | 5000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 80 | 6000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 80 | 7000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 80 | 8000/10000 batches | ms/batch  8.77 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 80 | 9000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 80 | 10000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  80 | time per epoch: 98.22s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 81 | 1000/10000 batches | ms/batch 15.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 2000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 4000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 5000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 6000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 7000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 8000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 81 | 10000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  81 | time per epoch: 98.41s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 82 | 1000/10000 batches | ms/batch 15.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 82 | 2000/10000 batches | ms/batch  8.81 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 3000/10000 batches | ms/batch  8.65 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 4000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 5000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 6000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 7000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 8000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 9000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 82 | 10000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  82 | time per epoch: 96.38s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 83 | 1000/10000 batches | ms/batch 15.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 2000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 5000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 6000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 7000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 9000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 83 | 10000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  83 | time per epoch: 98.38s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 84 | 1000/10000 batches | ms/batch 15.73 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 2000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 3000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 4000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 5000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 6000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 84 | 7000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 84 | 9000/10000 batches | ms/batch  9.51 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 84 | 10000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  84 | time per epoch: 99.23s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 85 | 1000/10000 batches | ms/batch 15.57 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 2000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 3000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 4000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 5000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 6000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 7000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 85 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 85 | 10000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  85 | time per epoch: 98.62s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 86 | 1000/10000 batches | ms/batch 15.97 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 86 | 2000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 86 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 86 | 4000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 86 | 5000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 86 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 86 | 7000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 86 | 8000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 86 | 9000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 86 | 10000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  86 | time per epoch: 99.55s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 87 | 1000/10000 batches | ms/batch 15.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 2000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 3000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 4000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 5000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 6000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 7000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 87 | 8000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 87 | 9000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 87 | 10000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  87 | time per epoch: 99.58s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 88 | 1000/10000 batches | ms/batch 15.72 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 88 | 2000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 88 | 3000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 88 | 4000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 88 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 88 | 6000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 88 | 7000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 88 | 8000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 88 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 88 | 10000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  88 | time per epoch: 99.22s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
[2024-10-02 07:04:20,745][absl][INFO] - Saving checkpoint at step: 880000
[2024-10-02 07:04:20,750][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 07:04:20,754][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_880000.
[2024-10-02 07:04:20,754][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_880000
[2024-10-02 07:04:20,755][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 07:04:20,756][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_880000
[2024-10-02 07:04:20,865][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_880000
[2024-10-02 07:04:20,871][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_880000.orbax-checkpoint-tmp-5 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_880000
[2024-10-02 07:04:20,879][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_880000`.
[2024-10-02 07:04:20,879][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_880000
[2024-10-02 07:04:20,880][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_460000
| epoch 89 | 1000/10000 batches | ms/batch 15.58 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 89 | 2000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 89 | 3000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 89 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 89 | 5000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 89 | 6000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 89 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 89 | 8000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 89 | 9000/10000 batches | ms/batch  8.72 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 89 | 10000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  89 | time per epoch: 97.14s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 90 | 1000/10000 batches | ms/batch 15.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 2000/10000 batches | ms/batch  8.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 90 | 3000/10000 batches | ms/batch  8.80 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 4000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 5000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 6000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 90 | 7000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 8000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 9000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 90 | 10000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  90 | time per epoch: 96.21s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 91 | 1000/10000 batches | ms/batch 15.88 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 91 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 91 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 91 | 4000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 91 | 5000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 91 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 91 | 7000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 91 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 91 | 9000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 91 | 10000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch  91 | time per epoch: 99.34s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.70
-----------------------------------------------------------------------------------------
| epoch 92 | 1000/10000 batches | ms/batch 15.69 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 92 | 2000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 92 | 3000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 92 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 92 | 5000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 92 | 6000/10000 batches | ms/batch  8.54 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 92 | 7000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 92 | 8000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 92 | 9000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 92 | 10000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  92 | time per epoch: 97.81s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
[2024-10-02 07:11:31,987][absl][INFO] - Saving checkpoint at step: 920000
[2024-10-02 07:11:31,993][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 07:11:31,997][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_920000.
[2024-10-02 07:11:31,997][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_920000
[2024-10-02 07:11:31,997][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 07:11:31,999][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_920000
[2024-10-02 07:11:32,111][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_920000
[2024-10-02 07:11:32,118][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_920000.orbax-checkpoint-tmp-6 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_920000
[2024-10-02 07:11:32,127][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_920000`.
[2024-10-02 07:11:32,128][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_920000
[2024-10-02 07:11:32,129][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_880000
| epoch 93 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 93 | 2000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 93 | 3000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 93 | 4000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 93 | 5000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 93 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 93 | 7000/10000 batches | ms/batch  9.53 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 93 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 93 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 93 | 10000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  93 | time per epoch: 98.95s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 94 | 1000/10000 batches | ms/batch 16.11 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 2000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 94 | 3000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 4000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 5000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 7000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 9000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 94 | 10000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  94 | time per epoch: 99.69s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 95 | 1000/10000 batches | ms/batch 15.92 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 95 | 2000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 95 | 3000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 95 | 4000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 95 | 5000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 95 | 6000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 95 | 7000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 95 | 8000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 95 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 95 | 10000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.71
-----------------------------------------------------------------------------------------
| end of epoch  95 | time per epoch: 99.03s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss: 1735417201357225984.00
-----------------------------------------------------------------------------------------
| epoch 96 | 1000/10000 batches | ms/batch 15.56 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 96 | 2000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 96 | 3000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 96 | 4000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 96 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 96 | 6000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 96 | 7000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 96 | 8000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 96 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 96 | 10000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  96 | time per epoch: 99.11s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.73
-----------------------------------------------------------------------------------------
| epoch 97 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 97 | 2000/10000 batches | ms/batch  9.53 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 97 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 97 | 4000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 97 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 97 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 97 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 97 | 8000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 97 | 9000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 97 | 10000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  97 | time per epoch: 98.70s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
[2024-10-02 07:20:38,538][absl][INFO] - Saving checkpoint at step: 970000
[2024-10-02 07:20:38,545][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 07:20:38,549][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_970000.
[2024-10-02 07:20:38,549][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_970000
[2024-10-02 07:20:38,549][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 07:20:38,551][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_970000
[2024-10-02 07:20:38,666][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_970000
[2024-10-02 07:20:38,670][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_970000.orbax-checkpoint-tmp-7 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_970000
[2024-10-02 07:20:38,678][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_970000`.
[2024-10-02 07:20:38,678][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_970000
[2024-10-02 07:20:38,679][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_920000
| epoch 98 | 1000/10000 batches | ms/batch 15.57 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 98 | 2000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 98 | 3000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 98 | 4000/10000 batches | ms/batch  9.47 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 98 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 98 | 6000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 98 | 7000/10000 batches | ms/batch  9.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 98 | 8000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 98 | 9000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 98 | 10000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  98 | time per epoch: 99.27s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 99 | 1000/10000 batches | ms/batch 15.82 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 99 | 2000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 99 | 3000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 99 | 4000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 99 | 5000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 99 | 6000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 99 | 7000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 99 | 8000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 99 | 9000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 99 | 10000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch  99 | time per epoch: 98.95s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 100 | 1000/10000 batches | ms/batch 15.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 100 | 2000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 100 | 3000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 100 | 4000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 100 | 5000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 100 | 6000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 100 | 7000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 100 | 8000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 100 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 100 | 10000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 100 | time per epoch: 98.90s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 101 | 1000/10000 batches | ms/batch 15.82 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 101 | 2000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 101 | 3000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 101 | 4000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 101 | 5000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 101 | 6000/10000 batches | ms/batch  9.62 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 101 | 7000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 101 | 8000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 101 | 9000/10000 batches | ms/batch  9.71 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 101 | 10000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 101 | time per epoch: 100.29s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 102 | 1000/10000 batches | ms/batch 15.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 3000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 4000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 5000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 6000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 7000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 8000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 9000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 102 | 10000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 102 | time per epoch: 98.41s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 103 | 1000/10000 batches | ms/batch 16.05 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 2000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 3000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 4000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 5000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 6000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 7000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 103 | 8000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 9000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 103 | 10000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 103 | time per epoch: 99.59s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 104 | 1000/10000 batches | ms/batch 15.60 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 2000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 3000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 4000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 5000/10000 batches | ms/batch  8.69 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 104 | 6000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 7000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 104 | 8000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 9000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 104 | 10000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 104 | time per epoch: 96.80s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 105 | 1000/10000 batches | ms/batch 15.87 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 2000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 105 | 3000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 5000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 6000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 7000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 105 | 8000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 9000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 105 | 10000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 105 | time per epoch: 99.61s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 106 | 1000/10000 batches | ms/batch 15.64 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 2000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 4000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 5000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 106 | 6000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 106 | 7000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 9000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 106 | 10000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 106 | time per epoch: 98.77s |
| Train Metrics | accuracy:  0.52 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 107 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 107 | 2000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 107 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 107 | 4000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 107 | 5000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 107 | 6000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 107 | 7000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 107 | 8000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 107 | 9000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 107 | 10000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 107 | time per epoch: 98.50s |
| Train Metrics | accuracy:  0.52 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.70
-----------------------------------------------------------------------------------------
| epoch 108 | 1000/10000 batches | ms/batch 15.54 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 108 | 2000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 108 | 3000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 108 | 4000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 108 | 5000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 108 | 6000/10000 batches | ms/batch  9.50 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 108 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 108 | 8000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 108 | 9000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 108 | 10000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 108 | time per epoch: 99.13s |
| Train Metrics | accuracy:  0.52 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 109 | 1000/10000 batches | ms/batch 15.61 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 109 | 2000/10000 batches | ms/batch  8.60 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 109 | 3000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 109 | 4000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 109 | 5000/10000 batches | ms/batch  8.77 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 109 | 6000/10000 batches | ms/batch  8.51 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 109 | 7000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 109 | 8000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 109 | 9000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 109 | 10000/10000 batches | ms/batch  8.77 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 109 | time per epoch: 94.76s |
| Train Metrics | accuracy:  0.52 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 110 | 1000/10000 batches | ms/batch 15.71 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 110 | 2000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 110 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 110 | 4000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 110 | 5000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 110 | 6000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 110 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 110 | 8000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 110 | 9000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 110 | 10000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch 110 | time per epoch: 98.60s |
| Train Metrics | accuracy:  0.52 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss: 1769648434303729664.00
-----------------------------------------------------------------------------------------
| epoch 111 | 1000/10000 batches | ms/batch 15.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 111 | 2000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 111 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.70
| epoch 111 | 4000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 111 | 5000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 111 | 6000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 111 | 7000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 111 | 8000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 111 | 9000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 111 | 10000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
-----------------------------------------------------------------------------------------
| end of epoch 111 | time per epoch: 98.42s |
| Train Metrics | accuracy:  0.50 | loss:  0.70
| Eval  Metrics | accuracy:  0.50 | loss: 152432160.00
-----------------------------------------------------------------------------------------
| epoch 112 | 1000/10000 batches | ms/batch 15.85 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 112 | 2000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 112 | 3000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 112 | 4000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 112 | 5000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 112 | 6000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 112 | 7000/10000 batches | ms/batch  9.52 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 112 | 8000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 112 | 9000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 112 | 10000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 112 | time per epoch: 99.10s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss: 5261127319552.00
-----------------------------------------------------------------------------------------
| epoch 113 | 1000/10000 batches | ms/batch 15.57 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 113 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 113 | 3000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 113 | 4000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 113 | 5000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 113 | 6000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 113 | 7000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 113 | 8000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 113 | 9000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 113 | 10000/10000 batches | ms/batch  8.72 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 113 | time per epoch: 98.91s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss: 3045080432640.00
-----------------------------------------------------------------------------------------
| epoch 114 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 114 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 114 | 3000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 114 | 4000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.70
| epoch 114 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.70
| epoch 114 | 6000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 114 | 7000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 114 | 8000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 114 | 9000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 114 | 10000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 114 | time per epoch: 98.89s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:   nan
-----------------------------------------------------------------------------------------
| epoch 115 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 3000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 115 | 4000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 115 | 5000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 6000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 7000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 8000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 9000/10000 batches | ms/batch  9.47 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 115 | 10000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 115 | time per epoch: 98.92s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 116 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 2000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 3000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 5000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 116 | 7000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 8000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 116 | 9000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 116 | 10000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 116 | time per epoch: 98.72s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 117 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 3000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 4000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 5000/10000 batches | ms/batch  8.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 6000/10000 batches | ms/batch  8.63 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 7000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 8000/10000 batches | ms/batch  8.62 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 117 | 9000/10000 batches | ms/batch  8.46 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 117 | 10000/10000 batches | ms/batch  8.80 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 117 | time per epoch: 94.89s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 118 | 1000/10000 batches | ms/batch 15.75 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 2000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 118 | 5000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 6000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 7000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 118 | 8000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 9000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 118 | 10000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 118 | time per epoch: 98.35s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 119 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 119 | 2000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 119 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 119 | 4000/10000 batches | ms/batch  8.81 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 119 | 5000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 119 | 6000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 119 | 7000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 119 | 8000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 119 | 9000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 119 | 10000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 119 | time per epoch: 98.22s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 120 | 1000/10000 batches | ms/batch 15.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 2000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 3000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 4000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 5000/10000 batches | ms/batch  8.66 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 6000/10000 batches | ms/batch  8.40 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 7000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 8000/10000 batches | ms/batch  8.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 120 | 9000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 120 | 10000/10000 batches | ms/batch  8.72 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 120 | time per epoch: 94.77s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 121 | 1000/10000 batches | ms/batch 15.80 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 121 | 2000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 121 | 3000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 121 | 4000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 121 | 5000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 121 | 6000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 121 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 121 | 8000/10000 batches | ms/batch  8.77 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 121 | 9000/10000 batches | ms/batch  9.53 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 121 | 10000/10000 batches | ms/batch  9.66 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 121 | time per epoch: 99.51s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 122 | 1000/10000 batches | ms/batch 15.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 2000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 3000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 122 | 6000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 7000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 122 | 8000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 9000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 122 | 10000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 122 | time per epoch: 98.77s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 123 | 1000/10000 batches | ms/batch 15.75 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 2000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 3000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 4000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 5000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 123 | 6000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 123 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 8000/10000 batches | ms/batch  8.80 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 9000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 123 | 10000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 123 | time per epoch: 99.00s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 124 | 1000/10000 batches | ms/batch 15.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 124 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 124 | 3000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.49 | Performance/Training loss:  0.69
| epoch 124 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 124 | 5000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 124 | 6000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 124 | 7000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 124 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 124 | 9000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 124 | 10000/10000 batches | ms/batch  8.55 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 124 | time per epoch: 97.62s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 125 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 3000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 4000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 5000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 6000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 125 | 7000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 8000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 9000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 125 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 125 | time per epoch: 98.21s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 126 | 1000/10000 batches | ms/batch 15.92 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 126 | 3000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 4000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 126 | 5000/10000 batches | ms/batch  8.78 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 6000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 7000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 8000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 9000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 126 | 10000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 126 | time per epoch: 98.13s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 127 | 1000/10000 batches | ms/batch 15.71 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 127 | 2000/10000 batches | ms/batch  8.69 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 127 | 3000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 127 | 4000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 127 | 5000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 127 | 6000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 127 | 7000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 127 | 8000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 127 | 9000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 127 | 10000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 127 | time per epoch: 96.99s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 128 | 1000/10000 batches | ms/batch 15.72 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 2000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 3000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 128 | 4000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 5000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 6000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 8000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 128 | 9000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 128 | 10000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 128 | time per epoch: 98.94s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 129 | 1000/10000 batches | ms/batch 15.70 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 129 | 2000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 129 | 3000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 129 | 4000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 129 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 129 | 6000/10000 batches | ms/batch  9.49 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 129 | 7000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 129 | 8000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 129 | 9000/10000 batches | ms/batch  9.56 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 129 | 10000/10000 batches | ms/batch  9.74 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 129 | time per epoch: 100.49s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 130 | 1000/10000 batches | ms/batch 15.94 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 130 | 2000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 3000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 4000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 5000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 7000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 130 | 9000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 130 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 130 | time per epoch: 100.00s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 131 | 1000/10000 batches | ms/batch 15.65 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 131 | 2000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 131 | 3000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 131 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 131 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 131 | 6000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 131 | 7000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 131 | 8000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 131 | 9000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 131 | 10000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 131 | time per epoch: 99.16s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 132 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 132 | 2000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 3000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 4000/10000 batches | ms/batch  8.78 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 6000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 8000/10000 batches | ms/batch  8.55 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 9000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 132 | 10000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 132 | time per epoch: 98.02s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 133 | 1000/10000 batches | ms/batch 15.49 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 2000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 3000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 5000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 6000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69

| epoch 133 | 7000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 133 | 8000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 9000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 133 | 10000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 133 | time per epoch: 98.24s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 134 | 1000/10000 batches | ms/batch 15.82 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 134 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 134 | 3000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 134 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 134 | 5000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 134 | 6000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 134 | 7000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 134 | 8000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 134 | 9000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 134 | 10000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 134 | time per epoch: 98.53s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 135 | 1000/10000 batches | ms/batch 15.57 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 3000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 5000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69

| epoch 135 | 6000/10000 batches | ms/batch  9.51 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 7000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 135 | 10000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 135 | time per epoch: 99.55s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 136 | 1000/10000 batches | ms/batch 15.89 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 136 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 136 | 3000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 136 | 4000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 136 | 5000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 136 | 6000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 136 | 7000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 136 | 8000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 136 | 9000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 136 | 10000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 136 | time per epoch: 99.26s |
| Train Metrics | accuracy:  0.50 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 137 | 1000/10000 batches | ms/batch 15.75 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 137 | 2000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 137 | 3000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 137 | 4000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 137 | 5000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 137 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 137 | 7000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.50 | Performance/Training loss:  0.69
| epoch 137 | 8000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 137 | 9000/10000 batches | ms/batch  8.44 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 137 | 10000/10000 batches | ms/batch  8.57 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 137 | time per epoch: 97.27s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.50 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 138 | 1000/10000 batches | ms/batch 15.65 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 2000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 3000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 4000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 5000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 6000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 7000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 8000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 9000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 138 | 10000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 138 | time per epoch: 98.49s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 139 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 3000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 4000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 5000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 6000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 7000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 8000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 9000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 139 | 10000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 139 | time per epoch: 99.15s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
| epoch 140 | 1000/10000 batches | ms/batch 15.82 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 140 | 2000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 140 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 140 | 4000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 140 | 5000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 140 | 6000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 140 | 7000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 140 | 8000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 140 | 9000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 140 | 10000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 140 | time per epoch: 98.93s |
| Train Metrics | accuracy:  0.51 | loss:  0.69
| Eval  Metrics | accuracy:  0.51 | loss:  0.69
-----------------------------------------------------------------------------------------
[2024-10-02 08:38:31,766][absl][INFO] - Saving checkpoint at step: 1400000
[2024-10-02 08:38:31,771][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:38:31,775][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1400000.
[2024-10-02 08:38:31,775][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1400000
[2024-10-02 08:38:31,776][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 08:38:31,777][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1400000
[2024-10-02 08:38:31,892][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1400000
[2024-10-02 08:38:31,898][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1400000.orbax-checkpoint-tmp-8 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1400000
[2024-10-02 08:38:31,907][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1400000`.
[2024-10-02 08:38:31,907][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1400000
[2024-10-02 08:38:31,908][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_970000
| epoch 141 | 1000/10000 batches | ms/batch 15.62 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69

| epoch 141 | 2000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.51 | Performance/Training loss:  0.69
| epoch 141 | 3000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 141 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 141 | 5000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 141 | 6000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 141 | 7000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.69
| epoch 141 | 8000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 141 | 9000/10000 batches | ms/batch  8.71 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 141 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
-----------------------------------------------------------------------------------------
| end of epoch 141 | time per epoch: 98.78s |
| Train Metrics | accuracy:  0.52 | loss:  0.69
| Eval  Metrics | accuracy:  0.52 | loss:  0.69
-----------------------------------------------------------------------------------------
[2024-10-02 08:40:21,166][absl][INFO] - Saving checkpoint at step: 1410000
[2024-10-02 08:40:21,174][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:40:21,178][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1410000.
[2024-10-02 08:40:21,178][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1410000
[2024-10-02 08:40:21,180][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1410000
[2024-10-02 08:40:21,291][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1410000
[2024-10-02 08:40:21,296][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1410000.orbax-checkpoint-tmp-9 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1410000
[2024-10-02 08:40:21,301][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1410000`.
[2024-10-02 08:40:21,301][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1410000
[2024-10-02 08:40:21,302][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1400000
| epoch 142 | 1000/10000 batches | ms/batch 15.94 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 142 | 2000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.69
| epoch 142 | 3000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.69
| epoch 142 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.69
| epoch 142 | 5000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.52 | Performance/Training loss:  0.69
| epoch 142 | 6000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.69
| epoch 142 | 7000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.69
| epoch 142 | 8000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.68
| epoch 142 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.68
| epoch 142 | 10000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.55 | Performance/Training loss:  0.68
-----------------------------------------------------------------------------------------
| end of epoch 142 | time per epoch: 98.88s |
| Train Metrics | accuracy:  0.53 | loss:  0.69
| Eval  Metrics | accuracy:  0.55 | loss:  0.68
-----------------------------------------------------------------------------------------
[2024-10-02 08:42:10,447][absl][INFO] - Saving checkpoint at step: 1420000
[2024-10-02 08:42:10,451][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:42:10,455][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1420000.
[2024-10-02 08:42:10,455][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1420000
[2024-10-02 08:42:10,455][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 08:42:10,456][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1420000
[2024-10-02 08:42:10,923][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1420000
[2024-10-02 08:42:10,928][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1420000.orbax-checkpoint-tmp-10 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1420000
[2024-10-02 08:42:10,935][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1420000`.
[2024-10-02 08:42:10,935][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1420000
[2024-10-02 08:42:10,936][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1410000
| epoch 143 | 1000/10000 batches | ms/batch 15.65 | Performance/Training accuracy:  0.56 | Performance/Training loss:  0.68
| epoch 143 | 2000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.56 | Performance/Training loss:  0.67
| epoch 143 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.56 | Performance/Training loss:  0.68
| epoch 143 | 4000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.57 | Performance/Training loss:  0.67
| epoch 143 | 5000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.56 | Performance/Training loss:  0.68
| epoch 143 | 6000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.58 | Performance/Training loss:  0.67
| epoch 143 | 7000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.57 | Performance/Training loss:  0.67
| epoch 143 | 8000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.58 | Performance/Training loss:  0.67
| epoch 143 | 9000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.58 | Performance/Training loss:  0.67
| epoch 143 | 10000/10000 batches | ms/batch  8.65 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.67
-----------------------------------------------------------------------------------------
| end of epoch 143 | time per epoch: 97.93s |
| Train Metrics | accuracy:  0.57 | loss:  0.67
| Eval  Metrics | accuracy:  0.58 | loss:  0.67
-----------------------------------------------------------------------------------------
[2024-10-02 08:43:59,106][absl][INFO] - Saving checkpoint at step: 1430000
[2024-10-02 08:43:59,107][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:43:59,110][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1430000.
[2024-10-02 08:43:59,111][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1430000
[2024-10-02 08:43:59,112][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1430000
[2024-10-02 08:43:59,225][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1430000
[2024-10-02 08:43:59,233][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1430000.orbax-checkpoint-tmp-11 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1430000
[2024-10-02 08:43:59,241][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1430000`.
[2024-10-02 08:43:59,241][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1430000
[2024-10-02 08:43:59,242][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1420000
| epoch 144 | 1000/10000 batches | ms/batch 15.88 | Performance/Training accuracy:  0.58 | Performance/Training loss:  0.67
| epoch 144 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.67
| epoch 144 | 3000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.58 | Performance/Training loss:  0.67
| epoch 144 | 4000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 144 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.58 | Performance/Training loss:  0.66

| epoch 144 | 6000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 144 | 7000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 144 | 8000/10000 batches | ms/batch  8.80 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 144 | 9000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 144 | 10000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
-----------------------------------------------------------------------------------------
| end of epoch 144 | time per epoch: 99.14s |
| Train Metrics | accuracy:  0.59 | loss:  0.66
| Eval  Metrics | accuracy:  0.59 | loss:  0.66
-----------------------------------------------------------------------------------------
[2024-10-02 08:45:48,777][absl][INFO] - Saving checkpoint at step: 1440000
[2024-10-02 08:45:48,782][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:45:48,787][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1440000.
[2024-10-02 08:45:48,787][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1440000
[2024-10-02 08:45:48,788][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 08:45:48,789][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1440000
[2024-10-02 08:45:48,900][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1440000
[2024-10-02 08:45:48,905][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1440000.orbax-checkpoint-tmp-12 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1440000
[2024-10-02 08:45:48,917][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1440000`.
[2024-10-02 08:45:48,917][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1440000
[2024-10-02 08:45:48,918][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1430000
| epoch 145 | 1000/10000 batches | ms/batch 15.97 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 145 | 2000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 145 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 145 | 4000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 145 | 5000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 145 | 6000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 145 | 7000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 145 | 8000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 145 | 9000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 145 | 10000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
-----------------------------------------------------------------------------------------
| end of epoch 145 | time per epoch: 99.11s |
| Train Metrics | accuracy:  0.60 | loss:  0.66
| Eval  Metrics | accuracy:  0.60 | loss:  0.66
-----------------------------------------------------------------------------------------
[2024-10-02 08:47:37,941][absl][INFO] - Saving checkpoint at step: 1450000
[2024-10-02 08:47:37,947][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:47:37,954][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1450000.
[2024-10-02 08:47:37,955][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1450000
[2024-10-02 08:47:37,957][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1450000
[2024-10-02 08:47:38,072][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1450000
[2024-10-02 08:47:38,078][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1450000.orbax-checkpoint-tmp-13 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1450000
[2024-10-02 08:47:38,088][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1450000`.
[2024-10-02 08:47:38,089][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1450000
[2024-10-02 08:47:38,090][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1440000
| epoch 146 | 1000/10000 batches | ms/batch 15.73 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.65
| epoch 146 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 146 | 3000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66

| epoch 146 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 146 | 5000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 146 | 6000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.65
| epoch 146 | 7000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 146 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 146 | 9000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.65
| epoch 146 | 10000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
-----------------------------------------------------------------------------------------
| end of epoch 146 | time per epoch: 99.28s |
| Train Metrics | accuracy:  0.60 | loss:  0.65
| Eval  Metrics | accuracy:  0.59 | loss:  0.66
-----------------------------------------------------------------------------------------

| epoch 147 | 1000/10000 batches | ms/batch 15.93 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.65
| epoch 147 | 2000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.65
| epoch 147 | 3000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.65
| epoch 147 | 4000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 147 | 5000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 147 | 6000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 147 | 7000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 147 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 147 | 9000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 147 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
-----------------------------------------------------------------------------------------
| end of epoch 147 | time per epoch: 98.51s |
| Train Metrics | accuracy:  0.61 | loss:  0.65
| Eval  Metrics | accuracy:  0.60 | loss:  0.66
-----------------------------------------------------------------------------------------
[2024-10-02 08:51:16,312][absl][INFO] - Saving checkpoint at step: 1470000
[2024-10-02 08:51:16,313][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:51:16,318][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1470000.
[2024-10-02 08:51:16,319][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1470000
[2024-10-02 08:51:16,319][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 08:51:16,320][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1470000
[2024-10-02 08:51:16,431][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1470000
[2024-10-02 08:51:16,437][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1470000.orbax-checkpoint-tmp-14 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1470000
[2024-10-02 08:51:16,446][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1470000`.
[2024-10-02 08:51:16,447][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1470000
[2024-10-02 08:51:16,447][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1450000
| epoch 148 | 1000/10000 batches | ms/batch 15.70 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 148 | 2000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 148 | 3000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 148 | 4000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 148 | 5000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 148 | 6000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 148 | 7000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.64
| epoch 148 | 8000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.64
| epoch 148 | 9000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.65
| epoch 148 | 10000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
-----------------------------------------------------------------------------------------
| end of epoch 148 | time per epoch: 98.94s |
| Train Metrics | accuracy:  0.61 | loss:  0.64
| Eval  Metrics | accuracy:  0.61 | loss:  0.65
-----------------------------------------------------------------------------------------
[2024-10-02 08:53:05,488][absl][INFO] - Saving checkpoint at step: 1480000
[2024-10-02 08:53:05,490][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:53:05,494][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1480000.
[2024-10-02 08:53:05,495][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1480000
[2024-10-02 08:53:05,496][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1480000
[2024-10-02 08:53:05,605][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1480000
[2024-10-02 08:53:05,610][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1480000.orbax-checkpoint-tmp-15 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1480000
[2024-10-02 08:53:05,620][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1480000`.
[2024-10-02 08:53:05,620][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1480000
[2024-10-02 08:53:05,621][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1470000
| epoch 149 | 1000/10000 batches | ms/batch 16.13 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 149 | 2000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 149 | 3000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 149 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 149 | 5000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.67
| epoch 149 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.67
| epoch 149 | 7000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.65
| epoch 149 | 8000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.64
| epoch 149 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 149 | 10000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.64
-----------------------------------------------------------------------------------------
| end of epoch 149 | time per epoch: 99.72s |
| Train Metrics | accuracy:  0.61 | loss:  0.65
| Eval  Metrics | accuracy:  0.61 | loss:  0.65
-----------------------------------------------------------------------------------------
[2024-10-02 08:54:55,270][absl][INFO] - Saving checkpoint at step: 1490000
[2024-10-02 08:54:55,277][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:54:55,281][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1490000.
[2024-10-02 08:54:55,281][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1490000
[2024-10-02 08:54:55,282][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 08:54:55,283][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1490000
[2024-10-02 08:54:55,744][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1490000
[2024-10-02 08:54:55,748][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1490000.orbax-checkpoint-tmp-16 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1490000
[2024-10-02 08:54:55,757][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1490000`.
[2024-10-02 08:54:55,758][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1490000
[2024-10-02 08:54:55,758][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1480000
| epoch 150 | 1000/10000 batches | ms/batch 15.45 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 2000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 3000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 4000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 5000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 6000/10000 batches | ms/batch  8.40 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 7000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 8000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.64
| epoch 150 | 9000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 150 | 10000/10000 batches | ms/batch  8.41 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
-----------------------------------------------------------------------------------------
| end of epoch 150 | time per epoch: 95.87s |
| Train Metrics | accuracy:  0.62 | loss:  0.64
| Eval  Metrics | accuracy:  0.61 | loss:  0.64
-----------------------------------------------------------------------------------------
[2024-10-02 08:56:41,937][absl][INFO] - Saving checkpoint at step: 1500000
[2024-10-02 08:56:41,941][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 08:56:41,945][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1500000.
[2024-10-02 08:56:41,946][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1500000
[2024-10-02 08:56:41,947][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1500000
[2024-10-02 08:56:42,063][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1500000
[2024-10-02 08:56:42,067][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1500000.orbax-checkpoint-tmp-17 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1500000
[2024-10-02 08:56:42,071][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1500000`.
[2024-10-02 08:56:42,071][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1500000
[2024-10-02 08:56:42,071][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1490000
| epoch 151 | 1000/10000 batches | ms/batch 15.75 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 151 | 2000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 151 | 3000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.60 | Performance/Training loss:  0.66
| epoch 151 | 4000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.68
| epoch 151 | 5000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.70
| epoch 151 | 6000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.53 | Performance/Training loss:  0.69
| epoch 151 | 7000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.68
| epoch 151 | 8000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.68
| epoch 151 | 9000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.54 | Performance/Training loss:  0.68
| epoch 151 | 10000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.55 | Performance/Training loss:  0.67
-----------------------------------------------------------------------------------------
| end of epoch 151 | time per epoch: 98.95s |
| Train Metrics | accuracy:  0.56 | loss:  0.67
| Eval  Metrics | accuracy:  0.55 | loss:  0.68
-----------------------------------------------------------------------------------------
| epoch 152 | 1000/10000 batches | ms/batch 15.47 | Performance/Training accuracy:  0.55 | Performance/Training loss:  0.67
| epoch 152 | 2000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.56 | Performance/Training loss:  0.67
| epoch 152 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.59 | Performance/Training loss:  0.66
| epoch 152 | 4000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.64
| epoch 152 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.64
| epoch 152 | 6000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 152 | 7000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 152 | 8000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.64
| epoch 152 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 152 | 10000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.63
-----------------------------------------------------------------------------------------
| end of epoch 152 | time per epoch: 98.69s |
| Train Metrics | accuracy:  0.60 | loss:  0.65
| Eval  Metrics | accuracy:  0.61 | loss:  0.64
-----------------------------------------------------------------------------------------
[2024-10-02 09:00:20,781][absl][INFO] - Saving checkpoint at step: 1520000
[2024-10-02 09:00:20,786][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:00:20,790][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1520000.
[2024-10-02 09:00:20,791][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1520000
[2024-10-02 09:00:20,791][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:00:20,793][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1520000
[2024-10-02 09:00:20,909][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1520000
[2024-10-02 09:00:20,913][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1520000.orbax-checkpoint-tmp-18 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1520000
[2024-10-02 09:00:20,924][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1520000`.
[2024-10-02 09:00:20,924][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1520000
[2024-10-02 09:00:20,925][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1500000
| epoch 153 | 1000/10000 batches | ms/batch 16.41 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 2000/10000 batches | ms/batch  9.09 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.63
| epoch 153 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 4000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 5000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 153 | 6000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 8000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 153 | 10000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
-----------------------------------------------------------------------------------------
| end of epoch 153 | time per epoch: 99.43s |
| Train Metrics | accuracy:  0.63 | loss:  0.63
| Eval  Metrics | accuracy:  0.62 | loss:  0.64
-----------------------------------------------------------------------------------------
[2024-10-02 09:02:10,659][absl][INFO] - Saving checkpoint at step: 1530000
[2024-10-02 09:02:10,664][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:02:10,672][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1530000.
[2024-10-02 09:02:10,673][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1530000
[2024-10-02 09:02:10,675][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1530000
[2024-10-02 09:02:10,783][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1530000
[2024-10-02 09:02:10,787][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1530000.orbax-checkpoint-tmp-19 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1530000
[2024-10-02 09:02:10,795][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1530000`.
[2024-10-02 09:02:10,795][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1530000
[2024-10-02 09:02:10,796][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1520000
| epoch 154 | 1000/10000 batches | ms/batch 15.85 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 154 | 2000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.62
| epoch 154 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 154 | 4000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.63
| epoch 154 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 154 | 6000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 154 | 7000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 154 | 8000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 154 | 9000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 154 | 10000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
-----------------------------------------------------------------------------------------
| end of epoch 154 | time per epoch: 99.62s |
| Train Metrics | accuracy:  0.63 | loss:  0.63
| Eval  Metrics | accuracy:  0.62 | loss:  0.64
-----------------------------------------------------------------------------------------
[2024-10-02 09:04:00,389][absl][INFO] - Saving checkpoint at step: 1540000
[2024-10-02 09:04:00,394][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:04:00,398][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1540000.
[2024-10-02 09:04:00,399][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1540000
[2024-10-02 09:04:00,399][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:04:00,400][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1540000
[2024-10-02 09:04:00,511][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1540000
[2024-10-02 09:04:00,516][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1540000.orbax-checkpoint-tmp-20 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1540000
[2024-10-02 09:04:00,523][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1540000`.
[2024-10-02 09:04:00,524][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1540000
[2024-10-02 09:04:00,525][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1530000
| epoch 155 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 155 | 2000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 155 | 3000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 155 | 4000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 155 | 5000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 155 | 6000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.63
| epoch 155 | 7000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 155 | 8000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 155 | 9000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 155 | 10000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
-----------------------------------------------------------------------------------------
| end of epoch 155 | time per epoch: 99.02s |
| Train Metrics | accuracy:  0.63 | loss:  0.62
| Eval  Metrics | accuracy:  0.62 | loss:  0.64
-----------------------------------------------------------------------------------------
[2024-10-02 09:05:50,182][absl][INFO] - Saving checkpoint at step: 1550000
[2024-10-02 09:05:50,187][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:05:50,190][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1550000.
[2024-10-02 09:05:50,191][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1550000
[2024-10-02 09:05:50,192][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1550000
[2024-10-02 09:05:50,298][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1550000
[2024-10-02 09:05:50,303][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1550000.orbax-checkpoint-tmp-21 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1550000
[2024-10-02 09:05:50,310][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1550000`.
[2024-10-02 09:05:50,310][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1550000
[2024-10-02 09:05:50,311][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1540000
| epoch 156 | 1000/10000 batches | ms/batch 15.94 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 156 | 2000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 156 | 3000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.63
| epoch 156 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 156 | 5000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
| epoch 156 | 6000/10000 batches | ms/batch  9.52 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 156 | 7000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.64
| epoch 156 | 8000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.62 | Performance/Training loss:  0.63
| epoch 156 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.61 | Performance/Training loss:  0.63
| epoch 156 | 10000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.63
-----------------------------------------------------------------------------------------
| end of epoch 156 | time per epoch: 98.59s |
| Train Metrics | accuracy:  0.63 | loss:  0.63
| Eval  Metrics | accuracy:  0.62 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 157 | 1000/10000 batches | ms/batch 16.01 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 157 | 2000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 157 | 3000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.63 | Performance/Training loss:  0.62
| epoch 157 | 4000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 157 | 5000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62

| epoch 157 | 6000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 157 | 7000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 157 | 8000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 157 | 9000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 157 | 10000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
-----------------------------------------------------------------------------------------
| end of epoch 157 | time per epoch: 99.12s |
| Train Metrics | accuracy:  0.64 | loss:  0.62
| Eval  Metrics | accuracy:  0.63 | loss:  0.63
-----------------------------------------------------------------------------------------
[2024-10-02 09:09:28,630][absl][INFO] - Saving checkpoint at step: 1570000
[2024-10-02 09:09:28,636][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:09:28,640][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1570000.
[2024-10-02 09:09:28,640][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1570000
[2024-10-02 09:09:28,640][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:09:28,641][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1570000
[2024-10-02 09:09:28,752][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1570000
[2024-10-02 09:09:28,757][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1570000.orbax-checkpoint-tmp-22 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1570000
[2024-10-02 09:09:28,764][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1570000`.
[2024-10-02 09:09:28,764][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1570000
[2024-10-02 09:09:28,765][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1550000
| epoch 158 | 1000/10000 batches | ms/batch 15.77 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 158 | 2000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 158 | 3000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 158 | 4000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 158 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 158 | 6000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 158 | 7000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 158 | 8000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 158 | 9000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 158 | 10000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 158 | time per epoch: 98.91s |
| Train Metrics | accuracy:  0.64 | loss:  0.61
| Eval  Metrics | accuracy:  0.63 | loss:  0.63
-----------------------------------------------------------------------------------------
[2024-10-02 09:11:18,103][absl][INFO] - Saving checkpoint at step: 1580000
[2024-10-02 09:11:18,104][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:11:18,108][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1580000.
[2024-10-02 09:11:18,109][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1580000
[2024-10-02 09:11:18,110][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1580000
[2024-10-02 09:11:18,216][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1580000
[2024-10-02 09:11:18,221][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1580000.orbax-checkpoint-tmp-23 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1580000
[2024-10-02 09:11:18,229][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1580000`.
[2024-10-02 09:11:18,229][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1580000
[2024-10-02 09:11:18,230][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1570000
| epoch 159 | 1000/10000 batches | ms/batch 15.24 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 159 | 2000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 159 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.62
| epoch 159 | 4000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 159 | 5000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 159 | 6000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 159 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.62
| epoch 159 | 8000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 159 | 9000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 159 | 10000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 159 | time per epoch: 98.77s |
| Train Metrics | accuracy:  0.64 | loss:  0.61
| Eval  Metrics | accuracy:  0.63 | loss:  0.63
-----------------------------------------------------------------------------------------
[2024-10-02 09:13:07,005][absl][INFO] - Saving checkpoint at step: 1590000
[2024-10-02 09:13:07,006][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:13:07,010][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1590000.
[2024-10-02 09:13:07,011][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1590000
[2024-10-02 09:13:07,011][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:13:07,012][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1590000
[2024-10-02 09:13:07,135][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1590000
[2024-10-02 09:13:07,141][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1590000.orbax-checkpoint-tmp-24 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1590000
[2024-10-02 09:13:07,150][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1590000`.
[2024-10-02 09:13:07,150][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1590000
[2024-10-02 09:13:07,151][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1580000
| epoch 160 | 1000/10000 batches | ms/batch 15.73 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 160 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 160 | 3000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 4000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 5000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 6000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 7000/10000 batches | ms/batch  9.52 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 8000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 9000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 160 | 10000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 160 | time per epoch: 98.88s |
| Train Metrics | accuracy:  0.65 | loss:  0.61
| Eval  Metrics | accuracy:  0.63 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 161 | 1000/10000 batches | ms/batch 15.97 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 161 | 2000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 161 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 161 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 161 | 5000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 161 | 6000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 161 | 7000/10000 batches | ms/batch  9.49 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 161 | 8000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 161 | 9000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 161 | 10000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 161 | time per epoch: 99.87s |
| Train Metrics | accuracy:  0.65 | loss:  0.61
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:16:46,698][absl][INFO] - Saving checkpoint at step: 1610000
[2024-10-02 09:16:46,703][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:16:46,707][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1610000.
[2024-10-02 09:16:46,708][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1610000
[2024-10-02 09:16:46,708][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:16:46,709][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1610000
[2024-10-02 09:16:46,822][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1610000
[2024-10-02 09:16:46,827][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1610000.orbax-checkpoint-tmp-25 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1610000
[2024-10-02 09:16:46,835][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1610000`.
[2024-10-02 09:16:46,836][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1610000
[2024-10-02 09:16:46,837][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1590000
| epoch 162 | 1000/10000 batches | ms/batch 15.82 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 162 | 2000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 162 | 3000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 162 | 4000/10000 batches | ms/batch  8.80 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 162 | 5000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 162 | 6000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 162 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 162 | 8000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 162 | 9000/10000 batches | ms/batch  8.78 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 162 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
-----------------------------------------------------------------------------------------
| end of epoch 162 | time per epoch: 98.94s |
| Train Metrics | accuracy:  0.65 | loss:  0.60
| Eval  Metrics | accuracy:  0.64 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 163 | 1000/10000 batches | ms/batch 15.78 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 2000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 3000/10000 batches | ms/batch  9.50 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 4000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.61
| epoch 163 | 5000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 6000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 7000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 8000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 9000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 163 | 10000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
-----------------------------------------------------------------------------------------
| end of epoch 163 | time per epoch: 98.82s |
| Train Metrics | accuracy:  0.66 | loss:  0.60
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
| epoch 164 | 1000/10000 batches | ms/batch 15.86 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 164 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 164 | 3000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 164 | 4000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 164 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 164 | 6000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 164 | 7000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 164 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 164 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 164 | 10000/10000 batches | ms/batch  9.20 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
-----------------------------------------------------------------------------------------
| end of epoch 164 | time per epoch: 99.03s |
| Train Metrics | accuracy:  0.66 | loss:  0.60
| Eval  Metrics | accuracy:  0.64 | loss:  0.63
-----------------------------------------------------------------------------------------
[2024-10-02 09:22:14,797][absl][INFO] - Saving checkpoint at step: 1640000
[2024-10-02 09:22:14,803][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:22:14,807][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1640000.
[2024-10-02 09:22:14,807][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1640000
[2024-10-02 09:22:14,807][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:22:14,809][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1640000
[2024-10-02 09:22:14,917][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1640000
[2024-10-02 09:22:14,923][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1640000.orbax-checkpoint-tmp-26 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1640000
[2024-10-02 09:22:14,933][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1640000`.
[2024-10-02 09:22:14,934][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1640000
[2024-10-02 09:22:14,935][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1610000
| epoch 165 | 1000/10000 batches | ms/batch 16.11 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 165 | 2000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 165 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 165 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 165 | 5000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 165 | 6000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 165 | 7000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 165 | 8000/10000 batches | ms/batch  9.08 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.60
| epoch 165 | 9000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
| epoch 165 | 10000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.60
-----------------------------------------------------------------------------------------
| end of epoch 165 | time per epoch: 98.36s |
| Train Metrics | accuracy:  0.66 | loss:  0.59
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:24:03,281][absl][INFO] - Saving checkpoint at step: 1650000
[2024-10-02 09:24:03,282][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:24:03,286][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1650000.
[2024-10-02 09:24:03,286][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1650000
[2024-10-02 09:24:03,289][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1650000
[2024-10-02 09:24:03,399][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1650000
[2024-10-02 09:24:03,404][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1650000.orbax-checkpoint-tmp-27 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1650000
[2024-10-02 09:24:03,413][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1650000`.
[2024-10-02 09:24:03,413][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1650000
[2024-10-02 09:24:03,414][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1640000
| epoch 166 | 1000/10000 batches | ms/batch 15.93 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 166 | 2000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 166 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.65 | Performance/Training loss:  0.61
| epoch 166 | 4000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 166 | 5000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 166 | 6000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 166 | 7000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 166 | 8000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 166 | 9000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 166 | 10000/10000 batches | ms/batch  8.66 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
-----------------------------------------------------------------------------------------
| end of epoch 166 | time per epoch: 98.56s |
| Train Metrics | accuracy:  0.66 | loss:  0.59
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:25:52,331][absl][INFO] - Saving checkpoint at step: 1660000
[2024-10-02 09:25:52,332][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:25:52,335][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1660000.
[2024-10-02 09:25:52,335][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1660000
[2024-10-02 09:25:52,335][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:25:52,336][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1660000
[2024-10-02 09:25:52,445][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1660000
[2024-10-02 09:25:52,451][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1660000.orbax-checkpoint-tmp-28 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1660000
[2024-10-02 09:25:52,461][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1660000`.
[2024-10-02 09:25:52,461][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1660000
[2024-10-02 09:25:52,462][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1650000
| epoch 167 | 1000/10000 batches | ms/batch 16.03 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 167 | 2000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 167 | 3000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 167 | 4000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 167 | 5000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 167 | 6000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 167 | 7000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 167 | 8000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.64 | Performance/Training loss:  0.62
| epoch 167 | 9000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 167 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
-----------------------------------------------------------------------------------------
| end of epoch 167 | time per epoch: 98.71s |
| Train Metrics | accuracy:  0.66 | loss:  0.59
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:27:41,289][absl][INFO] - Saving checkpoint at step: 1670000
[2024-10-02 09:27:41,290][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:27:41,298][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1670000.
[2024-10-02 09:27:41,298][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1670000
[2024-10-02 09:27:41,300][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1670000
[2024-10-02 09:27:41,757][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1670000
[2024-10-02 09:27:41,762][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1670000.orbax-checkpoint-tmp-29 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1670000
[2024-10-02 09:27:41,770][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1670000`.
[2024-10-02 09:27:41,770][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1670000
[2024-10-02 09:27:41,771][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1660000
| epoch 168 | 1000/10000 batches | ms/batch 15.43 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 168 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 168 | 3000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 168 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 168 | 5000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 168 | 6000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 168 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 168 | 8000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 168 | 9000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 168 | 10000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.61
-----------------------------------------------------------------------------------------
| end of epoch 168 | time per epoch: 99.07s |
| Train Metrics | accuracy:  0.67 | loss:  0.59
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
| epoch 169 | 1000/10000 batches | ms/batch 15.68 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 169 | 2000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 169 | 3000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 169 | 4000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 169 | 5000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 169 | 6000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 169 | 7000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.66 | Performance/Training loss:  0.59
| epoch 169 | 8000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 169 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 169 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
-----------------------------------------------------------------------------------------
| end of epoch 169 | time per epoch: 98.76s |
| Train Metrics | accuracy:  0.67 | loss:  0.58
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:31:20,249][absl][INFO] - Saving checkpoint at step: 1690000
[2024-10-02 09:31:20,253][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:31:20,258][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1690000.
[2024-10-02 09:31:20,258][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1690000
[2024-10-02 09:31:20,259][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:31:20,260][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1690000
[2024-10-02 09:31:20,368][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1690000
[2024-10-02 09:31:20,372][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1690000.orbax-checkpoint-tmp-30 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1690000
[2024-10-02 09:31:20,375][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1690000`.
[2024-10-02 09:31:20,375][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1690000
[2024-10-02 09:31:20,376][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1670000
| epoch 170 | 1000/10000 batches | ms/batch 15.47 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 2000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 3000/10000 batches | ms/batch  9.55 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 4000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 6000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 7000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 8000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 170 | 9000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 170 | 10000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
-----------------------------------------------------------------------------------------
| end of epoch 170 | time per epoch: 99.77s |
| Train Metrics | accuracy:  0.67 | loss:  0.58
| Eval  Metrics | accuracy:  0.64 | loss:  0.62
-----------------------------------------------------------------------------------------
| epoch 171 | 1000/10000 batches | ms/batch 16.07 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 171 | 2000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 171 | 3000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 171 | 4000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 171 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.59
| epoch 171 | 6000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 171 | 7000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 171 | 8000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 171 | 9000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 171 | 10000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
-----------------------------------------------------------------------------------------
| end of epoch 171 | time per epoch: 98.37s |
| Train Metrics | accuracy:  0.67 | loss:  0.58
| Eval  Metrics | accuracy:  0.65 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:34:59,286][absl][INFO] - Saving checkpoint at step: 1710000
[2024-10-02 09:34:59,292][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:34:59,299][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1710000.
[2024-10-02 09:34:59,299][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1710000
[2024-10-02 09:34:59,300][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:34:59,301][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1710000
[2024-10-02 09:34:59,410][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1710000
[2024-10-02 09:34:59,416][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1710000.orbax-checkpoint-tmp-31 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1710000
[2024-10-02 09:34:59,427][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1710000`.
[2024-10-02 09:34:59,428][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1710000
[2024-10-02 09:34:59,429][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1690000
| epoch 172 | 1000/10000 batches | ms/batch 15.92 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 172 | 2000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 172 | 3000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 172 | 4000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 172 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 172 | 6000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
| epoch 172 | 7000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 172 | 8000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 172 | 9000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.58
| epoch 172 | 10000/10000 batches | ms/batch  9.00 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.57
-----------------------------------------------------------------------------------------
| end of epoch 172 | time per epoch: 98.30s |
| Train Metrics | accuracy:  0.68 | loss:  0.57
| Eval  Metrics | accuracy:  0.65 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:36:47,881][absl][INFO] - Saving checkpoint at step: 1720000
[2024-10-02 09:36:47,882][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:36:47,886][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1720000.
[2024-10-02 09:36:47,887][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1720000
[2024-10-02 09:36:47,888][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1720000
[2024-10-02 09:36:48,006][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1720000
[2024-10-02 09:36:48,012][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1720000.orbax-checkpoint-tmp-32 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1720000
[2024-10-02 09:36:48,021][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1720000`.
[2024-10-02 09:36:48,021][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1720000
[2024-10-02 09:36:48,022][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1710000
| epoch 173 | 1000/10000 batches | ms/batch 15.90 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 2000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 3000/10000 batches | ms/batch  8.61 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 4000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 5000/10000 batches | ms/batch  8.35 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 6000/10000 batches | ms/batch  8.79 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 7000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.57
| epoch 173 | 8000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 9000/10000 batches | ms/batch  8.84 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 173 | 10000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.58
-----------------------------------------------------------------------------------------
| end of epoch 173 | time per epoch: 96.26s |
| Train Metrics | accuracy:  0.68 | loss:  0.57
| Eval  Metrics | accuracy:  0.65 | loss:  0.62
-----------------------------------------------------------------------------------------
[2024-10-02 09:38:34,890][absl][INFO] - Saving checkpoint at step: 1730000
[2024-10-02 09:38:34,895][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:38:34,901][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1730000.
[2024-10-02 09:38:34,902][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1730000
[2024-10-02 09:38:34,902][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:38:34,904][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1730000
[2024-10-02 09:38:35,024][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1730000
[2024-10-02 09:38:35,029][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1730000.orbax-checkpoint-tmp-33 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1730000
[2024-10-02 09:38:35,037][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1730000`.
[2024-10-02 09:38:35,037][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1730000
[2024-10-02 09:38:35,038][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1720000
| epoch 174 | 1000/10000 batches | ms/batch 16.04 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 2000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 3000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 4000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 174 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 6000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 7000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 9000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 174 | 10000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
-----------------------------------------------------------------------------------------
| end of epoch 174 | time per epoch: 98.64s |
| Train Metrics | accuracy:  0.68 | loss:  0.57
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 175 | 1000/10000 batches | ms/batch 15.98 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 175 | 2000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.56
| epoch 175 | 3000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.57
| epoch 175 | 4000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 175 | 5000/10000 batches | ms/batch  9.05 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.57
| epoch 175 | 6000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.67 | Performance/Training loss:  0.57
| epoch 175 | 7000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 175 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 175 | 9000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 175 | 10000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 175 | time per epoch: 99.32s |
| Train Metrics | accuracy:  0.68 | loss:  0.57
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 176 | 1000/10000 batches | ms/batch 16.14 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 176 | 2000/10000 batches | ms/batch  8.81 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 176 | 3000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 176 | 4000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.57
| epoch 176 | 5000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 176 | 6000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 176 | 7000/10000 batches | ms/batch  8.67 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 176 | 8000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 176 | 9000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 176 | 10000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.57
-----------------------------------------------------------------------------------------
| end of epoch 176 | time per epoch: 97.74s |
| Train Metrics | accuracy:  0.69 | loss:  0.56
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 177 | 1000/10000 batches | ms/batch 15.89 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 177 | 2000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 177 | 3000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 177 | 4000/10000 batches | ms/batch  8.78 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.56
| epoch 177 | 5000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 177 | 6000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.57
| epoch 177 | 7000/10000 batches | ms/batch  9.11 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 177 | 8000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 177 | 9000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 177 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 177 | time per epoch: 98.44s |
| Train Metrics | accuracy:  0.69 | loss:  0.56
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
[2024-10-02 09:45:51,025][absl][INFO] - Saving checkpoint at step: 1770000
[2024-10-02 09:45:51,032][absl][INFO] - Using Orbax as backend to save Flax checkpoints. For potential troubleshooting see: https://flax.readthedocs.io/en/latest/guides/training_techniques/use_checkpointing.html#orbax-as-backend-troubleshooting
[2024-10-02 09:45:51,036][absl][INFO] - Saving checkpoint to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000.
[2024-10-02 09:45:51,037][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:pre.checkpoint_1770000
[2024-10-02 09:45:51,037][absl][INFO] - Failed to get flag value for EXPERIMENTAL_ORBAX_USE_DISTRIBUTED_PROCESS_ID.
[2024-10-02 09:45:51,039][absl][INFO] - Skipping global process sync, barrier name: create_tmp_directory:post.checkpoint_1770000
[2024-10-02 09:45:51,151][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:save.checkpoint_1770000
[2024-10-02 09:45:51,157][absl][INFO] - Renaming /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000.orbax-checkpoint-tmp-34 to /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000
[2024-10-02 09:45:51,166][absl][INFO] - Finished saving checkpoint to `/data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000`.
[2024-10-02 09:45:51,167][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:finalize.checkpoint_1770000
[2024-10-02 09:45:51,168][absl][INFO] - Removing checkpoint at /data/tsoyda/RPG/master_thesis_taylan_soydan/event-ssm/outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1730000
| epoch 178 | 1000/10000 batches | ms/batch 16.13 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 178 | 2000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 178 | 3000/10000 batches | ms/batch  9.16 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.56
| epoch 178 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 178 | 5000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56

| epoch 178 | 6000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 178 | 7000/10000 batches | ms/batch  9.02 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 178 | 8000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 178 | 9000/10000 batches | ms/batch  9.42 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.57
| epoch 178 | 10000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 178 | time per epoch: 99.75s |
| Train Metrics | accuracy:  0.69 | loss:  0.56
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 179 | 1000/10000 batches | ms/batch 15.94 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 179 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 179 | 3000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 179 | 4000/10000 batches | ms/batch  9.03 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 179 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 179 | 6000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 179 | 7000/10000 batches | ms/batch  9.52 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 179 | 8000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.68 | Performance/Training loss:  0.56
| epoch 179 | 9000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 179 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 179 | time per epoch: 99.40s |
| Train Metrics | accuracy:  0.69 | loss:  0.56
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 180 | 1000/10000 batches | ms/batch 16.01 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 180 | 2000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55

| epoch 180 | 3000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 180 | 4000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 180 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 180 | 6000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 180 | 7000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 180 | 8000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 180 | 9000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 180 | 10000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
-----------------------------------------------------------------------------------------
| end of epoch 180 | time per epoch: 99.71s |
| Train Metrics | accuracy:  0.69 | loss:  0.56
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 181 | 1000/10000 batches | ms/batch 16.17 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 181 | 2000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 181 | 3000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 181 | 4000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 181 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 181 | 6000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 181 | 7000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 181 | 8000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 181 | 9000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 181 | 10000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 181 | time per epoch: 99.54s |
| Train Metrics | accuracy:  0.69 | loss:  0.55
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------

| epoch 182 | 1000/10000 batches | ms/batch 16.27 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 182 | 2000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 182 | 3000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.55
| epoch 182 | 4000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 182 | 5000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55

| epoch 182 | 6000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 182 | 7000/10000 batches | ms/batch  9.40 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 182 | 8000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 182 | 9000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 182 | 10000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 182 | time per epoch: 99.88s |
| Train Metrics | accuracy:  0.70 | loss:  0.55
| Eval  Metrics | accuracy:  0.65 | loss:  0.63
-----------------------------------------------------------------------------------------
| epoch 183 | 1000/10000 batches | ms/batch 16.03 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 183 | 2000/10000 batches | ms/batch  9.15 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 183 | 3000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 183 | 4000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 183 | 5000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 183 | 6000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 183 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 183 | 8000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 183 | 9000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.56
| epoch 183 | 10000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 183 | time per epoch: 98.59s |
| Train Metrics | accuracy:  0.70 | loss:  0.55
| Eval  Metrics | accuracy:  0.65 | loss:  0.64
-----------------------------------------------------------------------------------------
| epoch 184 | 1000/10000 batches | ms/batch 15.80 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 184 | 2000/10000 batches | ms/batch  9.50 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 184 | 3000/10000 batches | ms/batch  9.44 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 184 | 4000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 184 | 5000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 184 | 6000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 184 | 7000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.69 | Performance/Training loss:  0.55
| epoch 184 | 8000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 184 | 9000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 184 | 10000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 184 | time per epoch: 99.53s |
| Train Metrics | accuracy:  0.70 | loss:  0.55
| Eval  Metrics | accuracy:  0.65 | loss:  0.64
-----------------------------------------------------------------------------------------
| epoch 185 | 1000/10000 batches | ms/batch 16.07 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 185 | 2000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 185 | 3000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 185 | 4000/10000 batches | ms/batch  8.85 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 185 | 5000/10000 batches | ms/batch  8.75 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 185 | 6000/10000 batches | ms/batch  8.74 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 185 | 7000/10000 batches | ms/batch  8.89 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.55
| epoch 185 | 8000/10000 batches | ms/batch  8.33 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 185 | 9000/10000 batches | ms/batch  8.73 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 185 | 10000/10000 batches | ms/batch  8.70 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 185 | time per epoch: 95.61s |
| Train Metrics | accuracy:  0.70 | loss:  0.55
| Eval  Metrics | accuracy:  0.65 | loss:  0.64
-----------------------------------------------------------------------------------------
| epoch 186 | 1000/10000 batches | ms/batch 15.74 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 186 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 186 | 3000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 186 | 4000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 186 | 5000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 186 | 6000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 186 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 186 | 8000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 186 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 186 | 10000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 186 | time per epoch: 99.33s |
| Train Metrics | accuracy:  0.70 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.64
-----------------------------------------------------------------------------------------
| epoch 187 | 1000/10000 batches | ms/batch 16.06 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 187 | 2000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 187 | 3000/10000 batches | ms/batch  9.06 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 187 | 4000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 187 | 5000/10000 batches | ms/batch  8.96 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 187 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 187 | 7000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 187 | 8000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 187 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 187 | 10000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 187 | time per epoch: 98.99s |
| Train Metrics | accuracy:  0.71 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 188 | 1000/10000 batches | ms/batch 16.23 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 188 | 2000/10000 batches | ms/batch  9.69 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 188 | 3000/10000 batches | ms/batch  9.36 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 188 | 4000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 188 | 5000/10000 batches | ms/batch  8.86 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 188 | 6000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 188 | 7000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 188 | 8000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
| epoch 188 | 9000/10000 batches | ms/batch  8.99 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 188 | 10000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.55
-----------------------------------------------------------------------------------------
| end of epoch 188 | time per epoch: 99.87s |
| Train Metrics | accuracy:  0.70 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.64
-----------------------------------------------------------------------------------------
| epoch 189 | 1000/10000 batches | ms/batch 16.12 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 189 | 2000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 189 | 3000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 189 | 4000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 189 | 5000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 189 | 6000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 189 | 7000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 189 | 8000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 189 | 9000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 189 | 10000/10000 batches | ms/batch  9.17 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 189 | time per epoch: 98.52s |
| Train Metrics | accuracy:  0.71 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 190 | 1000/10000 batches | ms/batch 16.07 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 190 | 2000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 190 | 3000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 190 | 4000/10000 batches | ms/batch  8.87 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 190 | 5000/10000 batches | ms/batch  9.07 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 190 | 6000/10000 batches | ms/batch  8.98 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 190 | 7000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 190 | 8000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 190 | 9000/10000 batches | ms/batch  8.60 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 190 | 10000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 190 | time per epoch: 97.80s |
| Train Metrics | accuracy:  0.71 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.64
-----------------------------------------------------------------------------------------
| epoch 191 | 1000/10000 batches | ms/batch 16.44 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 191 | 2000/10000 batches | ms/batch  8.90 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 191 | 3000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 191 | 4000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 191 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 191 | 6000/10000 batches | ms/batch  8.91 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 191 | 7000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 191 | 8000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 191 | 9000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 191 | 10000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 191 | time per epoch: 99.58s |
| Train Metrics | accuracy:  0.71 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 192 | 1000/10000 batches | ms/batch 15.91 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 192 | 2000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 192 | 3000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 192 | 4000/10000 batches | ms/batch  8.97 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 192 | 5000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 192 | 6000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 192 | 7000/10000 batches | ms/batch  9.33 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54

| epoch 192 | 8000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 192 | 9000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 192 | 10000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 192 | time per epoch: 98.95s |
| Train Metrics | accuracy:  0.71 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 193 | 1000/10000 batches | ms/batch 15.80 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 193 | 2000/10000 batches | ms/batch  9.48 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 193 | 3000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 193 | 4000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 193 | 5000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 193 | 6000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 193 | 7000/10000 batches | ms/batch  9.41 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 193 | 8000/10000 batches | ms/batch  9.47 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 193 | 9000/10000 batches | ms/batch  9.13 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.53
| epoch 193 | 10000/10000 batches | ms/batch  9.10 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 193 | time per epoch: 99.63s |
| Train Metrics | accuracy:  0.71 | loss:  0.54
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 194 | 1000/10000 batches | ms/batch 16.08 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 194 | 2000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 194 | 3000/10000 batches | ms/batch  9.12 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 194 | 4000/10000 batches | ms/batch  9.47 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 194 | 5000/10000 batches | ms/batch  9.43 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 194 | 6000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 194 | 7000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53

| epoch 194 | 8000/10000 batches | ms/batch  9.01 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 194 | 9000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 194 | 10000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
-----------------------------------------------------------------------------------------
| end of epoch 194 | time per epoch: 99.91s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 195 | 1000/10000 batches | ms/batch 15.77 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 2000/10000 batches | ms/batch  9.45 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 195 | 3000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 4000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 5000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 195 | 6000/10000 batches | ms/batch  8.83 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 7000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 8000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 9000/10000 batches | ms/batch  9.35 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 195 | 10000/10000 batches | ms/batch  8.93 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 195 | time per epoch: 99.26s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 196 | 1000/10000 batches | ms/batch 16.43 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 196 | 2000/10000 batches | ms/batch  9.31 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 196 | 3000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 196 | 4000/10000 batches | ms/batch  9.22 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 196 | 5000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 196 | 6000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 196 | 7000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 196 | 8000/10000 batches | ms/batch  8.88 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 196 | 9000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 196 | 10000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 196 | time per epoch: 99.72s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 197 | 1000/10000 batches | ms/batch 16.34 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 197 | 2000/10000 batches | ms/batch  9.27 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 197 | 3000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 197 | 4000/10000 batches | ms/batch  9.29 | Performance/Training accuracy:  0.70 | Performance/Training loss:  0.54
| epoch 197 | 5000/10000 batches | ms/batch  9.04 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 197 | 6000/10000 batches | ms/batch  9.46 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 197 | 7000/10000 batches | ms/batch  9.28 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 197 | 8000/10000 batches | ms/batch  9.39 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 197 | 9000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.53
| epoch 197 | 10000/10000 batches | ms/batch  8.92 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
-----------------------------------------------------------------------------------------
| end of epoch 197 | time per epoch: 99.82s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 198 | 1000/10000 batches | ms/batch 16.11 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 198 | 2000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 198 | 3000/10000 batches | ms/batch  8.95 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 198 | 4000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 198 | 5000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 198 | 6000/10000 batches | ms/batch  9.25 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 198 | 7000/10000 batches | ms/batch  8.72 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53

| epoch 198 | 8000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 198 | 9000/10000 batches | ms/batch  9.24 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 198 | 10000/10000 batches | ms/batch  9.34 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
-----------------------------------------------------------------------------------------
| end of epoch 198 | time per epoch: 98.86s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 199 | 1000/10000 batches | ms/batch 16.06 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 199 | 2000/10000 batches | ms/batch  9.54 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 199 | 3000/10000 batches | ms/batch  9.32 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 199 | 4000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.53
| epoch 199 | 5000/10000 batches | ms/batch  8.94 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 199 | 6000/10000 batches | ms/batch  9.37 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 199 | 7000/10000 batches | ms/batch  9.58 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 199 | 8000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.53
| epoch 199 | 9000/10000 batches | ms/batch  9.18 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 199 | 10000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
-----------------------------------------------------------------------------------------
| end of epoch 199 | time per epoch: 100.03s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.65
-----------------------------------------------------------------------------------------
| epoch 200 | 1000/10000 batches | ms/batch 16.11 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 200 | 2000/10000 batches | ms/batch  8.82 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 200 | 3000/10000 batches | ms/batch  9.21 | Performance/Training accuracy:  0.72 | Performance/Training loss:  0.52
| epoch 200 | 4000/10000 batches | ms/batch  9.23 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 200 | 5000/10000 batches | ms/batch  9.38 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53

| epoch 200 | 6000/10000 batches | ms/batch  9.19 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 200 | 7000/10000 batches | ms/batch  8.76 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 200 | 8000/10000 batches | ms/batch  9.14 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
| epoch 200 | 9000/10000 batches | ms/batch  9.30 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.53
| epoch 200 | 10000/10000 batches | ms/batch  9.26 | Performance/Training accuracy:  0.71 | Performance/Training loss:  0.54
-----------------------------------------------------------------------------------------
| end of epoch 200 | time per epoch: 98.84s |
| Train Metrics | accuracy:  0.71 | loss:  0.53
| Eval  Metrics | accuracy:  0.65 | loss:  0.66
-----------------------------------------------------------------------------------------
[2024-10-02 10:27:55,799][absl][INFO] - Restoring orbax checkpoint from outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000
[2024-10-02 10:27:55,878][absl][INFO] - Restoring item from outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000.
[2024-10-02 10:27:55,993][absl][WARNING] - The transformations API will eventually be replaced by an upgraded design. The current API will not be removed until this point, but it will no longer be actively worked on.
[2024-10-02 10:27:56,004][absl][INFO] - Finished restoring checkpoint from outputs/2024-10-02-04-24-00/checkpoints/checkpoint_1770000.
[2024-10-02 10:27:56,004][absl][INFO] - Skipping global process sync, barrier name: Checkpointer:restore.checkpoint_1770000
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Using embed encoder
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
Unidirectional Model
Input dependent
Not learning Learning A with log or stablessm formula
Single Linear layer for Delta, B, C
SSM: 50 -> 2 -> 50
-----------------------------------------------------------------------------------------
| End of Training |
| Test  Metrics |  accuracy:  0.65 |  loss:  0.63
-----------------------------------------------------------------------------------------